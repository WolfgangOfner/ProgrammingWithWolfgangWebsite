[ { "title": "Deploy Applications from private Repositories using Flux GitOps and Azure Arc", "url": "/deploy-application-from-private-repositories-flux-gitops-azure-arc/", "categories": "Kubernetes, Cloud", "tags": "GitOps, Flux, Azure Arc, PAT, GitHub, Azure DevOps, Kubernetes", "date": "2023-06-19 00:00:00 +0200", "snippet": "In my previous post, I provided a comprehensive guide on installing a GitOps operator and seamlessly deploying an application with the Azure Arc Flux extension. The demonstration involved utilizing an application sourced from a public repository, an approach commonly observed in various scenarios. However, it is essential to acknowledge that the majority of organizations prefer hosting their applications in private repositories.In today’s post, I aim to delve into the process of configuring the Flux GitOps operator to establish privileged access to a private Git repository situated on the widely adopted GitHub platform. By unraveling the necessary steps and techniques, I hope to equip you with the knowledge and expertise required to navigate the intricacies of private repository integration effectively.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”.Deploy an Application from a private GitHub RepositoryTo facilitate the deployment of an application residing within a private GitHub (or any Git) repository, the command employed remains unchanged from the previous post. Utilize the same command, as follows:The sole distinction lies in the utilization of the –url parameter, where you specify the private repository. Notably, the application in this repository is the same as I used in the previous post. Additionally, I have included a demo from Microsoft, which can be found on their GitHub.Open the Azure portal and navigate to the GitOps pane of your Azure Arc instance. There, you will discover the pre-established GitOps operator. Although the deployment process has succeeded, it is noteworthy that the operator is presently labeled as “Non-compliant”. The GitOps operator is not compliant You can find more detailed information of the GitOps operator by clicking on it, followed by selecting the Configuration objects pane. Within this section, you will encounter three configuration objects, all of which are flagged as non-compliant and exhibit an error message. These configuration objects encompass the GitOps operator itself and the two configuration files that were provided during the installation process via the –kustomization parameter. The configuraiton objects display an error Within the message of the gitopsoperator object, you will encounter a specific error message that sheds light on the encountered issue: “failed to checkout and determine revision; unable to clone repository.” The underlying cause of this error can be attributed to the GitOps operator’s inability to clone the repository, primarily due to its private nature, consequently depriving the operator of the necessary access permissions.Create an Access Token to access private Git Repositories on GitHubTo provide the GitOps operator with access to a private Git repository on GitHub, you have the option of utilizing SSH keys or a Personal Access Token (PAT). In this demo, I will generate and use a PAT.First, open GitHub and click on your profile located in the top-right corner, then select “Settings.” Open the settings on GitHub Within the settings page, navigate to “Developer settings.” Open the developer settings In the Developer settings section, select “Personal access tokens” and click on “Generate new token.” Open the personal access token page On the personal access token page, you can configure the access token settings. Provide it with a meaningful name, set the expiration time, and select the desired scopes. For this demo, I will grant the access token full control over private repositories. However, it is advisable to assign the token the minimum required permissions based on your specific needs. Configure the personal access token Scroll to the bottom of the page and click on “Generate token” to create the access token. Generate the Token Once the token is generated, it will be displayed on the screen. It is crucial to save the token in a secure location as this is the only instance you will be able to view it. If you close the window without saving the token, you will lose access to it. In the event that the token is lost, you will need to regenerate it or create a new one. The PAT is displayed after it is created Please note that the displayed access token should be handled with care and stored securely to maintain the integrity and security of your GitHub repositories.Create an Access Token to access private Git Repositories on Azure DevOpsCreating a personal access token (PAT) to access private Git repositories on Azure DevOps follows a similar process to GitHub. Please follow the steps below:Click on the Settings icon located in the top-right corner of Azure DevOps, then select “Personal access tokens.” Select Personal access tokens in Azure DevOps Configure the PAT by providing it with a name, expiration time, and selecting the desired scope based on your requirements.Click on “Create” to generate the PAT. Create the Azure DevOps PAT Similar to GitHub, this is your only opportunity to copy the token. Once copied, ensure that you securely store it.After copying the token, you can close the window. Copy the created access token Remember, it is crucial to handle the access token securely and store it in a safe location to maintain the confidentiality and security of your Azure DevOps repositories.Configure the Flux GitOps Operator with the Personal Access TokenOnce you have obtained the personal access token (PAT), go to the GitOps operator in the Azure Portal. Inside the GitOps operator, navigate to the Source pane, where you can view the current configuration of the operator, including the URL and branch. Configure the Source of the GitOps operator In the Authentication section, select “Provide authentication information here.” Enter the username associated with the personal access token you created and paste the token in the provided field.Click on “Apply.” After a few minutes, you should observe the creation of several objects, and all of them should be in a running state, indicating a successful deployment. Additionally, the GitOps operator should be in a “Complient” state now. The deployment succeeded To verify the new resources, you can use the CLI tool kubectl. Execute the necessary commands, such as kubectl get namespaces, to check the newly created namespaces. Several namespaces were created ConclusionIn conclusion, integrating the Flux GitOps operator with private Git repositories follows a similar process to working with public repositories. The key difference lies in granting the operator access to the private repository. Access can be provided through SSH keys or by utilizing a personal access token (PAT).Creating a PAT involves a straightforward process, which is almost identical in both GitHub and Azure DevOps. By generating and configuring a PAT, you empower the GitOps operator to securely access and deploy resources from the private repository.Whether using SSH keys or a PAT, incorporating private Git repositories into your GitOps workflow with the Flux GitOps operator enables efficient and controlled application deployments, regardless of the repository’s visibility.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”." }, { "title": "Video - Unleashing the Potential of Hybrid Cloud - Streamlining Multi-Cloud Management and On-Premises Integration - Warsaw IT Days 2023", "url": "/video-hybrid-multi-cloud-warsaw-it-days/", "categories": "Video", "tags": "Azure Arc, Kubernetes, Youtube, Video, Azure, DevOps, Azure Key Vault, Speaking, Public Speaking, Conference", "date": "2023-05-01 00:00:00 +0200", "snippet": "" }, { "title": "Speaking about scaling Workloads and Azure DevOps Agents running in Kubernetes with KEDA at the DevOps Pro Europe 2023 Conference", "url": "/speaking-about-devops/", "categories": "Speaking", "tags": "Speaking, Public Speaking, Conference, Kubernetes, DevOps, KEDA, Azure DevOps", "date": "2023-04-24 00:00:00 +0200", "snippet": "I’m excited to announce that I will be speaking at the upcoming DevOps Pro Europe 2023 conference in Vilnius, Lithuania. My session, titled “Level Up your Kubernetes Scaling with KEDA”, will be held online on May 24, 2023.In my session, we will discuss the importance of handling varying loads in service-oriented applications and the challenges modern applications face. We will explore how Kubernetes offers a way to vary the number of application instances running based on CPU or RAM utilization with the Horizontal Pod Autoscaler, but how this may not be enough. We will dive into how external events can impact application scaling and how we can configure Kubernetes for “scale to 0” to run applications only when needed.Throughout the session, I will provide practical examples of how to create and configure autoscalers using Azure Kubernetes Service and KEDA (Kubernetes Event-driven Autoscaling) to respond to external events and scale applications in Kubernetes accordingly.You can find my session here. Don’t miss out on this opportunity to learn about Kubernetes scaling with KEDA. I hope to see you there!The abstract for my session is as follows:Level Up your Kubernetes Scaling with KEDAWhether it is a normal workday or Black Friday, service-oriented applications must be able to handle varying loads. This is the only way to ensure that users are provided with a good experience and that costs are kept to a minimum.Kubernetes offers a way to vary the number of application instances running based on CPU or RAM utilization with the Horizontal Pod Autoscaler. However, modern applications often depend on a variety of components and should be able to respond to external events. These may include new messages in a queue or metrics in Azure Monitor. As an application developer or operation manager, what do I need to consider to ensure that my application can respond to these events? How can I configure Kubernetes for “scale to 0” to run my application only when needed?Using Azure Kubernetes Service and KEDA (Kubernetes Event-driven Autoscaling), this session will show practical examples of how to create and configure autoscalers to respond to external events and scale applications in Kubernetes accordingly.Slides of the TalkYou can find the slides of the talk on GitHub." }, { "title": "Video - Scaling to Success Leveraging KEDA and Kubernetes for Optimal Azure DevOps Pipeline Performance - Warsaw IT Days 2023", "url": "/video-scaling-keda-kubernetes-warsaw-it-days/", "categories": "Video", "tags": "Kubernetes, Youtube, Video, KEDA, Azure DevOps, Docker, Speaking, Public Speaking, Conference, DevOps", "date": "2023-04-17 00:00:00 +0200", "snippet": "" }, { "title": "Video - Scaling Your Applications with Azure Kubernetes Service and KEDA - Welsh Azure User Group", "url": "/video-keda-welsh-azure-user-group/", "categories": "Video", "tags": "Kubernetes, Youtube, Video, KEDA, DevOps, Speaking, Public Speaking, Conference", "date": "2023-03-27 00:00:00 +0200", "snippet": "" }, { "title": "Speaking about Hybrid and Multi-Cloud with Azure Arc at the Warsaw IT Days 2023", "url": "/speaking-about-hybrid-multi-cloud-warsaw-it-days/", "categories": "Speaking", "tags": "Azure Arc, Speaking, Public Speaking, Conference, Kubernetes, Azure, DevOps, Azure Key Vault", "date": "2023-03-20 00:00:00 +0100", "snippet": "I am thrilled to announce my second session at Warsaw IT Days 2023, titled “Unleashing the Potential of Hybrid Cloud: Streamlining Multi-Cloud Management and On-Premises Integration”During this session, we’ll dive deep into how Microsoft Azure technologies can revolutionize your hybrid cloud strategy. We’ll focus on how Azure Arc and Kubernetes can help you easily manage and secure your multi-cloud environment, while also bringing Azure services to your own hardware.You’ll learn about the key features and capabilities of these technologies, and see real-world examples of how they can be used to optimize workloads, improve compliance, and drive innovation. It’s time to take your hybrid cloud to the next level!Don’t miss out on this opportunity to streamline your multi-cloud management and on-premises integration. Register now for the Warsaw IT Days 2023, and join me for this exciting online session on March 31, 2023!The abstract for my session is as follows:Unleashing the Potential of Hybrid Cloud: Streamlining Multi-Cloud Management and On-Premises IntegrationMost companies use cloud services for their projects nowadays. These companies leverage the seemingly endless scalability and the elastic pricing model of the cloud. Nevertheless, many projects still cannot use cloud services for various reasons. These may encompass regulatory restrictions, data protection, or just having an existing on-premises infrastructure.While these are all valid causes, why not combine an existing on-premises infrastructure with the power of the cloud? This is where Azure Arc comes into play. Azure Arc allows to project on-premises infrastructure into Azure and then apply Azure services like Azure Monitor or Azure Policy on the on-premises infrastructure.This session shows how a Swiss company uses Azure Arc to manage an on-premises Kubernetes cluster. Using Azure Arc enables the development team to leverage the power of the cloud and provide a streamlined DevOps process. All this can be achieved without the cluster being accessible from the internet.Slides of the TalkYou can find the slides of the talk on GitHub.Watch on YoutubeYou can find the recording of the talk on Youtube." }, { "title": "Speaking about scaling Workloads and Azure DevOps Agents running in Kubernetes with KEDA at the Warsaw IT Days 2023", "url": "/speaking-about-keda-warsaw-it-days/", "categories": "Speaking", "tags": "Speaking, Public Speaking, Conference, Kubernetes, DevOps, KEDA, Azure DevOps", "date": "2023-03-13 00:00:00 +0100", "snippet": "I’m excited to let you know that my session, “Scaling to Success: Leveraging KEDA and Kubernetes for Optimal Azure DevOps Pipeline Performance” will be taking place online on March 31, 2023 as part of Warsaw IT Days 2023.During the talk, we’ll be discussing the use of KEDA (Kubernetes-based Event-Driven Autoscaling) to scale Azure DevOps pipeline agents in a Kubernetes cluster. I’ll be sharing the benefits of using KEDA, such as automatic scaling based on workload, and how it can help improve the efficiency and cost-effectiveness of your pipeline. Plus, I’ll be walking you through a demo of how to set up and configure KEDA with Azure DevOps pipeline agents in a Kubernetes cluster.By attending this session, you’ll learn best practices for using KEDA to optimize your pipeline and gain a deeper understanding of how to leverage Kubernetes for scaling your DevOps operations. The session will be held online, so you can join from anywhere in the world.To register for the event, head over to the conference website. I hope to see you there on March 31, 2023!The abstract for my session is as follows:Scaling to Success: Leveraging KEDA and Kubernetes for Optimal Azure DevOps Pipeline PerformanceIn this talk, we will discuss the use of KEDA (Kubernetes-based Event-Driven Autoscaling) to scale Azure DevOps pipeline agents in a Kubernetes cluster. We will cover the benefits of using KEDA, such as automatic scaling based on workload, and how it can improve the efficiency and cost-effectiveness of your pipeline.There will also be a demonstration of how to set up and configure KEDA with Azure DevOps pipeline agents in a Kubernetes cluster. Attendees will learn best practices for using KEDA to optimize their pipeline and gain a deeper understanding of how to leverage Kubernetes for scaling their DevOps operationsSlides of the TalkYou can find the slides of the talk on GitHub.Watch on YoutubeYou can find the recording of the talk on Youtube." }, { "title": "Speaking about scaling Kubernetes Workloads with KEDA at the Welsh Azure User Group", "url": "/speaking-about-keda-welsh-azure-user-group/", "categories": "Speaking", "tags": "Speaking, Public Speaking, Conference, Kubernetes, DevOps, KEDA", "date": "2023-02-27 00:00:00 +0100", "snippet": "I’m excited to announce that I’ll be giving a talk at the Welsh Azure User Group meetup on March 15th, 2023! The topic of my session is “Scaling Your Applications with Azure Kubernetes Service and KEDA”In today’s world, it’s important for service-oriented applications to be able to handle varying loads. This is crucial for providing users with a good experience and for keeping costs under control. Kubernetes offers a solution to this problem through the Horizontal Pod Autoscaler, but what about applications that need to respond to external events? That’s where KEDA comes in.During my talk, we’ll explore how to use Azure Kubernetes Service and KEDA to create and configure autoscalers for your applications. We’ll go over practical examples and cover everything from responding to new messages in a queue to metrics in Azure Monitor. I’ll also be discussing how to configure Kubernetes for “scale to 0” to run your application only when needed.If you’re an application developer or operations manager looking to ensure that your applications can respond to events and scale accordingly, this talk is for you! So, mark your calendars and sign up on Meetup. I can’t wait to see you there!The abstract for my session is as follows:Level Up your Kubernetes Scaling with KEDAWhether it is a normal workday or Black Friday, service-oriented applications must be able to handle varying loads. This is the only way to ensure that users are provided with a good experience and that costs are kept to a minimum.Kubernetes offers a way to vary the number of application instances running based on CPU or RAM utilization with the Horizontal Pod Autoscaler. However, modern applications often depend on a variety of components and should be able to respond to external events. These may include new messages in a queue or metrics in Azure Monitor.As an application developer or operation manager, what do I need to consider to ensure that my application can respond to these events? How can I configure Kubernetes for “scale to 0” to run my application only when needed?Using Azure Kubernetes Service and KEDA (Kubernetes Event-driven Autoscaling), this session will show with practical examples how to create and configure autoscalers to respond to external events and scale applications in Kubernetes accordingly.Slides of the TalkYou can find the slides of the talk on GitHub.Watch on YoutubeYou can find the recording of the meetup on Youtube." }, { "title": "Speaking about Hybrid and Multi-Cloud with Azure Arc at the Azure Day", "url": "/speaking-about-multi-and-hybrid-cloud-at-azure-day/", "categories": "Speaking", "tags": "Azure Arc, Speaking, Public Speaking, Conference, Kubernetes, Azure, DevOps, Azure Key Vault", "date": "2023-01-16 00:00:00 +0100", "snippet": "I’m excited to announce that I’ll be hosting a session at Azure Day on February 28, 2023, organized by Entwickler.de and the BASTA! conference. Get ready to take your hybrid cloud strategy to the next level with Microsoft Azure technologies!In this session, we’ll be diving deep into Azure Arc and Kubernetes to see how they can help you easily manage and secure your multi-cloud environment while also bringing Azure services to your hardware. We’ll be exploring the key features and capabilities of these technologies, and showing real-world examples of how they can optimize workloads, improve compliance, and drive innovation.So, if you’re looking to streamline your IT operations, reduce costs, or simply gain more flexibility, join me at Azure Day. Let’s learn from the experts and discover new ways to transform our organizations!Make sure to check out the website of Azure Day at to learn more and register for the event. I can’t wait to see you all there!The abstract for my session is as follows:Unleashing the Potential of Hybrid Cloud: Streamlining Multi-Cloud Management and On-Premises IntegrationMost companies use cloud services for their projects nowadays. These companies leverage the seemingly endless scalability and the elastic pricing model of the cloud. Nevertheless, many projects still cannot use cloud services for various reasons. These may encompass regulatory restrictions, data protection, or just having an existing on-premises infrastructure.While these are all valid causes, why not combine an existing on-premises infrastructure with the power of the cloud? This is where Azure Arc comes into play. Azure Arc allows to project on-premises infrastructure into Azure and then apply Azure services like Azure Monitor or Azure Policy on the on-premises infrastructure.This session shows how a Swiss company uses Azure Arc to manage an on-premises Kubernetes cluster. Using Azure Arc enables the development team to leverage the power of the cloud and provide a streamlined DevOps process. All this can be achieved without the cluster being accessible from the internet.Slides of the TalkYou can find the slides of the talk on GitHub." }, { "title": "Video - Bring DevOps to the Swiss Alps - Boston Azure User Group", "url": "/video-boston-azure-user-group-azure-arc-devops/", "categories": "Video", "tags": "Azure Arc, Kubernetes, Youtube, Video, Public Speaking, Speaking", "date": "2022-12-05 00:00:00 +0100", "snippet": "" }, { "title": "Video - Monitor an on-premises k3s Cluster with Azure Monitor and Azure Arc", "url": "/video-monitor-an-on-prem-k3s-cluster-with-azure-monitor-and-azure-arc/", "categories": "Video", "tags": "Azure Arc, Kubernetes, Youtube, Video", "date": "2022-11-21 00:00:00 +0100", "snippet": "" }, { "title": "Speaking about Azure Arc at the Boston Azure User Group", "url": "/speaking-about-azure-arc-at-the-boston-azure-user-group/", "categories": "Speaking", "tags": "Azure Arc, Speaking, Public Speaking, Conference, Kubernetes, DevOps, k3s", "date": "2022-11-07 00:00:00 +0100", "snippet": "I am thrilled to announce that I will be speaking for the first time at the Boston Azure User Group. My session, titled ‘Bring DevOps to the Swiss Alps’ will explore the challenges and solutions for implementing DevOps practices in a remote and mountainous environment.This exciting event will be held online, so you can join from anywhere. Don’t miss this opportunity to learn from my experience and gain valuable insights into the field. Register now on Meetup and see you at the event!The abstract for my session is as follows:Bring DevOps to the Swiss AlpsMost companies use cloud services for their projects nowadays. These companies leverage the seemingly endless scalability and the elastic pricing model of the cloud. Nevertheless, many projects still cannot use cloud services for various reasons. These may encompass regulatory restrictions, data protection, or just having an existing on-premises infrastructure.While these are all valid causes, why not combine an existing on-premises infrastructure with the power of the cloud? This is where Azure Arc comes into play. Azure Arc allows to project on-premises infrastructure into Azure and then apply Azure services like Azure Monitor or Azure Policy on the on-premises infrastructure.This session shows how a Swiss company uses Azure Arc to manage an on-premises Kubernetes cluster. Using Azure Arc enables the development team to leverage the power of the cloud and provide a streamlined DevOps process. All this can be achieved without the cluster being accessible from the internet.Watch on YoutubeYou can find the recording of the meetup on Youtube." }, { "title": "Video - Use the TokenRequest API to create Tokens in Kubernetes 1.24", "url": "/video-use-the-tokenrequest-api-to-create-tokens-in-kubernetes/", "categories": "Video", "tags": "Azure Arc, Kubernetes, Youtube, Video", "date": "2022-10-24 00:00:00 +0200", "snippet": "" }, { "title": "Video - Securely connect to an on-premises Kubernetes Cluster with Azure Arc", "url": "/video-securely-connect-to-an-on-prem-kubernetes-cluster-with-azure-arc/", "categories": "Video", "tags": "Azure Arc, Kubernetes, Youtube, Video", "date": "2022-10-10 00:00:00 +0200", "snippet": "" }, { "title": "Speaking about Azure Arc at BASTA! Fall 2022 in Mainz", "url": "/speaking-about-azure-arc-at-basta-fall-2022/", "categories": "Speaking", "tags": "Azure Arc, Speaking, Public Speaking, Conference, Kubernetes, DevOps, k3s", "date": "2022-10-03 00:00:00 +0200", "snippet": "I am excited to announce that I will be speaking at the BASTA! Fall 2022 conference in Frankfurt, Germany. My session, titled ‘Bring DevOps to the Swiss Alps’, will explore the challenges and solutions for implementing DevOps practices in a remote and mountainous environment. Don’t miss this opportunity to learn from my experience and gain valuable insights into the field. See you at the conference!You can join the session in-person, or remote to learn more about Azure Arc and managing your on-premises Kubernetes cluster with Azure. You can find more information on the conference website.The abstract for my session is as follows:Bring DevOps to the Swiss AlpsMost companies use cloud services for their projects nowadays. These companies leverage the seemingly endless scalability and the elastic pricing model of the cloud. Nevertheless, many projects still cannot use cloud services for various reasons. These may encompass regulatory restrictions, data protection, or just having an existing on-premises infrastructure.While these are all valid causes, why not combine an existing on-premises infrastructure with the power of the cloud? This is where Azure Arc comes into play. Azure Arc allows to project on-premises infrastructure into Azure and then apply Azure services like Azure Monitor or Azure Policy on the on-premises infrastructure.This session shows how a Swiss company uses Azure Arc to manage an on-premises Kubernetes cluster. Using Azure Arc enables the development team to leverage the power of the cloud and provide a streamlined DevOps process. All this can be achieved without the cluster being accessible from the internet." }, { "title": "Video - Install Azure Arc on an on-premises k3s Cluster", "url": "/video-install-azure-arc-on-premises-k3s-cluster/", "categories": "Video", "tags": "Azure Arc, Kubernetes, k3s, Youtube, Video", "date": "2022-09-05 00:00:00 +0200", "snippet": "" }, { "title": "Secure Application Deployments in Azure Arc with Flux GitOps", "url": "/secure-application-deployments-azure-arc-flux-gitops/", "categories": "Kubernetes, Cloud", "tags": "Azure Arc, Flux GitOps, Application Deployment, Secure Deployments, Kubernetes, Configuration Management, k3s", "date": "2022-08-29 00:00:00 +0200", "snippet": "Azure Arc offers developers and administrators a seamless and robust GitOps process by leveraging the Flux extension, enabling efficient management and security.In my previous post “Manage your Kubernetes Resources with Kustomize” I delved into the utilization of Kustomize for generating configuration files tailored to your Kubernetes cluster and applications.Today, we will explore the utilization of Flux to deploy these meticulously crafted configurations to an on-premises k3s cluster. This integration of Azure Arc, Kustomize, and Flux empowers organizations with enhanced control and flexibility in their deployment workflows.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”.Understanding FluxFlux is a widely adopted open-source GitOps provider that serves as the GitOps operator for Azure Arc-enabled Kubernetes clusters. As an extension installed within your cluster, the Flux GitOps operator operates autonomously. It establishes an outbound connection, ensuring the security of your cluster without requiring any inbound connections. This modern approach empowers you to manage your deployments effectively.The Flux extension leverages Kustomize for configuring your deployments. If you would like to delve deeper into Kustomize, I invite you to refer to my previous post where I explored its intricacies and benefits. By combining the capabilities of Flux and Kustomize, you can streamline your deployment processes while adhering to best practices in the field of Kubernetes management.Introducing the Demo ApplicationYou can find the code of the finished demo application on GitHub.The demo application is designed with simplicity in mind and comprises a YAML file located in the repository’s root folder. This YAML file encompasses a Deployment and a Service, forming the core components of the application.In addition to the application YAML file, I have also included a kustomization file in my last post. This kustomization file instructs the Flux agent on which files to apply.By leveraging the code and the kustomization file, you can effortlessly deploy and manage the demo application using the power of Azure Arc, Flux, and Kustomize. Feel free to explore the GitHub repository for a comprehensive understanding of the demo application’s implementation.Installing the Flux ExtensionTo integrate the Flux operator as an extension with Azure Arc, you need to connect to the Master node of your on-premises Kubernetes cluster and execute the following command for installation:This command configures the installation of the Flux extension within the cluster-config namespace, granting access to the entire cluster through the scope parameter. It also sets up a Git repository and branch for Flux to interact with. The last line configures the kustomization configuration, specifying a name, path, and enabling the prune feature. When enabled, Kustomize will remove associated resources if their corresponding kustomization file is deleted, ensuring cluster cleanliness.The cluster-config namespace will contain a configuration map and several secrets used for communication with Azure. These secrets also enable connectivity to private Git repositories. In my next post, I will guide you on configuring a private Git repository as the source.The installation process may take a few minutes. Once completed, navigate to your Azure Arc resource, open the Extensions pane, and you should find the Flux extension listed there. The Flux extension has been installed If you review the installation output, you’ll notice an error indicating that the namespace was not found. The error message states: “namespace not specified, error: namespaces gitopsdemo not found.” The namespace was not found This error occurs because the YAML file specifies the gitopsdemo namespace, which hasn’t been created yet. Consequently, the application installation fails.To view the configuration of the previously installed GitOps operator, you can use the following command:Alternatively, in the Azure portal, navigate to the GitOps pane within your Azure Arc resource, click on the configuration, and if you used the same name, you should see “gitopsoperator.” Clicking on it will display the same error message as shown above in the CLI output.Let’s resolve this issue by deleting the current GitOps configuration and rectifying it. Execute the following command to delete the GitOps operator:Fixing the Failed DeploymentTo address the problem mentioned earlier, we need to create the namespace before deploying the application. Instead of adding the namespace directly to the existing YAML file, let’s adopt a more scalable approach. We’ll create separate folders for different deployment types, such as “App” and “Infrastructure,” and include a kustomization file in each folder. This organization ensures modularity, easy maintenance, and the ability to apply individual configurations.First, let’s create a YAML file in the “Infrastructure” folder to establish the new namespace:Next, open a terminal and execute the following commands to create a kustomization file in each folder:Make sure that you have the Kustomize CLI installed.To establish the dependency between the application deployment and the namespace creation, utilize the “dependsOn” attribute in the kustomization file:Wait a moment, and this time the deployment should succeed. In the Azure portal, open the GitOps pane within your Azure Arc resource, and you should observe the deployment status as “succeeded.” The deployment succeeded If you make any changes to your Git repository, such as modifying the image tag, the Flux operator will fetch and deploy those changes automatically.Check the pods in the gitops namespace to ensure that one pod of the application is running there. The application runs in the cluster The Flux operator ensures seamless updates whenever changes are made to the Git repository, providing a robust and automated deployment process.Deleting the GitOps ConfigurationWhen you delete the GitOps operator, the Azure CLI will prompt you to confirm the deletion. Additionally, it informs you that the prune flag has been enabled for this deployment. By confirming these messages, the GitOps operator and all associated resources will be deleted. In the case of this demo, it means that the namespace, application, and service will also be deleted. Once the deletion command completes, you will observe that no resources exist anymore. All resources got deleted Please note that although the prune flag is enabled, there might be instances where the prune operation doesn’t function as expected. At present, the cause of this behavior remains unknown, and there are no available logs or error messages for further analysis.Using Private Repositories for GitOpsIn the demo, a public Git repository was used to host the deployment files. However, in enterprise scenarios, applications are typically hosted on private Git repositories. In the upcoming post, I will guide you on securely connecting to a private repository in Azure DevOps or GitHub using SSH keys and the Flux GitOps operator.By leveraging SSH keys, you can establish a secure and authenticated connection between the GitOps operator and your private repository. This ensures the confidentiality and integrity of your source code and deployment configurations. I will provide step-by-step instructions in my next post to guide you through the process of configuring SSH keys and integrating them with the Flux GitOps operator.Stay tuned for my next post, where I will delve into the details of securely connecting to private repositories and enabling seamless GitOps workflows for enterprise applications.ConclusionIn summary, this article introduced GitOps with Flux in Azure Arc, simplifying and securing Kubernetes deployments. We explored Kustomize for flexible configuration management.By following a practical demonstration, we successfully deployed a demo application. Securely connecting to private Git repositories using SSH keys was also discussed. Embracing GitOps and Flux streamlines deployments, enhancing efficiency and security.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”." }, { "title": "Manage your Kubernetes Resources with Kustomize", "url": "/manage-kubernetes-resources-with-kustomize/", "categories": "Kubernetes, Cloud", "tags": "Kubernetes, Kustomize, Flux, Azure Arc, GitOps, Deployment, Configuration Management", "date": "2022-08-22 00:00:00 +0200", "snippet": "Azure Arc presents the Flux extensions as a powerful facilitator of the GitOps workflow. However, prior knowledge of Kustomize becomes indispensable for leveraging the full potential of the Flux extension.Today, our objective is to shed light on the essence of Kustomize and its profound impact on creating meticulous configuration files for your applications.Furthermore, I will guide you through the process of deploying your application using the Kustomize CLI, equipping you with the necessary expertise in application deployment. Let’s dive right in and explore the wonders of Kustomize!This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”.Kustomize - Kubernetes Native Configuration ManagementKustomize is powerful open-source tool designed to customize your Kubernetes configuration seamlessly. Unlike Helm, Kustomize takes a different approach by eliminating the need for templates to manage your configuration files. Notably, Kustomize has become an integral part of kubectl since version 1.14, allowing you to leverage its capabilities without any additional installations.When you unleash Kustomize on your folders, it diligently scans and compiles a comprehensive kustomization file. This file acts as a centralized hub, capturing vital information and references to all the YAML files discovered during the scanning process. Once you have your kustomization file in hand, you can effortlessly apply it to your Kubernetes cluster, bringing your configuration to life.To witness the power of Kustomize firsthand, let’s create a demo application and observe Kustomize in action.Deploy an Application to Kubernetes with KustomizeYou can find the code of the finished demo application on GitHub.Before you get started, install Kustomize.First, create a YAML file that contains a namespace definition:Next, open your command line and navigate to the folder containing the namespace YAML file. Use Kustomize to scan the folder and generate a kustomization file based on the detected YAML files.The –autodetect flag tells Kustomize to search for Kubernetes resources, while the –recursive flag ensures that sub-folders are also included in the search.The generated kustomization file will look like this:This is a simple kustomization file that references the previously created namespace file. It’s sufficient for an initial deployment. You can combine the build of the kustomization file and its deployment using the following command:After applying the kustomization file, you should see the newly created namespace. The namespace was created Flux with KustomizeIn the world of Azure Arc, Flux serves as the GitOps operator, seamlessly integrating with Kustomize for efficient deployment configuration. As we delve into the realm of Azure Arc, let’s explore the powerful combination of Flux and Kustomize.Kustomize offers a wide range of configuration parameters that enable fine-grained control over your deployments. Flux allows you to leverage these parameters during the creation of the Flux operator or incorporate them directly into the kustomization file. To better understand this integration, let’s examine an example kustomization file:In this example, the kustomization file showcases various configuration parameters. The interval specifies the frequency at which Flux checks for configuration drifts and removes changes made outside of Kustomize. The wait parameter ensures that Flux waits until all resources are ready before proceeding. The timeout defines the duration after which Kustomize aborts the operation if it exceeds the specified time. The retryInterval determines the interval between retry attempts. The prune flag enables the removal of stale resources, while the force flag allows for the recreation of resources if necessary. The targetNamespace specifies the namespace where the resources are deployed.Additionally, the sourceRef section defines the Git repository URL, the secretRef references a user PAT (personal access token) for accessing the repository, and the branch indicates the branch from which Kustomize fetches the resources.It’s worth noting that you can also configure Kustomize to use a local file instead of a URL by using the path parameter and specifying the folder path where your kustomization file resides.With Flux and Kustomize working in harmony, you have the flexibility to tailor your deployments and fine-tune the configuration to meet your specific needs. The combination of GitOps principles with the versatility of Kustomize empowers you to achieve a robust and streamlined deployment workflow within the Azure Arc ecosystem.ConclusionIn summary, Flux and Kustomize form a powerful duo within the Azure Arc ecosystem, enabling a streamlined and automated GitOps workflow for managing Kubernetes deployments. With Flux serving as the GitOps operator and Kustomize providing flexible configuration management, you can achieve consistency, reproducibility, and efficiency in deploying and managing your applications.By leveraging their integration, you empower your team to automate deployments, detect configuration drifts, and maintain the desired state of your Kubernetes resources. Embrace the combined capabilities of Flux and Kustomize to enhance the reliability and scalability of your Azure Arc deployments, unlocking the full potential of GitOps within your Kubernetes environment.In my next post, I will show you how to use Flux on Azure Arc to deploy your resources to Kubernetes with the help of Kustomize.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”." }, { "title": "Create a VPC Connection between Azure and Timescale", "url": "/create-vpc-connection-between-azure-and-timescale/", "categories": "Cloud", "tags": "PostgreSQL, Timescale, Azure, VNet peering, VNet, Azure CLI", "date": "2022-08-15 00:00:00 +0200", "snippet": "During the course of this week, I had the opportunity to assist one of my clients in the complex process of migrating their on-premises database to the highly sophisticated cloud-hosted TimescaleDB, a cutting-edge solution renowned for its exceptional performance and scalability. Given the client’s preference for the Azure platform, they expressed their desire to deploy the new TimescaleDB on Azure infrastructure.One remarkable feature offered by Timescale is the provision to host the database on Azure while also enabling a secure and private connection through Virtual Private Cloud (VPC) peering. This ingenious approach allows customers to establish a direct peering connection between their own Virtual Network (VNet) and the Timescale’s VNet, thus confining inbound traffic exclusively to this designated VNet. Consequently, it effectively restricts any access to the database from the vast expanse of the internet.Regrettably, I encountered a few challenges while attempting to establish the VPC peering, necessitating a detailed description of the troubleshooting and eventual success I achieved in this endeavor.IntroductionUpon conducting extensive research and reaching out to the responsive Timescale support team, I fortuitously stumbled upon an article, “Setting up a VPC on Azure”, within the Timescale knowledge base. This article delves into the intricacies of setting up a Virtual Private Cloud (VPC) on Azure, offering invaluable insights for individuals embarking on a similar journey.As I diligently followed the guide, I encountered a few stumbling blocks. Firstly, I encountered issues with certain commands in the Azure Command-Line Interface (CLI), where they failed to execute as expected. Secondly, I observed that the guide employed a rather convoluted approach, needlessly complicating the process. Specifically, in instances where a query yielded a value, the guide instructed users to manually extract and assign the value to a variable, rather than automating this assignment process.Follow this guide closely, as it will reveal the shortcomings and discrepancies present within the official documentation. By doing so, you will not only identify the specific areas that require attention and revision, but you will also obviate the need for manual variable assignment. The guide ensures that output values are automatically assigned, alleviating the burden of manual intervention.Let’s get startedBefore we delve into the technical aspects of the VPC peering, it is important to note that this article assumes you have already set up a TimescaleDB with VPC in Azure. If this is not the case, log into your Timescale account and proceed to create a new VPC within your desired Azure region. Please wait until the VPC reaches an active state before proceeding to create a new database, ensuring that the previously created VPC is designated as its location. Note that accessing this database will not be possible until the VPC peering process is completed. The connection to the TimescaleDb was refused To aid in our exploration, we shall primarily rely on the command line interface, utilizing PowerShell Core, the Azure CLI and Aiven CLI on an Ubuntu environment. While it is theoretically feasible to create a VPC peering through the Timescale portal, this approach fails to encompass all the essential steps, rendering it ineffective for our purposes.Let us now embark on this command line journey, as we unveil the intricate steps required to establish a successful VPC peering for your TimescaleDB.Create an Application in your AADTo proceed with the VPC peering setup, we will first create an application in your Azure Active Directory (AAD). Please follow the steps outlined below:First, log into your Azure account using the Azure CLI and ensure that you have set the desired subscription (if applicable). Use the following command:Once you are logged in and have set the appropriate subscription, execute the following command to create an application in your AAD. You can choose any display name you prefer:Please note that in the above command, we have included the –sign-in-audience AzureADandPersonalMicrosoftAccount flag, which allows multiple tenants to log into this application. However, it is important to clarify that only your specific tenant possesses the necessary credentials for authentication.At this juncture, it is crucial to highlight a notable discrepancy in the official guide. The guide suggests using the –available-to-other-tenants flag, which, to the best of my knowledge, does not exist and consequently will not function as intended. Instead, please ensure that you utilize the –sign-in-audience flag as specified above.Create a Service PrincipalIn the next step, we will create a service principal for the AAD application that we previously established. This service principal will play a crucial role in the subsequent process of peering your Virtual Network (VNet) with Timescale’s VNet. Please follow the instructions below:Execute the following command to create the service principal:By creating this service principal, you are effectively generating the necessary credentials that will facilitate the VPC peering between your VNet and Timescale’s VNet. This step is vital to ensure the successful establishment of the desired connection.Set a Password for the ApplicationTo ensure smooth authentication and secure access to Azure using the AAD application we created earlier, we need to set a password for the application. This password will serve as the necessary credential for logging into Azure. Please follow the steps outlined below:Execute the following command to reset the password for the AAD application:Upon executing this command, a warning will appear, cautioning you to exercise care as the output will contain sensitive credentials. However, in this case, we can safely ignore the warning, as obtaining the password for the Azure login is the exact purpose of this action.By setting the password for the application, we are ensuring a secure and authenticated access point to Azure. With this vital step complete, we are now prepared to proceed with the subsequent stages of the VPC peering process.Gather Information about your VNet, Resource Group, and SubscriptionIn order to proceed with the VPC peering configuration, we need to gather some essential information about your Virtual Network (VNet), the associated resource group, your subscription ID, and your tenant ID. Please follow the instructions below and replace the placeholders for “YOUR_VNET_NAME” and “YOUR_PEERING_NAME” with your specific values:Assign the Network Contributor Role to the Service PrincipalTo enable the service principal, which we created earlier, to create the peering from your VNet to the Timescale VNet, we need to assign the Network Contributor role to the service principal. This role will provide the necessary permissions for network-related operations.Execute the following commands to query the ID of the Network Contributor role and assign it to the service principal:The –scope flag ensures that the role assignment is limited to the VNet resource, providing the necessary level of access for the service principal to establish the VPC peering connection.Create a Service Principal for the TimescaleDB applicationTo establish a peering connection between your Timescale VPC project and your VNet, the Timescale Active Directory (AD) application requires a service principal in your Azure subscription. Execute the following code to create the service principal for the TimescaleDB application:Please note that you need to have the “Application Administrator” permission to execute the code provided. If you encounter an error message stating “Insufficient permissions to create the service principal,” it means you do not have the necessary permissions. Insufficient permissions to create the service principal Create a Custom Role for the Network PeeringTo provide the Timescale application with the necessary permissions to create the network peering, it is recommended to create a custom role that includes only the “Microsoft.Network/virtualNetworks/peer/action” permission. This ensures that you grant the least amount of permissions required for the task.First, save the following code as a JSON file, e.g., customRole.json. Replace &lt;YOUR_SUBSCRIPTION_ID&gt; with your actual subscription ID, and if desired, modify the name of the role:Next, execute the following command to create the custom role using the JSON file:Lastly, assign the custom role to the previously created Timescale service principal using the following command:Create an Access Token for the Aiven CLITo create an access token for the Aiven CLI in the Timescale portal, follow the steps below: Log into your account in the Timescale portal. Click on your account icon located on the top right corner of the page. Switch to the “Authentication” tab. Click on the “Generate token” button. Generate a new access token This will open a new window where you can enter a name for your access token and configure its expiration time. Configure the access token After setting the token name and expiration time, click on the “Generate token” button. The access token will be generated, and you should see it in the generated token list. Copy the access token Make sure to copy the access token at this point, as it will not be visible again. Safely store the access token as it will be required for authentication when using the Aiven CLI.Additionally, it is highly recommended to enable two-factor authentication (2FA) in your account settings for enhanced security. Enabling 2FA adds an extra layer of protection to your account by requiring an additional verification step during the login process.Install the Aiven CLITo install the Aiven CLI and log into your Timescale account, follow these steps:First, you need to install Python and pip if they are not already installed on your system. Use the following commands to install them:Once Python and pip are installed, you can proceed to install the Aiven CLI. Use the following command to install it:After the Aiven CLI is installed, you can log into your Timescale account using the CLI. Replace &lt;YOUR_USER&gt; in the following command with your Timescale username:This command will prompt you to enter your access token. Paste the access token that you previously generated in the Timescale portal and press Enter.Once you have successfully logged in, you are ready to use the Aiven CLI for further configuration steps.Retrieve the VPC Project IDTo retrieve the VPC Project ID, you can use the following command:The following code works because there is only 1 VPC per region. Replace timescale-azure-switzerland-north with your location if you are not using Switzerland North. The project ID is always 36 characters long, as far as I know.Create Peering ConnectionThe VPC project id and some of the previously created variables are now needed to create a peering connection from the Timescale VNet into your VNet. Use the following code to create the connection:Note that the input variables starting with user_ must be lowercase since the Aiven API can only handle lower-case inputs.Check the Peering Status and Assign VariablesCreating the peering connection might take some time but usually, it is only a couple of seconds. Before you proceed, make sure with the following command that the state of the connection is ACTIVE. Check the peering state If the state is not ACTIVE, wait a bit and retry the command from above. If the state is INVALID_SPECIFICATION or REJECTED_BY_PEER, check that VNet you passed as a parameter exists and that the Timescale application was given the proper permissions. After you checked that, repeat the command from above and the peering connection should be created.The output of this command is horrendous but you have to copy the value of to-tenant-id and to-network-id to the variables aiven_vnet_id and aiven_tenant_id. Make sure to replace the placeholder with your actual values.The tenant Id is a GuId and the VNet id should look something like “/subscriptions/&lt;SUBSCRIPTION_ID}&gt;/resourceGroups/…Create the VNet peering from your VNet to your VPCLog out of the Azure CLI and then log in with your service principal into your tenant and also into the Timescale tenant.After you successfully logged into both tenants, create the VNet peering between your VNet and the Timescale VNetYou can check the state of your peering connection with the following command:After creating the peering connection, the initial state will be APPROVED, and after a few seconds, it should transition to PENDING_PEER. The peering was approved and is pending Wait for a while, and the peering state should switch to ACTIVE. The peering is active If you follow the official documentation, note that the command to check the state is cut off and won’t work.Test the ConnectionWith the peering connection in place, test if you can log into your Timescale database. Make sure that you are connecting from within your peered VNet, otherwise, the connection won’t work. The TimescaleDb login worked Note that I used pgAdmin to test the connection despite using Azure Data Studio at the beginning of this tutorial. The PostgreSQL extension of Azure Data Studio was constantly crashing and I could not get it working again.Clean up your ResourcesIf you do not need your resources anymore, make sure to clean up everything you have created.Delete the VNet PeeringFirst, delete the VNet peering with the following command.Delete both Role BindingsNext, retrieve the Ids of the role bindings and then delete both.Delete the Custom RoleAfter you deleted the role binding, delete the previously created custom role.If you used a different name for the custom role, make sure to replace “VnetPeerCreator” with your name.Delete the Applications from your AADLastly, delete both applications from your AAD.Note that deleting an application from AAD is an irreversible action, and it permanently removes the application and its associated resources. Make sure to double-check the application IDs before executing the delete commands to avoid deleting the wrong applications. Also, ensure that you have the necessary permissions (“Application Administrator”) to delete the applications.ConclusionIn conclusion, this guide demonstrated how to create a private peering connection between your Azure subscription and a Timescale database. By establishing a direct and secure network connection, you can access your database from within your Azure resources while avoiding exposure to the public internet.This approach enhances security, reduces latency, and simplifies network architecture. By following the provided steps, you successfully set up the peering connection and tested its functionality." }, { "title": "Introducing GitOps", "url": "/introducing-gitops/", "categories": "Kubernetes, DevOps", "tags": "GitOps, IaC, Flux, ArgoCD, Git, DevOps", "date": "2022-08-08 00:00:00 +0200", "snippet": "GitOps is a way to manage infrastructure as code (IaC) which gains more and more traction lately.Today, I want to give you an introduction to GitOps, talk about the good and bad, and also introduce some GitOps tools you can use to get started.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”.What is GitOps?Infrastructure as code is nothing new and there are many popular tools such as Terraform, Pulumi, Ansible, and many more. IaC means that you have the configuration of your infrastructure in a file, mostly a YAML file. The advantage is that you can take this file and deploy it as often as you want. This enables you to have fast and repeatable deployments.With the rise of Kubernetes and cloud environments, we see more and more topics as code, such as network as code, configuration as code, or security as code. This allows developers to have everything as code. All these configuration files also come with more complexity. This is where GitOps comes into play.GitOps usually uses a separate Git repository that contains all your configuration files. This already allows developers to have a single source of truth and also allows them to create pipelines that deploy these configuration files. Therefore, you should always know what version of your configuration is running in a given environment. Additionally, since the changes are deployed via a pipeline, the developers don’t need access to the infrastructure anymore which will lead to increased security.Deployment ModesGitOps knows two different deployment modes: pull-based deployments push-based deploymentsAs the name suggests, the push-based deployment mode pushes your changes into the configured environment. This is the default way of deploying changes for most developers since this is exactly what a CD pipeline does.On the other hand, a pull-based deployment has an agent or operator running in your environment. This agent monitors a configured Git repo and branch for changes and if there are changes, pulls these changes and executes them. The advantage of this approach is that your environment can block all incoming traffic and only has to allow outgoing traffic on port 443.Azure Arc uses the pull-based approach and I will talk about this approach in more detail in a later post of this series.Benefits of GitOpsApplying GitOps to your deployment process has many advantages, such as: IaC files are checked into your version control. Run automated tests on the configuration files, for example, check if the YAML files are valid. Enforce pull requests for changes to increase the quality of your configuration files and share knowledge at the same time. Use CD pipelines for your deployment. Therefore, you will know what version of the configuration is installed in your environment and also allows for easy rollbacks in case of a problem. The Git repository is your single source of truth. You will have higher security since only the CD pipeline needs access to your environment.Disadvantages of GitOpsAs with every tool or feature, there are some downsides to consider: You will have to manage more Git repositories. The code might be less flexible, especially when using the pull approach. For example, you will need configuration files for each environment instead of changing variables during the deployment as you would do in a CD pipeline. There is no solution for the secret management and you will have to rely on an outside tool such as Azure Key Vault for your secrets and passwords.GitOps ToolsThere is a wide variety of tools available if you want to use the push deployment model for your configuration: Terraform Pulumi Ansible Chef Azure CLISince the pull model is a newer way of deploying your configurations (and applications), there are fewer tools available currently. The most used ones are: Flux (Flux is used in Azure Arc and will be used to deploy applications in a later post) ArgoCDConclusionGitOps takes the DevOps approach and extends it to your configuration files. This will allow developers to have everything configuration related in code files such as YAML files and will also increase the security since the developers don’t need to access the infrastructure directly anymore.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”." }, { "title": "Use the TokenRequest API to create Tokens in Kubernetes 1.24", "url": "/use-the-tokenrequest-api-to-create-tokens-in-kubernetes/", "categories": "Kubernetes, Cloud", "tags": "Kubernetes, k3s, Rancher, On-premises, Azure Arc", "date": "2022-08-01 00:00:00 +0200", "snippet": "In one of my former posts, Securely connect to an on-premises Kubernetes Cluster with Azure Arc, I showed you how to securely connect to an on-premise Kubernetes cluster using Azure Arc. To achieve that, I have create a user and then retrieved the token of the user.This method has changed a bit from Kubernetes version 1.24 on and in this post, I will show you how to get this token if you are using the new K8s version.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”.Breaking Change with Tokens in Kubernetes 1.24When I described how you can connect to your Kubernetes cluster, I showed you how to create a new service account and bind a role to this account. See the following code as a reminder.Back then I said that this cluster binding also creates the secret which we will retrieve and then use to connect to the cluster. The code to do that was as following:From Kubernetes 1.24 on, this token is not automatically created anymore. Therefore if you execute the code above, the $SECRET_NAME variable will be empty and as a result you won’t be able to retrieve the token (obviously since it was not created in the first place).What is new in Kubernetes 1.24As with every new Kubernetes version, there are many changes and new features associated with the release. You can find all the changes and also upgrade notes in the Kubernetes changelog on GitHub. The LegacyServiceAccountTokenNoAutoGeneration feature gate is beta, and enabled by default. When enabled, Secret API objects containing service account tokens are no longer auto-generated for every ServiceAccount. Use the TokenRequest API to acquire service account tokens, or if a non-expiring token is required, create a Secret API object for the token controller to populate with a service account token by following this guide.This means that you have to create the token yourself which I will show you in the next section.Use the TokenRequest API to create the TokenKubernetes 1.22 introduced the TokenRequest API which is now the recommended way to create tokens because they are more secure than the previously used Secret object. To use the API, first create a new service account and bind a role to it. Then use “kubectl create token &lt;Service Account Name&gt; to create the token.The create token command automatically creates the token and prints it to the console. Use the TokenRequest API Use a Service Account Secret Objects to access the K8s ClusterIf you can’t use the TokenRequest API, then you can create the secret yourself. To create the access token in Kubernetes 1.24 manually, you have to create a service account and bind a role to this account first. This is the same code as in my last post:Next, add a new secret to your cluster with the following code:Make sure that the annotation “kubernetes.io/service-account.name:” has the name of the previously created service account as value. After the secret is created, Kubernetes automatically fills in the information for the token.Assign the value of the name in the previously created secret to the SECRET_NAME variable and then use following code to read the automatically created value of the token and then print it to the console:The following screenshot shows the whole process and the printed token. Create a new service account and print its token When you delete the service account, the associated secret will be automatically deleted too.ConclusionKubernetes 1.24 removed the automated creation of secrets when creating new service accounts. Use the TokenRequest API instead to create tokens which is easier and also more secure than creating tokens manually.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”." }, { "title": "Programming With Wolfgang is now on Youtube", "url": "/programming-with-wolfgang-youtube-channel/", "categories": "Video", "tags": "Youtube, Video", "date": "2022-07-25 00:00:00 +0200", "snippet": "As the title already mentions, I have created a Youtube channel and uploaded my first video explaining how to install Azure Arc to manage an on-premise Kubernetes cluster.I am not sure yet where this channel will go but for now, I will try to bring you the same educational content as I do on my blog.If you want to share or introduce a topic, feel free to text me. I am happy to host guests and talk tech." }, { "title": "Speaking about KEDA at the Kubernetes Community Days 2022 in Berlin", "url": "/speaking-about-keda-at-the-kcd-berlin-2022/", "categories": "Speaking", "tags": "KEDA, Speaking, Public Speaking, Conference", "date": "2022-06-20 00:00:00 +0200", "snippet": "I am excited to announce that I will be speaking at the Kubernetes Community Days 2022 in Berlin. My session will be on Wednesday, July 29 live on stage. If you are in Berlin, check out my talk and say hi.The abstract for my session is as follows:Level Up your Kubernetes Scaling with KEDAWhether it is a normal workday or Black Friday, service-oriented applications must be able to handle varying loads. This is the only way to ensure that users are provided with a good experience and that costs are kept to a minimum.Kubernetes offers a way to vary the number of application instances running based on CPU or RAM utilization with the Horizontal Pod Autoscaler. However, modern applications often depend on a variety of components and should be able to respond to external events. These may include new messages in a queue or metrics in Azure Monitor.As an application developer or operation manager, what do I need to consider to ensure that my application can respond to these events? How can I configure Kubernetes for “scale to 0” to run my application only when needed?Using Azure Kubernetes Service and KEDA (Kubernetes Event-driven Autoscaling), this session will show with practical examples how to create and configure autoscalers to respond to external events and scale applications in Kubernetes accordingly.You can find the session on the conference website: Level Up your Kubernetes Scaling with KEDA." }, { "title": "Monitor an on-premises k3s Cluster with Azure Monitor and Azure Arc", "url": "/monitor-on-premises-k3s-cluster-with-azure-monitor-and-azure-arc/", "categories": "Kubernetes, Cloud", "tags": "Kubernetes, k3s, Rancher, On-premises, Azure Arc, Monitoring, Azure Monitor", "date": "2022-06-13 00:00:00 +0200", "snippet": "Azure Arc allows you to project your on-premises Kubernetes cluster into Azure. Doing so enables you to manage the cluster from Azure with tools such as Azure Monitor or Cloud Defender.Today, I want to show you how to install the Container Insights Extension which enables you to monitor your pods and nodes from the on-premise cluster in Azure.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”.Install the Azure Monitor ExtensionUsing Azure Monitor with your on-premises Kubernetes cluster is surprisingly easy. All you have to do is to execute the following Azure CLI command on the Master node of your cluster:The parameter of the command should be self-explanatory. The most interesting one is probably the –name parameter. This Azure CLI command creates the namespace you defined with the –name parameter but it is empty at first glance. It creates an Azure Monitor Agent Deployment and ReplicaSet in the kube-system namespace though. The newly created namespace contains a config map and some secrets to ensure a safe communication with Azure. Display the Azure Monitor Agent pods Additionally, the –name parameter defines the name of the extension which you can find in the Azure Portal in the Extensions pane. The Azure Monitor Extension in the Azure Portal The Azure CLI command automatically creates a new Log Analytics Workspace for the metrics and logs of the extensions. You can also use an existing Work Analytics Workspace. Use the following command to assign the Log Analytics Workspace Id to a variable and then use this variable for the Azure Monitor extension:You can also display the installed extensions using the Azure CLI with the following command:Create Dashboards in the Azure PortalAfter you have installed the extension, it collects metric information and sends them to Azure. This allows you to use Azure Monitor the same way as you would use it with Azure VMs. Open Azure Arc in the Azure Portal and navigate to the Insights pane. There you can see various dashboards already. You can change what you want to display and also switch the scope, for example, from the Cluster scope to the Container scope. Additionally, you can set various filters such as a time range. Display various dashboards in the Azure Portal For even more insight into your cluster or pods, open the Metrics pane in Azure Arc. There you can create charts and display useful information. The following screenshot shows a chart that displays the pod count and the used CPU percentage of all nodes. Create custom charts to display information Another neat feature of Azure Monitor is Alerting. Go to the Alerting pane and there you can create alerts based on custom rules. For example, you could send an email to an administrator if the CPU usage of the cluster is greater than 80% over 5 minutes.ConclusionMonitoring your on-premise Cluster is as easy as it could be with Azure Arc. All you need is a single Azure CLI command to install the Azure Monitor extension. This extension collects various metrics and sends them to Azure. There, you can create dashboards or alerts. All this works the same way as when using Azure Monitor with Azure VMs.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”." }, { "title": "Securely connect to an on-premises Kubernetes Cluster with Azure Arc", "url": "/securely-connect-to-on-premises-kubernetes-with-azure-arc/", "categories": "Kubernetes, Cloud", "tags": "Kubernetes, k3s, Rancher, On-premises, Azure Arc, RBAC", "date": "2022-06-06 00:00:00 +0200", "snippet": "In my last post, I installed Azure Arc which allowed me to project my k3s cluster into Azure. The installation was done directly on the Master node of the cluster and developers would also need to connect to the master node to execute any commands on the Kubernetes cluster.Today, I want to show you how to give developers access using RBAC (Role-based access control) and let them connect to the Kubernetes cluster through Azure Arc.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”.Create a User on the Kubernetes ClusterTo authorize a user to access the Kubernetes cluster, you first have to create a user account and then give this user permissions using the kubectl cluterrolebinding command. Use the following command on the Master node to create a new admin user and give this user the cluster-admin role:This command additionally creates a secret for the user that contains a JWT token. You can read the token with the following command and then print it to the console:The following screenshot shows all the commands and also the printed token: Create an user and print the token to the console Copy the token as you will need it to access the Kubernetes cluster through Azure Arc.Access the k3s Cluster in the Azure Portal with Azure ArcWhen you open the Azure Arc resource in the Azure Portal and go to any Kubernetes resources pane, you will see a message that you have to sign in to view the Kubernetes resources. Sign in to view your Kubernetes resources Paste the previously created token into the text box and click Sign in. Now you should see the resources of the Kubernetes cluster. Display Kubernetes resources in Azure Arc Access the k3s Cluster from a Developer Computer with Azure ArcUsing the Azure Portal to access the Kubernetes cluster is nice but as a developer, I am used to using kubectl or any custom dashboards. To access the Kubernetes cluster from my Windows computer, I will use the following Azure CLI command.Replace &lt;TOKEN&gt; with the previously created token. You can use this command on any computer as long as the Azure CLI is installed. The command downloads the Kubernetes config file, sets the context, and creates a proxy connection through Azure Arc to the Kubernetes cluster. Create a connect to the Kubernetes Cluster After the connection is established, open a new terminal window and use kubectl as you are used to. It is also possible to use any dashboard to display the resources from the Kubernetes cluster. I like to use Octant from VMWare but you can use whatever dashboard you feel comfortable. For more information about Octant and how to install it, see “Azure Kubernetes Service - Getting Started” Access the Kubernetes Cluster with a dashboard ConclusionUsing Azure Arc enables you to access an on-premises cluster securely from your machine or the Azure Portal. All you have to do is to create a user on the Kubernetes cluster and give this user the desired permissions. Then retrieve its access token and use this token to connect to the cluster.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”." }, { "title": "Install Azure Arc on an On-premises k3s Cluster", "url": "/install-azure-arc-on-premises-k3s-cluster/", "categories": "Kubernetes, Cloud", "tags": "Kubernetes, k3s, Rancher, On-premises, Azure Arc", "date": "2022-05-30 00:00:00 +0200", "snippet": "In my last post, I have created an on-premises k3s cluster and introduced the requirements of this project. One of the requirements/restraints of this project is that the k3s cluster sits behind a firewall that blocks all incoming traffic.Today, I will show you how to install Azure Arc which will help to manage the cluster with Azure, even though it is not accessible over the internet.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”.Install Azure CLI and prepare the Cluster for Azure ArcThe installation of Azure Arc will be executed on the master node of the k3s cluster. To install Azure Arc, you have to install the Azure CLI and the connectedk8s extension first. Use the following commands to install the CLI, the extension, and then login into Azure:I prefer using the –use-device-code parameter for the az login command because I had some problems when trying to log in on a remote machine. Open the displayed URL and enter the code. You will be logged in a couple of seconds later. Install the Azure CLI and login into Azure Another step you have to take before installing Azure CLI is to register 3 resource providers. They are necessary for Azure Arc to be installed. Registering them might take up to 10 minutes. You can use the following commands to register the resource provider and also check if the registration is finished. Register Service Provider in Azure Make sure that the RegistrationState is Registered for all 3 resource providers. There is one last step you have to take after the resource providers are registered. You have to move the config file of the Kubernetes cluster. Azure Arc expects the config file under /root/.kube/config whereas k3s installs the config under /etc/rancher/k3s/k3s.yaml. Use the following command to move the config file to the right location:In case you get the error message “mv: cannot move ‘/etc/rancher/k3s/k3s.yaml’ to ‘/root/.kube/config’: No such file or directory”, create the /.kube folder using the following command:After moving the config file, install Azure Arc with the following command:Make sure to use a location close to your on-premises infrastructure. Currently, not all Azure locations support Azure Arc. The Azure CLI will tell you if you selected an unsupported region and what regions are available right now. The name parameter configures the name of the Azure Arc instance. Install Azure Arc Check the Azure Arc InstallationThe installation takes around 5-10 minutes. In the past, I often got disconnected and had to reconnect. Use the following command to check if the Azure Arc installation is finished:This command prints all Azure Arc instances in the provided resource group to the console. You should see the previously created Azure Arc instance. Azure Arc got installed You can also see the Azure Arc instance in your resource group in the Azure Portal. Click on it and you will see some information about your cluster like the Kubernetes version or what distribution you are using (I updated the k3s cluster since the last post and therefore have a newer version now). Azure Arc in the Azure Portal Azure Arc installs all the needed applications in the newly created azure-arc namespace. These applications manage the connection to Azure, authentication to Azure Active Directory, and collect metrics that can be displayed in dashboards in the Azure Portal. Applications in the azure-arc Namespace Back in the Azure Portal, click on any of the Kubernetes resources panes on the left and you will see that you need to sign in to view the Kubernetes resources. Sign in to view your Kubernetes resources In my next post, I will show you how to authenticate a user to sign in to the Kubernetes cluster using Azure Arc.VideoHere is the blog post as video:ConclusionAzure Arc can be easily installed using the Azure CLI and allows you to project your cluster into Azure. This means that you can see information such as the K8s version and the distribution in the Azure Portal. To do that, Azure Arc installs a couple of applications in the azure-arc namespace.In my next post, I will show you how to authenticate a user to sign in to the Kubernetes cluster using Azure Arc.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”." }, { "title": "I am back", "url": "/i-am-back/", "categories": "Miscellaneous", "tags": "Azure Arc", "date": "2022-05-23 00:00:00 +0200", "snippet": "For the first time in two years, I have taken some time away from my blog. This doesn’t mean that I have been lazy though. First, it was nice to take a step back and take care of my (mental) health. and then I talked at a conference in Germany about KEDA. It was nice to travel and especially be at an in-person conference. It’s been quite some time since the last time.I also started to write regularly for the german Windows Developer magazine. First I wrote about KEDA and now have just finished the second article of a three-part series about Azure Arc. Besides writing these articles, I started to prepare for the Microsoft AZ-204 certification. This certification is necessary for the AZ-400 - Designing and Implementing Microsoft DevOps Solutions certification which I want to do as well soon.The biggest accomplishment of my time away was finally finishing my Microservice Series - From Zero to Hero series. I took a lot of time to check all 65 blog posts and made some quality improvements.I will be back on my regular schedule next week focusing on a new series about Azure Arc. Azure Arc is an amazing service that allows you to manage your infrastructure, no matter where it runs, on-premises, or at another cloud provider. It also allows you to run Azure App Services or managed SQL instances on-premises." }, { "title": "Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc", "url": "/manage-on-premises-kubernetes-with-azure-arc/", "categories": "Kubernetes", "tags": "Kubernetes, k3s, Rancher, On-premises, Azure Arc, Azure Monitor, CI/CD, Flux, GitOps", "date": "2022-02-21 00:00:00 +0100", "snippet": "Azure Arc is a great tool to manage your on-premises hardware with Azure. This series will focus on managing a Kubernetes (k3s) cluster and will show how to install Azure Arc, and how to use different Azure services to manage the cluster.Project Requirements and RestrictionsThe project for this demo has the following requirements and restrictions: Two on-premises Ubuntu 20.04 VMs Install and manage a Kubernetes distribution Developers must use CI/CD pipelines to deploy their applications A firewall blocks all inbound traffic Outbound traffic is allowed only on port 443 Application logging Monitor Kubernetes and Vms metrics Alerting if something is wrongThe biggest problem with these restrictions is that the firewall blocks all inbound traffic. This makes the developers’ life way hard, for example, using a CD pipeline with Azure DevOps won’t work because Azure DevOps would push the changes from the internet onto the Kubernetes cluster.All these problems can be solved with Azure Arc though. Let’s see how to implement all this requirements from start to finish. Azure Arc - Getting Started Install an on-premises k3s Cluster Install Azure Arc on an On-premises k3s Cluster Securely connect to an on-premises Kubernetes Cluster with Azure Arc Monitor an on-premises k3s Cluster with Azure Monitor and Azure Arc Use the TokenRequest API to create Tokens in Kubernetes 1.24 Introducing GitOps Manage your Kubernetes Resources with Kustomize Secure Application Deployments in Azure Arc with Flux GitOps Deploy Applications from private Repositories using Flux GitOps and Azure Arc Coming soon: CD with Helm Charts using Flux Azure Key Vault integration Azure App Services running on on-premises infrastructure Azure Managed SQL instance running on on-premises infrastructure Azure RBAC to access the cluster tbd" }, { "title": "Install an on-premises k3s Cluster", "url": "/install-on-premises-k3s-cluster/", "categories": "Kubernetes", "tags": "Kubernetes, k3s, Rancher, On-premises", "date": "2022-02-14 00:00:00 +0100", "snippet": "Using cloud technologies is amazing and makes a developer’s life so much easier. Lately, I have to work with an on-premises Kubernetes cluster and I had to realize how much work it is to do all these things cloud providers offer. That’s the reason why I got into Azure Arc. In my last post, Azure Arc - Getting Started, I explained what Azure Arc is and how it can be used to manage on-premises resources.Today, I would like to get more practical and show you how to install an on-premises k3s cluster and in the next post, I will install Azure Arc to manage the cluster.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”.Project Requirements and RestrictionsMy project has the following requirements and restrictions: Two on-premises Ubuntu 20.04 VMs Install and manage a Kubernetes distribution Developers must use CI/CD pipelines to deploy their applications A firewall blocks all inbound traffic Outbound traffic is allowed only on port 443 Application logging Monitor Kubernetes and Vms metrics Alerting if something is wrongThe biggest problem with these restrictions is that the firewall blocks all inbound traffic. This makes the developers’ life way hard, for example, using a CD pipeline with Azure DevOps won’t work because Azure DevOps would push the changes from the internet onto the Kubernetes cluster.All these problems can be solved with Azure Arc though. Let’s start with installing a Kubernetes distribution and project it into Azure Arc.Installing an on-premises k3s ClusterSince I am a software architect and not really a Linux guy, I decided to use k3s as my Kubernetes distribution. K3s is a lightweight and fully certified Kubernetes distribution that is developed by Rancher. The biggest advantage for me is that it can be installed with a single command. You can find more information about k3s on the Rancher website.My infrastructure consists of two Ubuntu 20.04 VMs. One is called master and will contain the Kubernetes control plane and also serve as a worker node. The second VM, called worker, will only serve as a worker node for Kubernetes.To get started, connect to the master server via ssh and install k3s with the following command:There are several options to configure the installation. For this demo, the default is fine but if you want to take a closer look at the available options, see the Installation Options. Install k3s The installation should only take a couple of seconds. After it is finished, use the Kubernetes CLI, kubectl, to check that the cluster has one node now: The master node got installed If you want a very simple Kubernetes installation, you are already good to go.Add Worker Nodes to the k3 ClusterTo add worker nodes to the k3s cluster, you have to know the k3s cluster token and the IP address of the master node. To get the token, use the following command on the master node:The node token should look as follows: Get the node token Copy the token somewhere and then connect to the worker VM via ssh. Use the following command to install k3s on the VM and also add it to the cluster:Replace the URL with the URL of your master node and also replace the token with your token. Add a new k3s worker node Go back to the master VM and you should see two nodes in your cluster now: The worker node got added to the cluster You can repeat this process for every node you want to add to the cluster.ConclusionK3s is a fully certified, lightweight Kubernetes distribution developed by Rancher. It can be easily installed and is a great tool to get started with Kubernetes when you have to use on-premises infrastructure.In my next post, I will install Azure Arc and project the cluster to Azure. This will allow managing the cluster in the Azure portal.This post is part of “Azure Arc Series - Manage an on-premises Kubernetes Cluster with Azure Arc”." }, { "title": "Speaking about KEDA at the BASTA! Conference in Frankfurt", "url": "/speaking-about-keda-at-the-basta-conference/", "categories": "Speaking", "tags": "KEDA, Speaking, Public Speaking, Conference", "date": "2022-02-07 00:00:00 +0100", "snippet": "There is light at the end of the tunnel with the pandemic and therefore in person conferences and international travel is coming back. I am excited to travel to Germany and speak at the BASTA! conference in Frankfurt about KEDA.You can join the session in-person, or remote to learn more about KEDA. If you can’t make it to the conference but still want to learn more about KEDA, see KEDA - Kubernetes Event-driven AutoscalingThe abstract for my session is as follows:Level Up your Kubernetes Scaling with KEDAWhether it is a normal workday or Black Friday, service-oriented applications must be able to handle varying loads. This is the only way to ensure that users are provided with a good experience and that costs are kept to a minimum.Kubernetes offers a way to vary the number of application instances running based on CPU or RAM utilization with the Horizontal Pod Autoscaler. However, modern applications often depend on a variety of components and should be able to respond to external events. These may include new messages in a queue or metrics in Azure Monitor.As an application developer or operation manager, what do I need to consider to ensure that my application can respond to these events? How can I configure Kubernetes for “scale to 0” to run my application only when needed?Using Azure Kubernetes Service and KEDA (Kubernetes Event-driven Autoscaling), this session will show with practical examples how to create and configure autoscalers to respond to external events and scale applications in Kubernetes accordingly.You can find the session on the conference website: Level Up your Kubernetes Scaling with KEDA." }, { "title": "Introducing Kubernetes Services", "url": "/introducing-kubernetes-services/", "categories": "Kubernetes", "tags": "Kubernetes", "date": "2022-01-31 00:00:00 +0100", "snippet": "Pods can be deleted and recreated at any time, making them a so-called nonpermanent resource in Kubernetes. Therefore, the IP address of a pod will likely change regularly. This behavior will cause problems when you have a resource, for example, a DNS record, pointing to your application. It will be impossible to route to the application since the IP address of the pods will always change.This is why Kubernetes introduced the Service resource.Kubernetes Service ResourceA Service provides a permanent endpoint for your application. This means that its IP address never changes. Therefore, you can point a DNS record or another application to the Service. The Service then redirects the traffic to a pod of the associated application. The Service can access the pods because it uses selectors and not an IP address.Let’s have a look at a simple Service definition:This service publishes port 80 and redirects the request to port 8080 of a pod with the label “app=my-application” The targetPort property is optional. The Service redirects the request to the same value as port if targetPort is not set.Services can expose multiple ports, for example, one for HTTP and one for HTTPS. When using more than one port, you have to provide a name for each port, as shown in the following example:Service TypesCurrently, there are four service types in Kubernetes: ClusterIP LoadBalancer NodePort ExternalNameClusterIPClusterIP is the default type of a Kubernetes service. This type exposes the service on a Kubernetes internal IP Address and port. This means that only applications inside the Kubernetes cluster can access this service.LoadBalancerThis type exposes the service externally using a cloud provider’s load balancer. For example, if you use the LoadBalancer Service in an Azure Kubernetes Service cluster, Azure will create a load balancer in Azure and route the requests to a NodePort and ClusterIP Service. These services are created automatically.NodePortNodePort is a service type that I have only used in on-premises clusters. This service type exposes the service to the outside and also creates a ClusterIP Service for internal requests. The default range of available ports for NodePort is 30000-32767. This property can be configured by using the –service-node-port-range flag.ExternalNameThis type maps the service to external fields like (foo.bar.com) and returns a CNAME record. I have never used this service, therefore I can’t talk much about it. All I know is that you need at least kube-dns version 1.7 or CoreDNS 0.0.8 or higher to use the type ExternalName.ConclusionThis post gave a very simple introduction to Kubernetes Services and the four available types. For more details about Services, see the official Kubernetes documentation.If you want to get started with Kubernetes, I would recommend you to use a managed service like Azure Kubernetes Service (AKS)." }, { "title": "Use .NET Secrets in a Console Application", "url": "/use-net-secrets-in-console-application/", "categories": "Programming", "tags": ".NET 6, Git", "date": "2022-01-24 00:00:00 +0100", "snippet": ".NET Core made it easy to configure your application. Currently, I am working on a .NET 6 console application and this showed me that especially ASP.NET MVC makes it easy to set up middleware such as dependency injection. When using a console application, it is not hard but it requires a bit more work than the web application.In this post, I would like to show you how to use .NET secrets in your .NET 6 console application.Create a new .NET 6 console applicationYou can find the code of the demo on GitHub.Create a new .NET 6 console application using your favorite IDE or the command line. First, add install the following two NuGet packagesNext, create a new class, that will read the appsettings file and also the NETCORE_ENVIRONMENT environment variable.The NETCORE_ENVIRONMENT variable is the default variable to configure your environment in .NET. This variable contains values such as Development or Production and can be used to read a second appsettings file for the specific environment. For example, in production, you have a file called appsettings.Production.json which overrides some values from the appsettings.json file.Next, add a new file, called appsettings.json, and add the following code there:This file contains a username and password. Values that should never be checked into source control!Lastly, add the following code to your Program.cs file:This code creates a new instance of SecretAppsettingReader and then reads the values from the appsettings.json file. Start the applications and you should see the values printed to the console. Read the values from appsettings Add Secrets to your ApplicationThe application works and reads the username and password from the appsettings file. If a developer adds his password during the development, it is possible that this password gets forgotten and ends up in the source control. To mitigate accidentally adding passwords to the source control, .NET introduced secrets.To add a secret, right-click on your project and then select “Manage User Secrets” in Visual Studio. Add a User Secret This should create a secrets.json file and add the Microsoft.Extensions.Configuration.UserSecrets NuGet package. Sometimes Visual Studio 2022 doesn’t install the package, so you have to install it by hand with the following command:You can use the secrets.json file the same way as you would use the appsettings.json file. For example, add the “MySecretValues” section and a new value for the “Password”:There is one more thing you have to do before you can use the secrets.json file. You have to read the file using AddUserSecrets in the SecretAppsettingReader file:The AddUserSecrets method takes a type that indicates in what assembly the secret resides. Here I used Program, but you could use any other class inside the assembly.Start the application and you should see that the password is the same value as in the secrets.json file. The password was read from the secret When you check in your code into Git, you will see that there is no secrets.json file to be checked in. Therefore, it is impossible to check in your secrets like passwords.ConclusionSecrets help developers to keep their repositories free of passwords and other sensitive information like access tokens. .NET 6 allows you to set up these secrets with only a couple of lines of code.You can find the code of the demo on GitHub." }, { "title": "Create Git Commits in an Azure DevOps YAML Pipeline", "url": "/create-git-commits-in-azure-devops-yaml-pipeline/", "categories": "DevOps", "tags": "Azure, Azure DevOps, Git, YAML", "date": "2021-12-20 00:00:00 +0100", "snippet": "This week I encountered the need to commit some code to a Git repository in an Azure DevOps pipeline. First, I had no idea how to could work because the repository has some branch policies and require the committer to create a pull request.As it turns out, if you configure your pipeline with the right permissions, committing code in the pipeline is quite simple.Configure the Pipeline to bypass Pull Request PoliciesFirst, you have to configure the permissions of the build service to bypass pull request policies and also to be allowed to commit to your repository. To do that, go to the Project Settings –&gt; Repository –&gt; select your repository and then click on the Security tab.Select your Build Service and set Bypass policies when pushing Contribute to allowed. Set the permissions of the Build Service Create a Pipeline to Checkout and Commit to Git BranchesWith the permissions in place, create a new YAML pipeline. For this demo, I use this very simple one.All this pipeline does is to set an email address and user name in the gitconfig file and then it writes a new file called data.txt to the root folder and commits this change. You can split all these tasks also into separate tasks and don’t have to do them all in the same one.Run the pipeline and it should finish successfully. The pipeline ran successfully Open your repository and you should see the data.txt file there. The test file got created and committed This demo is very simple but still should show you everything you need to know to create commits in your pipeline yourself. I will extend this demo in a future post and will use it to change some configurations of my application for an Azure Arc deployment.ConclusionThis post showed how you can create Git commits inside your Azure DevOps YAML pipeline and how to configure your Build Service to bypass pull request policies.You can find the code of the demo on GitHub." }, { "title": "Azure Arc - Getting Started", "url": "/azure-arc-getting-started/", "categories": "Cloud", "tags": "Azure, Azure Arc, GitOps, SQL, Kubernetes, Postgre SQL", "date": "2021-12-13 00:00:00 +0100", "snippet": "Today, many companies use cloud services, but there is also still a sizeable group of companies that can’t use cloud services for various reasons. Modern software becomes more and more complex and, therefore, managing your applications in your own data center becomes quite daunting. This is where Azure Arc comes into play to help you administer your on-premise, or multi-cloud applications and also helps you to govern your data.Over the next couple of posts, I will dive into Azure Arc and show you how it can enable you to modernize and manage your on-premise applications and databases. Today, I would like to give you an overview of the features, pricing, and capabilities of Azure Arc.What is Azure Arc?Azure Arc helps you to govern and manage your on-premise and multi-cloud applications. It offers the following features to do so: Use Azure services and management features like Azure policies for your resources, no matter where they are deployed. Manage any CNCF certified Kubernetes cluster, virtual machines, and databases as if they were running in Azure. Manage your application by projecting them into the Azure Resource Manager. Introduce DevOps practices to modernize your existing applications.Azure Arc supports the following resources: Any CNCF certified Kubernetes cluster Physical or virtual machines running either Windows or Linux SQL Server running 2012 R2 or higher Azure data services like Azure SQL Managed Instance and PostgreSQL Hyperscale servicesAzure Arc is a bridge to the cloud to project your on-premise (or in another cloud) resources into Azure. Having all your resources, whether on-premise or in another cloud, projected into Azure allows you to gain central visibility of all your resources and also helps with regulatory compliance and data sovereignty since your data can stay in your data center while being managed via Azure.The offered features are mostly used in hybrid scenarios. You can have your SQL server on-prem and use Azure Arc for additional security like providing security best practices using Azure Sentinel and Azure Defender. Additionally, Azure Advisor can give you recommendations for more best practices. Being able to run Azure services everywhere will enable you to modernize your applications and data center while keeping full control over your data.No other cloud provider has an offering like Azure Arc and therefore it shouldn’t be surprising that Microsoft puts a heavy focus on extending its capabilities.This is a very high-level overview of the features provided by Azure Arc. Let’s take a more detailed look into each offering in the next sections.Azure Arc ServicesCurrently, Azure Arc supports the following resources: Server Kubernetes Cluster Databases Data ServicesAzure Arc-enabled ServersAzure Arc for physical or virtual servers allows you to manage or monitor these servers using Azure Monitor, Azure Policy, or Azure Automation. Additionally, it gives you security features with Azure Sentinel and Defender for Cloud. To enable Azure Arc for your servers, you have to install an agent. This agent sends regular heartbeat messages to Azure Arc to make sure that it is still connected. It does not matter where your servers reside. This can be on-premise, with another cloud provider, or something mixed.Having your server project to Azure allows you to use analytics or logging features of Azure like Azure Insights or the Dependency Map. Besides the logging capabilities, Azure Arc provides the possibility for a unified management experience as you can schedule updates on your server or track changes of software or licenses. All this can be done in one place.For more details see About Azure Arc-enabled servers.Azure Arc-enabled KubernetesAzure Arc for Kubernetes installed its agent in the azure-arc namespace in your Kubernetes cluster. Installing the agent will place your K8s cluster in a resource group and also allow you to add tags to it. The agent can be installed on any CNCF certified Kubernetes cluster. In a later post, I will show you how to connect an on-premise k3s cluster to Azure Arc.Connecting your K8s cluster to Azure Arc allows you to manage your cluster in the Azure portal. There, you have an overview of all namespaces, workloads, and services (it is the same view as if you were using an AKS cluster). You can also use Application Insights, create alerts, or monitor your containers with Container Insights.Another neat feature is the GitOps operator. This operator can be installed in your Kubernetes cluster and then will manage your deployments automatically for you. This will enable you to provide continuous deployment even if your cluster is behind a firewall and not accessible from the internet. The Gitops operator can use YAML files of Helm charts and polls every 5 minutes (configurable) for changes in your repository.For more details see About Azure Arc-enabled Kubernetes.SQL Server on Azure Arc-enabled ServersThe Azure Arc agent can be installed on SQL Server that runs on a physical or virtual machine running Windows or Linux. As the other Arc offers, Azure Arc for SQL enables you to use advanced logging and security features of Azure like Log Analytics or Azure Security Center and Azure Sentinel. Azure Arc SQL Architecture For more details see SQL Server on Azure Arc-enabled servers.Azure Arc-enabled Data ServicesI have never used Azure Arc-enabled data services so I can’t tell you too much about it. According to the documentation, it helps you to manage your SQL managed instances and PostgreSQL Hyperscale services by providing policies and regular updates using the Microsoft Container Registry. Additionally, you can use cloud-like elasticity on your on-premise databases and scale them dynamically, as if they were running in the cloud.Azure Arc provides a unified management experience and speeds up the deployment which can be done in seconds using either the Azure portal or the Azure CLI. Azure Arc Kubernetes Overview For more details see What are Azure Arc-enabled data services.PricingEvery Azure Arc service has a different pricing schema. The control plane is free for the server and Kubernetes offering and then you pay for each additional server or on a per CPU basis. Azure Arc-enabled SQL managed instance is billed on a per CPU base whereas you can get discounts if you can use Azure Hybrid Benefit.The Kubernetes pricing is not bad at all as you get the first 6 CPUs for free and pay $2 per month for each additional CPU. The pricing is worth it as I use it to have a unified management view of the cluster and I also use the GitOps operator to integrate continuous deployment although the cluster is not accessible from the internet or my Azure DevOps Server.For more details see Azure Arc pricing.Additional features of Azure like Azure Monitor or Log Analytics have the same costs as if you used them without Azure Arc.Getting StartedMicrosoft offers the Azure Arc Jumpstart and the Azure Migration and Modernization Program to help you get started with Azure Arc.Both platforms should give you an overview of the services and some samples the get started. The Azure Arc Jumpstart is more developer-focused whereas the Azure Migration and Modernization Program is more business-oriented. Nevertheless, I would recommend you to take a look at both websites.ConclusionAzure Arc is a great offering that allows you to bring Azure services to your on-premise solutions. This allows you to modernize your applications without changing your operations processes. Additionally, you have a central management platform for your services, no matter where they reside. All these features, compared with the relatively low costs (the SQL offering seems a bit expensive to me) will help you to streamline your operations processes and modernize your data center.In my next post, I will show you how to install a k3s cluster on-premise and then use Azure Arc to manage it via Azure.No other cloud provider offers a feature like Azure Arc." }, { "title": "Use AAD Authentication for Applications running in AKS to access Azure SQL Databases", "url": "/aad-authentication-for-applications-running-in-aks-to-access-azure-sql-databases/", "categories": "Kubernetes, Cloud", "tags": "Azure, SQL, AAD, Azure CLI, Azure SQL, AKS, Kubernetes, Managed Identity", "date": "2021-12-06 00:00:00 +0100", "snippet": "Removing passwords and using identities to access resources is the way to go for new applications. In my last posts Use AAD Authentication for Pods running in AKS and Implement AAD Authentication to access Azure SQL Databases, I showed you how to enable AAD for Azure Kubernetes Service and how to use AAD authentication to access an Azure SQL database.In this post, I want to show you how to use your own managed identity to configure an application running in AKS to access an Azure SQL database.This post is part of “Microservice Series - From Zero to Hero”.Configure AKS for AAD AuthenticationYou can find the details about the configuration of the AKS cluster in my previous post Use AAD Authentication for Pods running in AKS. Here is the short version to set up your AKS cluster:Set up some variables before you start. Make sure to replace with the subscription id where your AKS cluster is running.Assign the Managed Identity Operator and Virtual Machine Contributor role to the managed identity of the AKS cluster.Next, add and install the aad-pod-identity Helm chart. See Helm - Getting Started and Deploy to Kubernetes using Helm Charts for more information about Helm charts.Lastly, create a new managed identity. This managed identity will be used to authenticate the application in your Azure SQL database.Configure the Azure SQL Server and Database for AAD AuthenticationYou can find the details about the configuration of the Azure SQL Server and database in my previous post Implement AAD Authentication to access Azure SQL Databases. Here is the short version to set up your AKS cluster:Set a user or group as admin of your SQL server. You can use the portal or the CLI. Set the SQL Server Admin If you use the Azure portal, don’t forget to click Save after selecting a user or group.Login to your SQL server with this admin user (or a member of the admin group) using AAD Authentication. Log in using the server admin user Open a new query window and set some permission for your previously created managed identity.Make sure to change the database name and/or the managed identity name if you are not using the same names as I do.Configure your Application running in AKS to use AAD AuthenticationYou can find the code of the demo on GitHub.There are a few things you have to do before you can use AAD authentication for your SQL Server.First, add two files to the Helm chart of your application, aadpodidentity, and aadpodidentitybinding. Aadpodidentity creates a resource of type AzureIdentity and configures the name of the managed identity, its resource id, and client id.The second file creates a resource of the type AzureIdentityBinding and tells your application what managed identity it should use to acquire the authentication token.If you do not want to use Helm or the values.yaml file, you can use normal strings in both files. You can find the resource id, and client id in the previously created variables when creating the managed identity.Add the previously created new values to the values.yaml file:The variables starting and ending with two underscores (__) will be replaced in the CD pipeline. You can read more about this in Replace Helm Chart Variables in your CI/CD Pipeline with Tokenizer.Next, add the following label in the deployment.yaml file in your Helm chart in the labels section of metadata and template.This label is necessary for the AzureIdentityBinding resource to select the right deployment. Make sure that the value of the label matches the value of the selector in the aadpodidentitybinding.yaml file.Note that the files aadpodidentitybinding.yaml and aadpodidentity.yaml are not committed to my demo on GitHub. This is necessary to ensure users can deploy the application when they are not using pod identity. Without pod identity enabled, the Helm deployment would fail with these two files there.Configure the CD PipelineYou can find the code of the demo on GitHub.The connection string is different when using AAD authentication. Since you don’t need a username or password anymore, make sure to edit your connection string. It should something like this:Lastly, make sure to add the new variable to your pipeline.Also, add the client and resource id and the tenant id as the secret to your pipeline. Add secrets to the pipeline Test the ImplementationCheck in your code and let the Azure DevOps pipeline run. After the deployment is finished, go to your application and you should be able to load your data from the database. Retrieving data from the database works ConclusionGetting started with AAD authentication is not easy and Microsoft’s documentation is incomplete and a bit misleading at the time of this writing. There are several approaches and it took me some time to get it working. Once you figured out how it works, it is quite simple to configure, as you have seen in this post.In my next post, I will show you how to make the necessary changes in the CI pipeline so that everything gets configured and deployed automatically.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Upgrade a Microservice from .NET 5.0 to .NET 6.0", "url": "/upgrade-microservice-from-net-5-to-net-6/", "categories": "ASP.NET, Docker", "tags": "Azure DevOps, CI, Docker, xUnit, .NET 5, .NET 6, NuGet", "date": "2021-11-29 00:00:00 +0100", "snippet": ".NET 6, the fastest and best .NET ever just got released. The improved performance, new C# features, and the 3-year long-term support are great incentives to upgrade existing applications.Let’s have a look at how much work it is and how to upgrade existing .NET 5 microservices to .NET 6.System Requirements for .NET 6.0To use .NET 6.0 you have to install the .NET 6.0 SDK from the dotnet download page and Visual Studio 2022 or later.Uprgrade from .NET 5.0 to .NET 6.0You can find the code of the demo on GitHub.Before you begin, check the breaking changes in .NET 6 to make sure that your code is compatible with the new version.Upgrading a microservice from .NET 5 to .NET 6 is as simple as it could be. All you have to do is to change the TargetFramework from net5.0 to net6.0 for every *.csproj file If you use my demo, you can change all .NET versions by changing the DefaultTargetFramework in the common.props file in the root folder of each microservice.After updating the .NET version, rebuild your solution and check that everything still builds. If you get an error, try to execute a “clean solution” or delete all bin folders.Once the solution builds, update your NuGet packages to the newest version. Update your NuGet packages Build the solution again and check for errors.If you are using xUnit, you might have some errors since xUnit removed Throw with ThrowAsync when you are checking for exceptions. Replace Throw with ThrowAsync wherever needed and rebuild the application.After fixing all compile errors, run all your unit tests and make sure that all tests still run successfully. Run all unit tests Update DockerfilesAll my microservices run in Docker containers and therefore I have to update the Dockerfile to use the image for .NET 6.Replace the following two lines:withThe demo project uses Docker also to build the application in the CI pipeline. The beauty of this approach is that you don’t have to change anything in your build pipeline.Update Swagger and Swagger UISwagger was a bit complicated to configure in the past but a couple of versions ago, the configuration got simplified. Updating the whole solution is a great opportunity to simplify the Swagger configuration.First, remove the Swagger comment configuration in the .csproj file. This is not needed anymore.Next, update the service definition in the Startup.cs class. All you need is AddEndpointsApiExplorer and AddSwaggerGen. You can add additional information like an email and description to AddSwaggerGen.Lastly, add UseSwagger and UseSwaggerUI to the Configure method.The RoutePrefix property allows you to configure the route to the Swagger UI. Setting it to string.Empty configures the application to display the Swagger UI when no URL is entered.If you don’t want to update the configuration and use Swagger as always, all you have to do is update the XML comment section in the .csproj from .net5.0 to .net6.0.Run the updated CI PipelineCheck in your changes and the build pipeline in Azure DevOps should run successfully. The .NET 6 build was successful Considering new .NET 6 Features.NET 6 put an emphasis on simplifying the structure of the project and files. You can change your files to use these features like global usings or top-level statements. You can also choose to keep the structure the way it already is. Both work just fine.Limitations in November 2021As of this writing, Azure Functions v4 supports .NET 6 but there is no Docker image for it yet. Since the CI/CD pipeline deploys the Azure Function in a Docker container, I can’t update it yet and leave it as it is.ConclusionUpgrading from .NET 5 to .NET 6 is very fast and simple. Most applications should be able to update without any problems by simply changing the .NET version number in the project file and the Dockerfile. Make sure to update your NuGet packages and test your application after the update to make sure that everything still works.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": ".NET 6 Minimal APIs - Getting Started", "url": "/net-6-minimal-apis-getting-started/", "categories": "ASP.NET, Programming", "tags": ".NET 6, C#, REST, API, Swagger", "date": "2021-11-22 00:00:00 +0100", "snippet": "Microsoft released .NET 6 and the most talked topic of this new version is minimal API. The promise of minimal APIs is that you can write APIs with barely any code. When I read the announcement, I wasn’t sure if I like this new feature.Now that it is released, let’s try minimal APIs and see if it is any good.You can find the code of this demo on GitHub.Prerequisites for .NET 6Before you can use .NET 6, you have to download Visual Studio 2022. You can use the Community edition free of charge. This version offers the same features as the Professional one but does not allow you to use it for commercial projects.Create your first Minimal API projectOpen Visual Studio and select create a new project. On this page, select ASP.NET Core Web API and click Next. Select the API template Provide a name for your new project and on the next page make sure to uncheck “Use controllers (uncheck to use minimal APIs). Use minimal APIs This creates a new Web API project. If you take a look at the project structure, you will see that it is minimal indeed. The minimal API project structure The only file of interest in this project is the Program.cs file. This file configures the whole application.Program.csIf you are familiar with the Startup.cs file, you will see many similarities like app.UseHttpsRedirect or services.AddSwaggerGen(). The method app.MapGet configures an endpoint for HTTP requests and returns five random weather forecasts. You can test this using the Swagger UI.Start your application and you will see the Swagger UI. Execute the Get method and you will see five random weather forecasts. Testing the Swagger UI Edit the Minimal API projectThe default minimal API project is already very small but you can make it even smaller. In the launchSettings.json file, replace “swagger” with “” for the launchUrl. Then edit the Program.cs file with the following code:When you start your application, you will see the Hello World string in your browser. Return a string from the API Configure Post RequestsAdditionally to MapGet, there are MapPost, MapDelete and MapPut to configure the respective HTTP methods. Use MapPost and then a Lambda function to create a simple HTTP Post endpoint.This endpoint takes in int and if the value is greater than zero, it returns Accepted, otherwise it returns Bad Request. The example is very simple but you could adept this method to do some more complicated operations like writing something to a database.When you use Postman to test the Post method and send a number greater than zero, you will the a HTTP 202 status code. The valid request returns accepted If you send a number smaller than zero, you will get a HTTP 400 status code. The invalid request returns bad request Could be a more complex method like loading something from the databaseWill I use Minimal APIs?Minimal APIs are very simple but currently I don’t see myself ever using them. Especially for beginners, this might be too confusing since the template is using top-level statements and is hiding a lot of important aspects of programming. If you take a look at the Program.cs file again, you will see that there are no usings, namepsace, class or methods. These are important concepts and I think it will be more confusing for new programmers to see them only sometimes.Additionally, the whole configuration is in the same file and method now. Previously, the Startup.cs had two different methods to configure the application and the Program.cs file configured the web server. This is now all crammed into one file. This means that the whole configuration is in one place, but this also means that the file can get quite big and unclear if there is a lot of configuration. Actually, it is not only configuration but it is also the logic of the endpoints.Maybe I am just bad with adapting to change and in a while I will love this new approach. Currently, I don’t see that happening and will continue using controllers even for small APIs.ConclusionMicrosft’s big focus of .NET 6 is to remove boiler plate code and make applications as simple as possible. Minimal APIs allow developers to create APIs with basically a single file where they can configure the application but also write the code of the endpoints.So far, I am not a big fan but time will tell how good this feature is.You can find the code of this demo on GitHub." }, { "title": "C# 10.0 - What's new", "url": "/c-sharp-10-whats-new/", "categories": "Programming", "tags": ".NET 6, C#", "date": "2021-11-15 00:00:00 +0100", "snippet": "It is November again which means that Microsoft released a new version of .NET. This year, Microsoft released with .NET 6 a long-term support (LTS) version, and guarantees support for .NET 6 for 3 years. Along with .NET 6, Microsoft published C# 10.In this post, I will look into some of the new features of .NET 6 in combination with C# 10.You can find the code of this demo on GitHub.Prerequisites for .NET 6 and C# 10Before you can use .NET 6 and C# 10, you have to download Visual Studio 2022. You can use the Community edition free of charge. This version offers the same features as the Professional one but does not allow you to use it for commercial projects.Removing Boilerplate codeWhen you create a new console application, you are probably used to seeing the Program.cs file that contains some usings, a namespace, the Program class, and the Main method which writes something to the console. This is a lot of code for a single console output. The old console application template Microsoft thinks that this is way more complicated than it has to be and removed all of that and reduced the program.cs file to a single line that writes a message to the console. The default console application template Another reason why Microsoft removed all the boilerplate code is to make it easier for beginners to get started. This might be true but it also hides a lot of basic knowledge that beginners should learn early on. Additionally, if you want to add a method, you have to add the class and Main method manually.All in all, I understand Microsoft’s decision but I am not a big fan of it yet. Perhaps this will change in a year when I got used to it. Time will tell.Global UsingsGlobal usings is probably my favorite feature of C# 10 since it allows developers to create a single file and add usings there. These usings will be added automatically to every file in the project. This should reduce the number of usings in each file and will help to make the files clearer.Let’s look at some code. In the Program.cs file, initialize a new variable from a class.Note that you don’t have to add a using for this class. The NullParameterChecking class looks as follows:As you can see in the code, you do not have to add brackets after the namespace. This is also a new feature and will help to reduce the indentation of the code.Lastly, create a new file that will contain the global usings. The name of the file does not matter. I prefer naming it GlobalUsings.cs. This file contains all usings that you want to apply globally:Record StructsC# 9 already introduced records for classes. C# 10 improves this feature and adds records for structs. You can define a struct with a single line and the compiler will automatically create properties according to the parameters of the constructor.The Coordinate struct has three properties, X, Y, and Z. You can create a new struct and then access these properties the same way as if you had defined them yourselves.Null Paramter CheckingEvery developer knows that we should check all parameters for null but it is a tedious task. Especially if your method has several parameters it is quite a lot of typing for such a simple task. C# 10 can create these checks almost automatically for you. All you have to do is to add one ! after the parameter and C# 10 (maybe it is Visual Studio though) will create a null check that throws an exception if the parameter is null.ConclusionThis post gave a short introduction to the new features of C# 10 and .NET 6. C# is a quite mature language therefore the changes are not that massive anymore. Maybe I am a bit slow to adapt to the new changes but so far I am not too excited about them. Allowing us to use global usings is a nice feature and also the automatic null checks of parameters will boost productivity. The other features have to prove themselves when I start using them in a real-world project.Update 13.12.21: Approximately one month into .NET 6 and I have to say that I use the new Namespace feature and Global Usings all the time. I still haven’t used records structs or minimal APIs.In my next post, I will take a look at minimal APIs which is probably Microsoft’s most talked about feature of .NET 6.You can find the code of this demo on GitHub." }, { "title": "Automatically set Azure Service Bus Queue Connection Strings during the Deployment", "url": "/automatically-set-service-bus-queue-connection-string-during-deployment/", "categories": "DevOps, Cloud", "tags": "Azure, Azure DevOps, Helm, Azure Service Bus, CI-CD", "date": "2021-11-08 00:00:00 +0100", "snippet": "In a previous post, I showed how to add the connection string of an Azure Service Bus Queue to your application using Azure DevOps pipelines. This was done using variables. The solution works, but it is not dynamic. In a cloud environment, you often delete and re-create environments. Therefore, you will have different access tokens which results in changing connection strings.In this post, I will show you how you can read the connection string using Azure CLI and then pass it to your application inside an Azure DevOps pipeline.This post is part of “Microservice Series - From Zero to Hero”.Use Azure CLI to read the Azure Service Bus Queue Connection StringYou can find the code of the demo on GitHub.Using Azure CLI allows you to read the connection string of an Azure Service Bus Queue with a single command using the Azure CLI task in the Azure DevOps pipeline. After reading the connection string, use Write-Host to set it to the AzureServiceBusConnectionString variable.The continueOnError parameter is not really necessary but it makes testing without an Azure Service Bus queue easier. With this parameter set to true, the pipeline will continue, even if no service bus connection string was found.Use a Template inside the Azure DevOps PipelineIn Improve Azure DevOps YAML Pipelines with Templates, I explained how to use templates to make your pipeline clearer. To continue using templates, I placed the code from above in a separate template named GetServiceBusConnectionString. Then I call this template during the deployment and pass the needed variables as parameters.I use this template during the deployment of a pull request and for deploying to the test and production environment. The finished template looks as follows:Lastly, add the following variables to your pipeline. Note to replace my values with your corresponding ones.That’s already all you have to change. You don’t have to change more because the output variable of the GetServiceBusConnectionString template has the same name as the previously used one. The Tokenizer task will read the variable and add it to the Helm chart. You can read more about that in Replace Helm Chart Variables in your CI/CD Pipeline with Tokenizer.Run your application and you should still be able to access your queue.ConclusionUsing a cloud environment allows your infrastructure and applications to be flexible. Therefore, your configuration also has to be flexible. This post showed how you can easily read the connection string of an Azure Service Bus Queue using Azure CLI. By doing so, your application will always read the correct connection string and will work no matter what happens to the underlying infrastructure.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Implement AAD Authentication to access Azure SQL Databases", "url": "/implement-aad-authentication-to-access-azure-sql-databases/", "categories": "Cloud, Programming", "tags": "Azure, SQL, AAD, C#, .NET, Entity Framework Core, Azure CLI, Azure SQL", "date": "2021-11-01 00:00:00 +0100", "snippet": "Microsoft promotes going passwordless for a while now. Azure offers authentication against the Azure Active Directory where applications can acquire access tokens using their identity. Another use case would be accessing an SQL database running on Azure. Although, in theory, this sounds very easy, my experience showed that it can get tricky.Today I want to show you how to configure Azure SQL with Azure Active Directory authentication and how to avoid annoying pitfalls.This post is part of “Microservice Series - From Zero to Hero”.Configure AAD Authentication for an Azure SQL ServerYou can find the code of the demo on GitHub.In one of my previous posts, I created an SQL Server that runs all my databases. If you also have an SQL server, you have to set an Active Directory admin. This admin can be either an AAD user or a group. Without this admin, the SQL server won’t be able to authenticate your users against the AAD.You can either use the Azure portal or Azure CLI to set the Active Directory admin. If you use the portal, open your SQL server and select the Active Directory admin pane. There, click on Set admin, search for your user or group and save your selection.If you use the Azure CLI, use the following query to get all AAD users:The –query parameter filters the response to only display the principal name of the AAD user. Get the principal name of all AAD users My AAD currently has only one user and therefore returns only this one principal name. Once you know the principal name of the group or user you want to set as admin, you can filter using the –filter flag so your query only returns this one entity. Save the return value in a variable, so you can reuse it in the next step.With the user set to the variable, use the following command to set the Active Directory admin. Replace the resource group and server name with your corresponding values. Set the Active Directory admin Configure the Test Application to use AAD AuthenticationYou can find the code of the demo on GitHub.Open the SQL management tool of your choice, for me, it’s Microsoft SQL Server Management Studio (SSMS), and log in with the user you previously set as the Active Directory admin. Log in using the server admin user You should be able to log in and see all the databases on the server. The-login-was-successful Now it is time to test the login with a test application. To use AAD authentication when developing, you have to sign in to Visual Studio. Visual Studio then uses this user to request an access token and authenticate you to the SQL server. Since you should not use the admin account for your application, I log in with a different user.Before you can use your test application, you have to make some small changes. First, install the following NuGet packages:Next, create a custom SQL authentication provider. This class requests an access token from the AAD.With the authentication provider set up, register your DbContext in the Startup.cs class and add the previously created authentication provider.Lastly, set the following connection string in your appsettings.json file.Testing the AAD AuthenticationIf you are using my demo application, make sure that you have set the “UseInMemoryDatabase” setting in the appsettings.json and appsettings.Development.json files to false. Otherwise, the application will use an in-memory database.When you start the demo application, you will see the Swagger UI. Execute the Get request for Customer and you will see the following error message: The-login-to-the-Customer-database-failed The login failed because the user logged in to Visual Studio has no access to the Customer database. You have to add your users to the database before they can access it.Add Users to your DatabaseLog in to the database with the user you previously set as the admin. Add your Visual Studio user with the following code and also give the user the desired roles.Now you should be able to log in with this user. If you use SSMS, make sure that you select the Customer database as the default database. If you don’t set Customer as your default database, SSMS will use the Master database and since the user does not exist in this database, the login will fail. The user does not exist in the Master database Try the Test Application againStart the test application and execute the Get request again. This time you should get some customers. You may get the following error though: Login to the SQL server failed If you google this error message, you won’t find much helpful information. Also, Microsoft’s documentation doesn’t mention anything about this error.The problem you encounter here is that your user exists in multiple tenants and the authentication provider does not know which tenant it should use. To fix this problem, you have to add the tenant where the SQL server resides to your authentication provider.Replace the XXX with your actual tenant Id.Run your application again and now you should be able to retrieve the data from the database. Successfully retrieved data from the database To verify that you loaded the data from the right database, log in to your database using SSMS and query the Customers in your Customer database. The result should be the same as you saw previously in your test application. Query customers from the database using SSMS Improving the Test ApplicationThe test application can retrieve data using the AAD authentication but the code is not pretty yet. Especially the part where the tenant Id is hard-coded into the authentication provider. Let’s improve this code a bit.First add a new property, TenantId to the appsettings.json file.Next, add a constructor to your CustomAzureSqlAuthProvider with a string as the parameter. This string will contain the tenant Id. Assign the parameter to a private variable. Then replace the hard-coded tenant Id with the new private variable.Lastly, read the value of the tenant Id from the appsettings.json file and pass it to the constructor of your authentication provider in the Startup.cs class.Run your application again to make sure that everything still works.Pass the Tenant Id during the DeploymentYou want to pass the tenant id during the deployment to keep it secret. If you commit it to your version control, everyone would be able to read it. You have to take the following steps to pass the tenant id during the deployment:Add a variable for the tenant id to your values.yaml file in your Helm chart. This variable has to start and finish with two underscores (__) so the Tokenizer task in the Azure DevOps pipeline can replace the value. Next, add your tenant id as a secret variable in your pipeline. Don’t forget to update the connection string in your deployment pipeline too.The Tokenizer task will replace the tenant id in the Helm chart with your actual tenant id and therefore will override the empty TenantId value in your appsettings.json file. For a detailed explanation of how and why this works, see Replace Helm Chart Variables in your CI/CD Pipeline with Tokenizer.ConclusionAzure Active Directory authentication to access your databases is a great feature to get rid of passwords. This should also streamline the development process since you don’t have to share passwords with new developers. All you have to do is to add the developer to the desired database so they can log in.There may be some roadblocks on the way and Microsoft’s documentation only showcases the happy path of the integration. This post should help you with the most common pitfalls and shows how to avoid them.In my next post, I will show you how to use AAD authentication with your application running in AKS.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Use AAD Authentication for Pods running in AKS", "url": "/use-aad-authentication-for-pods-running-in-aks/", "categories": "Kubernetes, Cloud", "tags": "Azure, AKS, Kubernetes, AAD, Helm", "date": "2021-10-25 00:00:00 +0200", "snippet": "Since the dawn of time, authentication has been a problem for developers and security engineers. Weak passwords make it easy for attackers to get access to areas where they don’t belong. Even if you have a very long and complex password, you might be at risk. Passwords get leaked almost every day nowadays. The current recommendation for authentication is passwordless. This means that no password exists anymore and you authenticate with an authenticator app or a security key.This approach is safe but not possible for applications. This is where Azure Active Directory authentication comes into play. Resources in Azure can authenticate using their identity and then get an access token from AAD. Using these identities is quite easy with standard resources like App Service or Key Vault but not as easy when using Kubernetes.Today, I would like to show you how you can configure your Azure Kubernetes Service cluster to use AAD Pod Identity to authenticate against the AAD.This post is part of “Microservice Series - From Zero to Hero”.What are Managed IdentitiesEvery application faces the same challenges when it comes to securing its credentials. Managed identities eliminate the need for credentials and help to make applications more secure at the same time. Since there are no passwords with managed identities, no password can be compromised which could give unauthorized people access to resources.Managed identities can be used to access a database or an Azure Key Vault where the application can load all required credentials for resources that do not support managed identities yet. Additionally, managed identities can be used without any additional costs. There are two types of managed identities, system-assigned and user-assigned.System-Assigned Managed IdentitySome resources in Azure like App Service or Azure Container Registry allow you to enable a managed identity directly on that service. This identity is created in the Azure AD and shares the same lifecycle as the service instance. When you delete the service, the system-assigned managed identity will be also deleted. System assigned managed identity in ACR User-Assigned Managed IdentityA user-assigned managed identity is a standalone resource in Azure. It can be assigned the resource and can be used in the same way as the system-assigned managed identity to authenticate that resource. The user-assigned managed identity has an independent lifecycle. This means that you can delete the Azure resource to which the identity is assigned and the identity does not get deleted.Enable Managed Identities for Applications running in Azure Kubernetes Service (AKS)Enabling managed identities for applications in AKS, so-called pod identities, is unfortunately not straightforward. There are different documentation but neither of them works reliably. You can find a demo integration on GitHub but the documentation is missing some steps and the scripts did not work for me (mostly due to wrong file paths).Follow this guide to install pod identity on your AKS cluster and then test it with a demo application. Afterwards, I will go into the details of what is happening.Set up your AKS Cluster for Pod IdentitiesFor this tutorial, I will assume that you have no AKS cluster created yet. This demo uses the Azure CLI and bash. Before you begin, create a couple of variables, which you can use throughout this demo. Note to replace the value of SubscriptionId with the Id of your Azure Subscription. Set some variables Next, create an AKS cluster with a managed identity and the azure network plugin.After the AKS cluster is created, retrieve the client Id of the managed identity of the cluster. Then use this client Id to assign the Managed Identity Operator and Virtual Machine Contributor roles to the managed identity.This finishes the setup of the AKS cluster. Connect to your newly created cluster and let’s continue to configure the pod identity.Configure Pod Identity in Azure Kubernetes ServiceInstall the AAD Pod Identity Helm chart using Helm. If you do not know Helm, see Helm - Getting Started for more information. Add and install the Helm chart with the following command: The AAD Pod Identity Helm Chart was installed successfully After the installation is finished, you will see a message to verify that all associated pods have been started. Use the following command to check that:This should show that all pods are running. All AAD Pod Identity Pods are running Next, create a new identity and export its client Id and Id: Create a new Identity With this identity, create a new Kubernetes object of the type AzureIdentity and add the identity Id and client Id to it.Optionally, you could save the AzureIdentity in a file and then apply this file. If you are using this approach, you have to replace the variables with their actual values though.The last step of the configuration of pod identities is to create an AzureIdentityBinding using the following command:Test the usage of the Pod IdentityMicrosoft has a test application that will retrieve a token and give you a success message when the operation is completed. If something went wrong, you will see an error message. Deploy the test application with the following command:Wait for around a minute and then check the logs of the pod. You can do this either using kubectl or a dashboard. Kubectl gives you a wall of text that might be hard to read.I prefer Octant as my dashboard. You can find more information about the usage and installation in Azure Kubernetes Service - Getting Started. The token was retrieved successfully What happens inside Kubernetes when adding Pod IdentityThere was a lot going on during the demo which might be hard to understand at first. Let’s go through it step by step.Azure Kubernetes Service Cluster InstallationThe AKS cluster is created with the flag –network-plugin azure. AKS supports two network modes, kubenet and azure, whereas kubenet is the default mode.AAD Pod Identity is disabled by default on clusters using the Kubnet network mode. This is due to security concerns because Kubenet is susceptible to APR spoofing. This makes it possible for pods to impersonate other pods and gain access to resources that they are not allowed to access. The Azure network plugin prevents ARP Spoofing.After the cluster is installed, the code creates a service principal. This service principal is used to authenticate against the AAD and retrieve tokens for the pods. Adding the Managed Identity Operator and Virtual Machine Contributor roles to the service principal is necessary so it can assign and un-assign identities from the worker nodes that run on the VM scale set.Managed Identity Controller (MIC) and Node Managed Identity (NMI)The Managed Identity Controller (MIC) monitors pods through the Kubernetes API Server. When the MIC detects any changes, it adds or deletes AzureAssignedIdentity if needed. Keep in mind that the identity is applied when a pod is scheduled. This means if you change your configuration, you have to re-schedule your pods for the changes to be applied.The Node Managed Identity (NMI) intercepts traffic on all nodes and makes an Azure Active Directory Authentication Library (ADAL) request to get a token on behalf of the pods. The request to get a token is sent to the Azure Instance Metadata Service (IMDS) at 169.254.169.254. The answer is redirected to the NMI pod due to changes to the iptable rules of the nodes.AzureIdentity and AzureIdendityBindingAzureIdentity can be a user-assigned identity, service principal, or service principal with a certificate and is used to authenticate the requests from the NMI to get a token from the AAD.The AzureIdendityBinding connects the AzureIdentity to a pod. The AzureIdentity is assigned to a pod if the selectors are matching.The following screenshot shows the workflow of the token acquisition. Token acquisation workflow (GitHub) Uninstalling AAD Pod IdentityYou can use the following code to uninstall pod identity.The iptables that got modified by the NMI pods should be cleaned up automatically when the pod identity pods are uninstalled. If the pods get terminated unexpectedly, the entries in the iptables are not removed. You can do that with the following commands:ConclusionUsing AAD authentication in Azure Kubernetes Service is unfortunately not straightforward, especially since the documentation is missing some crucial information. Nevertheless, going passwordless is definitely worth the hassle and will make it easier to manage your applications in the future.In my next post, I will show you how to use this AAD authentication to access your database without a password.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Automatically scale your AKS Cluster", "url": "/automatically-scale-your-aks-cluster/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure, YAML, AKS, Kubernetes, Monitoring, Prometheus, Grafana, KEDA, Azure DevOps", "date": "2021-10-18 00:00:00 +0200", "snippet": "If you followed this series, you have learned how to scale your applications using the Horizontal Pod Autoscaler (HPA) or KEDA, Kubernetes Event-driven Autoscaling. Both approaches can automatically scale out and scale in your application but also suffer from the same shortcoming. When the existing nodes have no resources left, no new pods can be scheduled.Today, I would like to tell you how to automatically scale your Azure Kubernetes Cluster to add or remove nodes using the cluster autoscaler.This post is part of “Microservice Series - From Zero to Hero”.Working without automatic Cluster scalingYou can find the code of the demo on GitHub.In Azure Kubernetes Service - Getting Started, I have created a new Azure Kubernetes Cluster with one node. This cluster works fine if you ignore the fact that one node does not provide high availability, but since the creation of the cluster, I have added more and more applications. When I have to scale one of my applications to several pods, the node runs out of resources and Kubernetes can’t schedule the new pods. New Pods can not be scheduled To get the error message why the pod can’t be started, use the following command:Replace kedademoapi-68b66664cb-jjhvg with the name of one of your pods that can not be started and enter the namespace where your pods are running. You will see the error message at the bottom of the output. The Pod cant be started due to insufficient CPU Verify your Worker NodesIf you are using Azure Kubernetes Service, you have two options to verify how many worker nodes are running. First, open your AKS cluster in the Azure portal and navigate to the Node pools pane to see how many nodes are running at the moment. As you can see, my cluster has only one node: Node count in the Azure Portal The second option to verify the number of nodes is using the command line. Use the following command to display all your nodes:This command will display one worker node. One worker node exists Configure AKS Cluster AutoscalerIn an earlier post, I have created a YAML pipeline in Azure DevOps to create my AKS cluster using Azure CLI. The code looks as follows:The cluster autoscaler can be easily enabled and configured using the enable-cluster-autoscaler flag and setting the minimum and maximum node count.The cluster autoscaler has a wide range of settings that can be configured using the cluster-autoscaler-profile flag. For a full list of all attributes and their default values, see the official documentation.The default values are usually good, except that I would like to scale down faster. Therefore, I change two settings of the cluster autoscaler profile:Test the Cluster AutoscalerThe cluster autoscaler sees pods that can not be scheduled and adds a new node to the cluster. Open the Node pools pane of your AKS cluster in the Azure portal and you will see that your cluster is running two nodes now. Two Nodes in the Azure Portal Using the CLI also shows that your cluster has two nodes now. Two Nodes in the CLI Use the following command to see that all pods got scheduled on one of the two nodes (replace kedademoapi-test with your K8s namespace):This command displays all your pods in the given namespace and shows on which node they are running. All Pods got scheduled ConclusionModern applications must react quickly to traffic spikes and scale out accordingly. This can be easily achieved using the Kubernetes Horizontal Pod Autoscaler or KEDA. These approaches only schedule more pods and your cluster can easily run out of space on its worker nodes. The cluster autoscaler in Azure Kubernetes Services helps you when the cluster runs out of resources and can automatically add new worked nodes to your cluster. Additionally, the cluster autoscaler also removes underutilized nodes and therefore can help you to keep costs to a minimum.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Update DNS Records in an Azure DevOps Pipeline", "url": "/update-dns-records-in-an-azure-devops-pipeline/", "categories": "DevOps, Cloud", "tags": "DevOps, Azure, YAML, AKS, Kubernetes, Monitoring, Prometheus, Grafana, KEDA, Azure DevOps", "date": "2021-10-11 00:00:00 +0200", "snippet": "Infrastructure as Code (IaC) is a great process to manage the creation of your infrastructure. In an earlier post, I have created such a pipeline using Azure CLI to create all my resources in Azure. This pipeline creates an AKS cluster and all necessary services like a database server, Azure Function, and Azure Service Bus Queue.The only part missing was an update to my DNS service to point my domain to the newly created AKS cluster.This post is part of “Microservice Series - From Zero to Hero”.Prepare Azure DevOps to update DNS SettingsYou can find the code of the demo on GitHub.If you are using an Azure DNS Zone, you usually don’t have to prepare anything in your Azure DevOps environment. I am using a lock on my DNS Zone though. A lock can be used to prevent changes or deletion. Since the DNS Zone manages my DNS entries for my website, mail server, and demos, it is important to not change and especially delete them.The problem with the lock is that the Azure DevOps service principal that runs the Azure DevOps pipeline, has the Contributor role. This role is not allowed to create or delete locks. Therefore, I have created a new custom role and added this role to the service principal. You can read about it in my last post, Create Custom Roles for Azure DevOps in Azure.Update DNS Records in the Azure DevOps PipelineAs always, I start by defining some variables at the top of the pipeline.These variables contain the information about my DNS Zone and also a list of records that I want to update.The first step of my pipeline is to remove the lock on the DNS Zone. You can skip this step if you do not have a lock. The code gets the id of the name with the provided name and then uses this id to delete the lock.Next, you have to query the name of the public IP address of your AKS cluster. This name then can be used to get the IP address of the Nginx ingress controller running inside the Kubernetes cluster. The resource group of the aks cluster is automatically generated by Azure using the MC_prefix and then adding your resource group name in which your AKS cluster resides, the AKS cluster name, and the location of the cluster.Write-Host can be used to make the variable accessible outside of the task.Unfortunately, at this time it is not possible to update DNS records in Azure using Azure CLI. Therefore, you have to delete the existing records first and then add new ones. To delete all outdated DNS records, iterate over the previously created list of DNS records. Additionally, you have to get the IP address of each DNS record to be able to delete it. If the IP address is empty or null, nothing will happen.After all outdated DNS records are deleted, iterate over your DNS records list again and add them to the DNS Zone. The used IP address is the one published using Write-Host in a previous task.Optionally, create a new CanNotDelete lock for the Azure DNS Zone to protect it from deletion.Testing the IaC PipelineStart the pipeline and it should run successfully. The Pipeline ran successfully After the pipeline is finished, check if your DNS records were updated. First, go to your AKS cluster in the Azure portal and select the Service and Ingress pane. There you can see the external URL of the Nginx controller. Check the external IP of the Nginx Controller Go to your Azure DNS Zone and you should see that the URLs of your records are the same as the Nginx controller external IP. The DNS records were updated ConclusionUpdating DNS records in an Azure DevOps pipeline is a simple and fast way to react to changes in your infrastructure. Unfortunately, it is not possible to update existing records but this post showed how to delete and re-create your DNS records.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Create Custom Roles for Azure DevOps in Azure", "url": "/create-custom-roles-for-azure-devops-in-azure/", "categories": "Cloud, DevOps", "tags": "DevOps, Azure, Azure DevOps", "date": "2021-10-04 00:00:00 +0200", "snippet": "By default, the Azure DevOps service principal that is used to run a CI/CD pipeline with Azure resources gets the Contributor role assigned. This role can create new services but sometimes the Azure pipeline has to execute a task that is outside of the scope of the Contributor role, for example, adding RBAC assignments or deleting locks.In this post, I will show you how to create a custom role in Azure and how to assign it to the Azure DevOps service principal.This post is part of “Microservice Series - From Zero to Hero”.Create a new Azure Custom RoleIn my next post, I want my Azure DevOps pipeline to be able to update my DNS records. Since the DNS zone is a very sensitive resource, I have added a lock so it can not be deleted. This lock also prevents the Azure DevOps pipeline from deleting DNS records. Additionally, the Contributor role of the pipeline service principal is not allowed to create new locks either. Since there is no built-in role for creating and deleting locks, I have to create my own.To create a new role go to your subscription in the Azure portal, select the Access control (IAM) pane and then click Add under Create a custom role. Add a custom role In the Create a custom role window, provide a name and a description for your new role and then click on Permissions. Provide a name and description for the new custom role In the Permissions tab, click the Add permissions button and search for locks on the flyout. Select the Write and Delete permission from the Microsoft.Authorization/locks permission. Configure the permissions Go to the Review + create tab and click Create to create the new custom role.Assign the Custom Role to the Azure DevOps Service PrincipalAfter the new role is created, click on + Add and select Add role assignments on the Access control (IAM) pane of your subscription. Add a new role assignment This opens a flyout where you can search for the previously created custom role and also for the service principal of your Azure DevOps pipeline. If you do not know the service principal, go to your Azure Active Directory and select Enterprise applications. You should see Azure DevOps there. Add the custom role to the service principal After the role was assigned, go to the Role assignments tab of the Access control (IAM) pane and you should see the previously created role assignment there. The role was assigned ConclusionAzure comes with a grave variety of pre-defined roles for your services and users. Though sometimes, you need special permissions that are not built-in. This is where custom roles come into play. You can create a role with all the permissions you need and assign this role to a user, group, or service principal. This post showed how to assign the new role to a service principal that is used by Azure DevOps and in my next post, I will show you how to use this service principal to update DNS records in a CI/CD pipeline.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Deploy KEDA and an Autoscaler using Azure DevOps Pipelines", "url": "/deploy-keda-and-autoscaler-using-azure-devops-pipelines/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure, YAML, Docker, Helm, AKS, Kubernetes, Monitoring, Prometheus, Grafana, Istio, Kiali, KEDA, Azure DevOps", "date": "2021-09-27 00:00:00 +0200", "snippet": "In my last post, KEDA - Kubernetes Event-driven Autoscaling, I showed how to deploy a KEDA scaler to scale a microservice depending on the queue length of an Azure Service Bus Queue. The deployment of KEDA used Helm and the autoscaler was deployed using a simple YAML file. This is fine to learn new tools and technologies but in a modern DevOps environment, we want to have an automated deployment of KEDA itself and also of the scaler.In today’s post, I will show you how to deploy KEDA to Kubernetes using an Azure DevOps pipeline and how to add the KEDA scaler to the Helm charts of an existing microservice.This post is part of “Microservice Series - From Zero to Hero”.Deploy KEDA with an Azure DevOps Infrastructure as Code (IoC) PipelineYou can find the code of the demo on GitHub.In one of my previous posts, Use Infrastructure as Code to deploy your Infrastructure with Azure DevOps, I created a YAML pipeline in Azure DevOps to deploy my whole infrastructure to Azure. This included the Azure Kubernetes Service cluster, an SQL database server, several components for Kubernetes like a cert-manager and ingress controller, and many more. The pipeline uses YAML and Azure CLI to define the services. The big advantage of having such a pipeline is that I can easily create my whole infrastructure from scratch with a single button click. This makes it fast, and easily repeatable without worrying about forgetting anything.This pipeline is perfect to create a new namespace in the Kubernetes cluster, add the Helm chart for KEDA and install it.First, add the following two variables to the pipeline. This is not necessary but I like to have most of the configuration in variables at the beginning of the pipeline.Next, use the HelmDeploy task of Azure DevOps and add the Helm chart of KEDA to your Kubernetes cluster.After adding the Helm chart, update it with the following lines of code:The last step is to install the previously added Helm chart. Use the –create-namespace argument to create the namespace if it does not exist and also make sure to add a version number. Without the version, the deployment will fail.Add the Azure Service Bus Queue Scaler to an existing Microservice Helm chartYou can find the code of the demo on GitHub. If you want to learn more about Helm see Helm - Getting Started and Deploy to Kubernetes using Helm Charts.Inside the demo application, you can find the KedaDemoApi, and inside there are the charts and kedademoapi folder. Helm reads the YAML files inside this folder and creates Kubernetes objects. To add the KEDA scaler to the Helm chart, create a new file and name it kedascaler.yaml. This file will contain the ScaledObject which configures the trigger. The file has the following content:Next, create a second file, called kedatriggerauthentication.yaml, which will contain the Trigger-Authentication. This file will configure the access of the Scaled Object to the Azure Service Bus Queue and references a secret in Kubernetes.Helm is a template engine that replaces the placeholder in the double braces with the corresponding values in the values.yaml file. For example, {{ .Values.kedascaler.minReplicaCount }} will be replaced with the value of the variable minReplicaCount in the kedascaler section of the values.yaml file.Add the following values at the bottom of the values.yaml file:Replace Variable Values during the Deployment in the Continous Deployment PipelineThe values in the values.yaml file are hard-coded but it is also possible to pass variables. The secretKey value AzureServiceBus__ConnectionString is such a variable. You can set this variable in your CI or CD pipeline and use the Tokenizer task to replace AzureServiceBus__ConnectionString with the actual value of the variable. For more details, see Replace Helm Chart Variables in your CI/CD Pipeline with Tokenizer.Testing the ImplementationRun the Infrastructure pipeline and afterwards the CI pipeline of the KedaDemoApi. Both pipelines should finish successfully. Install KEDA using the Infrastructure Pipeline The Microservice with the Scaler was deployed successfully ConclusionCI/CD YAML pipelines in Azure DevOps can be used to easily install KEDA using Helm charts. This allows for fast, reproducible deployments which results in a low error rate. Helm can also be used to deploy the KEDA scaler with an existing microservice. This allows developers to quickly add the KEDA scaler to the Helm chart and also does not require any changes in the deployment pipeline to deploy the new scaler.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "KEDA - Kubernetes Event-driven Autoscaling", "url": "/keda-kubernetes-event-driven-autoscaling/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure, YAML, Docker, Helm, AKS, Kubernetes, Monitoring, Prometheus, Grafana, Istio, Kiali, KEDA", "date": "2021-09-20 00:00:00 +0200", "snippet": "Autoscaling is one of my favorite features of Kubernetes. So far we have discussed the Horizontal Pod Autoscaler (HPA) which can scale pods based on CPU or RAM usage. This is a nice start but especially in distributed applications, you often have several components outside of your pods. This can be an Azure Blob Storage, Azure Service Bus, MongoDB, or Redis Stream. The HPA can not scale your pods based on metrics from these components. That’s where KEDA comes into play.KEDA, Kubernetes event-driven autoscaling allows you to easily integrate a scaler into your Kubernetes cluster to monitor an external source and scale your pods accordingly.This post is part of “Microservice Series - From Zero to Hero”.What is KEDAKEDA is a Kubernetes event-driven autoscaler that allows you to scale your applications according to events that occur inside or outside of your Kubernetes cluster. It is very easy to install KEDA using a Helm chart and it also runs on any platform no matter what vendor or cloud provider you use. The community and the KEDA maintainers have created more than 45 built-in scalers that allow scaling on events from sources like Azure Service Bus, Azure Storage Account, Redis Streams, Apache Kafka, or PostgreSQL. Additionally, it provides out-of-the-box integration with environment variables, K8s secrets, and pod identity.Another neat feature is that KEDA can scale deployments or jobs to 0. Scaling to zero allows you to only spin up containers when certain events occur, for example, when messages are placed in a queue. This is the same behavior as serverless solutions like Azure Functions but this feature allows you to run Azure Functions outside of Azure.KEDA is a CNCF Sandbox project and you can find more information about the project on GitHub or Keda.sh.Deploy KEDA to your Kubernetes ClusterKEDA can be easily installed using Helm charts. If you want to learn more about Helm see Helm - Getting Started and Deploy to Kubernetes using Helm Charts.First, add the KEDA Helm repo and update it.Next, create a new namespace called keda and install Keda there. You must install the Helm chart in the keda namespace, otherwise, KEDA will not work.That’s already it. You have successfully installed KEDA in your Kubernetes cluster.Configure KEDA to scale based on an Azure Service Bus QueueYou can find the code of the demo on GitHub.After KEDA is installed, it is time to create your first scaler. The scaler is a custom resource in Kubernetes and is of the kind ScaledObject. This scaled object references a deployment or job that should be scaled, a trigger, in our example an Azure Service Bus, a reference to a Kubernetes secret that contains the connection string to the queue, and some configuration properties about the scaling itself.This scaled object defines that it should run a minimum of 0 replicas and a maximum of 10 of the kedademoapi. The cooldown period between scale events is 30 seconds and the queue it monitors has the name KedaDemo. When the queue has more than 5 messages, the scale-up event is triggered. I find this parameter a bit unintuitive but it is what it is.The second custom resource you have to define is a TriggerAuthentication. This object contains a reference to a Kubernetes secret that contains the connection string to the Azure Service Bus Queue. The SAS (Shared Access Signature) of the connection string has to be of type Manage. You can learn more about Azure Service Bus and SAS in Replace RabbitMQ with Azure Service Bus Queues.You can display the secrets of the namespace kedademoapi-test with the following command: Get Secrets of the Namespace You can also find the secret in the dashboard. See Azure Kubernetes Service - Getting Started for more information about Octant and how to access your Kubernetes cluster. The Secret in the Dashboard If you take a look at the screenshot above, you will see that the name and key of the TriggerAuthentication object correspond with the values in the dashboard.Place the ScaledObject and the TriggerAuthentication in a .yaml file and deploy it to the namespace where the application you want to deploy is running. Deploy it with the following command:You could also place both objects in separate files and execute the command for each file.Testing the Keda Service Bus ScalerIf you are using my KedaDemoApi, you can execute the Post method to write random messages into the queue. If you want to learn more about how to deploy applications into Kubernetes and how to set the connection string, see my “Microservice Series - From Zero to Hero”. Write Messages into the Queue Open the Azure Service Bus Queue and you will see the messages there. The Messages in the Queue This should automatically trigger the scale-out event and start more pods with your application. You can check the running pods with the following command:As you can see, five pods are now running. The Application was scaled out Testing Scale to 0You can use the Get method of the KedaDemoApi to receive all messages on the queue. The method will return the number of received messages. Receive all Messages from the Queue When you check the Azure Service Bus Queue, you will see that there are no messages left. No Messages are left in the Queue Since there are no messages left in the Queue, the KEDA scaler should scale the application to 0. Execute the get pods command again and you should see that no pods are running anymore. The Application was scaled to 0 ConclusionKEDA is a great tool to scale your workloads in Kubernetes. It allows you to choose from a wide variety of scalers and even lets you connect to external resources like Azure Monitor or a database like PostgreSQL. Especially the scale to 0 feature allows you to remove the allocated resources to a minimum and helps you to save money operating your Kubernetes cluster.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Unit Testing .NET 5 Console Applications with Dependency Injection", "url": "/unit-testing-net-5-console-applications/", "categories": "Programming", "tags": "C#, .NET 5, Dependency Injection, xUnit, Moq, FluentAssertions", "date": "2021-09-13 00:00:00 +0200", "snippet": "In my last post, I created a .NET 5 console application and configured dependency injection. This was pretty straight-forward and only needed a couple of lines of code.Today, I will show you how to create unit tests when using dependency injection in a .NET 5 (or .NET Core) console application.Create Unit Tests using xUnitYou can find the code of the demo on GitHub.Create a new .NET 5 test project using xUnit and create a reference to the main project. You can use any unit testing and faking framework you like. For this demo, I am using xUnit and Moq. Additionally, I am using FluentAssertions to make the assertions more readable.Implementing unit tests for a .NET 5 console application with dependency injection is the same as for any other project. The only minor difficulty is how to test the following method which creates an instance of IGreeter and then calls the Greet() method on it.The difficulty when testing this method is the IServiceProvider which has to be configured to be able to create an instance of the IGreeter interface.Create a fake Service ProviderThe solution to the problem is to create a fake IServiceProvider and IServiceScope object. The IServiceProvider fake then can be configured to return a fake IServiceScopeFactory object. This fake object can be used as the IServiceProvider parameter of the GreetWithDependencyInjection() method. The full code looks as follows:The code in the constructor might look complicated but if you start a new project, all you have to do is to copy it into the new project and you are good to go.Test Classes without the IServiceProvider InterfaceAs previously mentioned, every other class besides the Program.cs can be tested as you are used to. For example, testing the ConsoleGreeter is straight-forward. Create a new object of the ConsoleGreeter, add a fake interface in the constructor and then call the method you want to test. The method should return the value you expect.Testing the Unit TestsRun all the unit tests and you should see both running successfully. The Tests ran successfully ConclusionCreating unit tests for a .NET 5 console application that uses dependency injection only takes a couple of lines of code to configure the service provider. This code can be copied to any new project, making it even easier to set up.You can find the code of the demo on GitHub." }, { "title": "Configure Dependency Injection for .NET 5 Console Applications", "url": "/configure-dependency-injection-for-net-5-console-applications/", "categories": "Programming", "tags": "C#, .NET 5, Dependency Injection", "date": "2021-09-06 00:00:00 +0200", "snippet": "Back in the .NET framework days dependency injection was not the easiest task. Fortunately, Microsoft made it a first class citizen with the introduction of .NET Core. This is especially true for ASP .NET Core application where dependency injection can be used with a single line of code.Unfortunately, .NET Core (and .NET 5) console applications do not come pre-configured for dependency injection. Therefore, this post will show you how to configure it for you .NET 5 console application.Configure Depndency Injection for a .NET 5 Console ApplicationYou can find the code of the demo on GitHub.Create a new .NET 5 (.NET Core also works) application and install the Microsoft.Extensions.Hosting NuGet package. Next, create the following method which creates a DefaultBuilder and also allows you to register your instances with the dependency injection module.The AddTransient method configures dependency injection to create a new instance of the object every time it is needed. Alternatively, you could use AddScoped or AddSingleton. Scoped objects are the same within a request, but different across different requests and Singleton objects are the same for every object and every request.You can add as many services as your application needs.Use Dependency Injection in a .NET 5 Console ApplicationThe dependency injection module is already configured and now you can use it. Pass the Services of the previously created IHostBuilder to a new method where you can instantiate a new object with it.The GreetWithDependencyInjection method uses dependency injection to create a new instance of the IGreeter class. Since I previously configured to use the ConsoleGreeter for the IGreeter interface, I get an instance of ConsoleGreeter. This is normal dependency injection functionality and nothing console application specific though.After the object is instantiated, you can use it like any other object and, for example, call methods like Greet on it.Use Constructor InjectionCreating objects using dependency injection is good but it is still too complicated. It would be better if the needed objects are created automatically in the constructor. This is where constructor injection comes in handy. Again, this is nothing console application specific, just good old dependency injection but you create a class that accepts one or more interfaces in the constructor. The dependency injection module automatically passes the right object into the constructor without any developers work needed.Let’s take a look at the ConsoleGreeter class.This class takes an IFooService object in the constructor and then uses this object to call the DoCoolStuff() method inside the Greet() method. This enables developers to change the dependency injection configuration and therefore change the behavior of the application without touching the classes that use these objects.Testing the Dependency InjectionStart the application and you should see the following output in your console window. Testing the Dependency Injection When you replace the ConsoleGreeter with the ApiGreeter in the dependency injection configuration and start the program again, you should see a different greeting message. Testing the changed DI Configuration ConclusionDependency injection helps developers to write more testable and overall better applications. .NET 5 and .NET Core do not come with DI pre-configured but as you have seen in this post, it is very easy to configure it. All you have to do is to install the Microsoft.Extensions.Hosting NuGet package and add a couple lines of code.In my next post, I will add unit tests and how you how to configure the dependency injection in the test project.You can find the code of the demo on GitHub." }, { "title": "Add Istio to an existing Microservice in Kubernetes", "url": "/add-Istio-to-existing-microservice-in-kubernetes/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure, YAML, Docker, Helm, AKS, Kubernetes, Monitoring, Prometheus, Grafana, Istio, Kiali", "date": "2021-08-30 00:00:00 +0200", "snippet": "My last post highlighted some of Istio’s features and showed how to apply them to your microservices.In this short post, I will show you how to add Istio to an existing application.This post is part of “Microservice Series - From Zero to Hero”.Add Istio to an existing Application running KubernetesIf you already have an application or microservices running in your Kubernetes cluster and want to add Istio support, all you have to do is to add the following label to the namespace.The next time a pod is created, the new label will be applied and the sidecar will be injected automatically. You can verify the flow of your application in Kiali, once the label is applied. Visualize the request flow Note, of course you have to have Istio installed in your cluster For more information about Kiali and Istio, see my post Istio in Kubernetes - Getting Started.ConclusionIf you already have an application running in Kubernetes, all you have to do is add a label to the namespace to enable Istio. The label will be applied to pods when they are created the next time.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Use Istio to manage your Microservices", "url": "/use-istio-to-manage-your-microservices/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure, YAML, Docker, Helm, AKS, Kubernetes, Monitoring, Prometheus, Grafana, Istio, Kiali", "date": "2021-08-23 00:00:00 +0200", "snippet": "In my last post, I talked about Istio and how it can be installed. I also mentioned some of the features of the service mesh but only in theory.This post will show you some features of Istio, how to implement them and how they can help you build better applicationsThis post is part of “Microservice Series - From Zero to Hero”.Installing IstioIf you haven’t installed Istio or want more information about it, see my post Istio in Kubernetes - Getting Started.Request RoutingBy default, all requests are routed evenly between pods. If you have two different versions of a service running, for example, version 1 and version 2, the request will be routed once to v1 and once to v2. You can observe the request routing using Kiali. For more information about Kiali, see my last post Istio in Kubernetes - Getting Started. The Traffic is routed evenly As you can see, the requests are routed evenly between the details and the reviews service (red box) and also routed evenly within the reviews service to v1, v2, and v3 (blue box).Following DevOps principles, we want to deploy often. To deploy as often as possible, developers have to deploy features that are not fully ready yet or need more testing before all users can access the new feature. This is where request routing comes into play. You can configure Istio to route no requests to a specific service, a small percentage, for example, 1% of all requests, or only route the requests of a specific user to the new service.Especially routing a small percentage of the requests can be very useful. This is called canary deployment. Often companies only allow employees to use a new feature. After they confirmed that everything is working as expected, a small percentage of actual users get to use the feature. If everything is still ok, the percentage gets increased until it reaches 100%. If an error occurs that was not found during testing, only a small percentage of the users is affected, and not all.Configure Request RoutingThe demo application of Istio is great and you should use it as starting point in your Istio journey. Create virtual services with the following commandAlso, make sure that you created the default traffic rules while installing Istio.Let’s have a look at the defined routes which you just created.These services route traffic always to v1 of the service. For example, if the user calls the reviews service, the request is always routed to v1 of the reviews service.Once the routes are created, you can check that everything was applied correctly with the following commandCreate a couple of requests using your browser or the following loop “for i in $(seq 1 100); do curl -s -o /dev/null “http://$GATEWAY_URL/productpage”; done” and then go back to the Kiali graph dashboard. There you can see that the requests are only routed to v1 of the reviews service. The Traffic is only routed to V1 Routing is also possible based on the user identity. This can be useful if a tester wants to access the new service.You can delete the created routing rules with the following command:Fault InjectionFault injection is another cool feature of Istio which helps you to build more robust applications. Probably every developer has seen applications that work fine in testing but once under production load, problems start to surface. These problems can be hard to analyze or reproduce, especially in a microservice architecture.Istio’s fault injection allows you to intercept requests and return an HTTP error or delay requests to provoke a timeout.Inject HTTP 500 ErrorsIn the following example, I want to test how the application works when the review service is not available. The user should see a good error message and nothing cryptic. Additionally, I want to test this in my production environment and no user should be affected by it. Therefore, I configure the injection of the HTTP 500 error to only affect a user called Jason.Let’s have a look at the created virtual service.This virtual service applies the rule when the ratings service is called and the user matches jason. Then it injects a fault with a probability of 100% (always) and returns the HTTP code 500. You could change the 100 to 10 and test how the application behaves with “random” interrupts.Open the demo application and log in as jason without a password. You will see that the rating service is not available and the user gets a clear error message which does not interrupt the user experience. The Rating Service is not available as Jason Log out and you will see that the rating service works as expected. The Rating Service works if the user is not Jason When you are done with your testing, delete the virtual service with the following command:ConclusionIstio brings some cool features like fault injection and request routing which help developers to build more robust and resilient applications which will lead to a better user experience. The sample code provides a great starting point with many examples and will help you to get started.In my next post, I will show you how easy it is to add Istio to an existing application.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Istio in Kubernetes - Getting Started", "url": "/istio-getting-started/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure, YAML, Docker, Helm, AKS, Kubernetes, Monitoring, Prometheus, Grafana, Istio, Kiali", "date": "2021-08-16 00:00:00 +0200", "snippet": "My last post introduced the concept of a service mesh and how it can help to manage your Kubernetes cluster, especially with hundreds or thousands of pods running. The post was only theoretical and was probably a bit abstract if you have never worked with a service mesh.Therefore, I want to show you how to install Istio and a sample application on your Kubernetes cluster in this post.This post is part of “Microservice Series - From Zero to Hero”.Installing Istio in KubernetesThis demo assumes that you already have a Kubernetes cluster set up. If not see Azure Kubernetes Service - Getting Started to set up a new cluster and also configure the connection to Kubernetes. Usually, I work with Windows but for this demo, I will use the Windows Subsystem for Linux. You can use all commands on Linux or Mac as well.To get started, download the newest version of Istio. Download the newest Version of Istio Alternatively, go to GitHub and download the desired version.After the download is finished, navigate into the downloaded Istio folder and set the path to the /bin folder as the Path variable. Set the Istio Path Next, install Istio with the demo profile in your Kubernetes cluster. Istio comes with several profiles which have different configurations for the core components. The demo profile installs all components. You can find more information about the profiles in the Istio docs. Install Istio in Kubernetes The installation should only take a couple of seconds.Install an Istio Demo ApplicationIstio offers a nice demo application which I will use in this demo. Before you install it, I would recommend creating a new namespace in your K8s cluster. Additionally, set the istio-injection=enabled label on the namespace. This label configures the automatic injection of the Envoy sidecar. Create and tag the Namespace Next, install the sample app in the previously created namespace. Install the Sample Application After the sample application is installed, make sure that all services are up and running. Check the installed Services Also, check that all pods are started and running correctly. Check the installed Pods The demo application is only accessible through the internal network. To make it accessible from the outside, install the Istio ingress gateway in the same namespace. Install the Gateway The gateway acts as a load balancer and only provides an external URL. Use the following command to get the application’s URL and its ports. Get the IP Adress of the Sample Application Alternatively, use the following commands to read the URL and port and combine them in the GATEWAY_URL variable.Test the Sample ApplicationEnter URL:Port/productpage in your browser and you should see the sample application. Test the Sample Application Install Istio AddonsIstio comes with a wide range of additional software which can be easily installed with the following command. Install Istio Addons This installs every available addon. If you only want to install certain products, use their specific YAML files. For example, install only Prometheus with the following command.The above code installed many useful tools like Grafana, Jaeger, and Zipkin. I will talk more about these tools in my next post. For now, let’s take a look at Kiali which is a pretty cool tool to visualize the flow and useful information of requests in distributed applications. Kiali is already installed. All you have to do is to activate the port forwarding with the following command.Open your browser, enter localhost:20001 and you should see the Kiali dashboard. On the left side select Graph, then select the istio-demo namespace from the drop-down and you should see the services of the demo application. Execute the following command to produce 100 requests which then will be visualized in the graph.Play a bit around with the settings. You can, for example, enable the traffic animation and the response time of the requests. The graph also shows that most of the traffic is routed from the productpage to the details microservice. Traffic Flow with Kiali Analyzing Istio ErrorsIstio is very robust but sometimes things go wrong. You can analyze a namespace with the following command.If you followed this demo and don’t add a namespace (-n istio-demo), the analysis process will run in your current namespace (usually the default namespace) and will return an error that Istio is not enabled. Check for Errors ConclusionIstio is an easy to install service mesh that comes with many useful applications like Grafana, Prometheus, and Jaeger. This demo used the Istio demo application and showed how to visualize your microservice dependencies and the request flow using Kiali.In my next post, I will show you more features of Istio like fault injection, request routing, and traffic shifting between microservices.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Service Mesh in Kubernetes - Getting Started", "url": "/service-mesh-kubernetes-getting-started/", "categories": "Kubernetes, Cloud", "tags": "Docker, Helm, AKS, Kubernetes, Monitoring, Prometheus, Grafana, Istio, Consul, Linkerd", "date": "2021-08-09 00:00:00 +0200", "snippet": "If you have followed this series then you have learned what microservices are, how to program and deploy them, and how to monitor your Kubernetes cluster and applications. The last missing puzzle piece in your microservice journey is how to manage applications with hundreds or thousands of pods and microservices.This is where a service mesh comes into play. This post will talk about the pro and cons of a service mesh and also will give an overview of existing solutions.This post is part of “Microservice Series - From Zero to Hero”.What is a Service Mesh?Imagine you have an application running hundreds or thousands of microservices like Netflix already had in 2015. Netflix Microservices Architecture in 2015 (Source Youtube) These microservices probably have doubled by now. All these microservices are communicating with each other, need monitoring, tracing, and security like encryption. Configuring each microservice would take forever and is impractical in practice. To accomplish all these tasks you have to add a layer above your applications. This is where a service mesh comes into play.A Service mesh separates your business logic from managing the network traffic, security and monitoring. This should help to increase the productivity of the developers whereas network and operation specialists can configure the Kubernetes cluster. The separation is often achieved by using sidecars. This means that the service mesh injects an additional container (sidecar) into your pod. The sidecar is often an Envoy container which is a popular open-source proxy. The Envoy container intercepts requests from and to the container to modify the traffic.Another popular feature of service meshes is fault injection and rate-limiting. This allows you to configure how many requests randomly fail (returning HTTP 500) and also to time-out services if they send too many requests. Especially the fault injection helps developers to test error handling and often helps to make the application more resilient to gracefully handling errors.Service Mesh Projects for KubernetesThere are three big service mesh competitors in the Kubernetes world at the moment: Istio, Linkerd, and Consul.IstioIstio was originally developed by Lyft and has been adopted by many major technology companies like Google or Microsoft. Istio is the most popular service mesh solution and has an active community. As of this writing, the project has almost 28 thousand stars and more than 5 thousand forks on GitHub. Istio can be easily installed using Helm charts and also installs many useful tools to operate your Kubernetes cluster, like Prometheus, Jaeger, Grafana, and Zipkin.In my next post, I will show you how to install Istio using its Helm charts and how to configure the traffic management. For more information see istio.io.ConsulConsul is the probably second most popular open-source service mesh out there and has been developed by HashiCorp. Consul promises better performance compared to Istio because it uses an agent-based model in comparison to Istio’s multiple services. The agents run on the Kubernetes nodes and therefore can locally cache settings to improve the performance.I have not worked with Consul yet therefore I would recommend that you check out consul.io for more information about Consul’s features and how they work.LinkerdLinkerd is another popular service mesh and has been completely re-written for version 2. Both versions combined have around 12 thousand stars on GitHub and also have a very active community. Linkerd provides a good-looking dashboard to help operators understand what is happening inside the Kubernetes cluster in real-time. Additionally, it installs useful tools like Prometheus and Grafana.Linkerd can be installed via its own CLI. For more information see linkerd.io.Disadvantages of a Service MeshAs always, using a service mesh comes not only with advantages but also with some disadvantages. Probably the biggest disadvantage is that developers/operators need to learn yet another tool to run their application. Especially in the beginning, using a service mesh might add some problems due to more possibilities of misconfiguration and therefore breaking your application rather than helping.Another disadvantage of a service mesh is that it adds some overhead and especially additional resource usage. Istio measured that the Envoy proxy sidecar uses 350 mCPUs and around 40 MB memory per 1000 requests. Additionally, it adds 2.65 ms to the 90th percentile latency. This might not sound like much but if your cluster has 1000 pods running, it adds up.ConclusionA service mesh can help operators to manage their Kubernetes cluster managing traffic, security, and monitoring. A big advantage is that these features are separate from the business logic and can be independently managed and implemented. This post gave a very short introduction to services meshes and in my next post, I will show you how to install Istio and how to take advantage of the features it provides.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Create Grafana Dashboards with Prometheus Metrics", "url": "/create-grafana-dashboards-with-prometheus-metrics/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure DevOps, Azure, Nginx, YAML, CI-CD, Docker, Helm, AKS, Kubernetes, Monitoring, Prometheus, Grafana", "date": "2021-08-02 00:00:00 +0200", "snippet": "In my last post, I added the Prometheus client library to my .NET microservices. This was the first step to implementing monitoring of the microservices which are running in a Kubernetes cluster.In this post, I will show you how to create your first dashboard with Grafana using the Prometheus data.This post is part of “Microservice Series - From Zero to Hero”.Add the Microservices to the Prometheus Configuration for ScrappingYou can find the code of the demo on GitHub.You have to add the microservices as targets in Prometheus before you can visualize their Prometheus data. I installed Grafana Loki in a previous post which also installs Prometheus. You can find the details in Collect and Query your Kubernetes Cluster Logs with Grafana Loki.After you have installed Prometheus, open the configuration of the Prometheus server. You can use a Kubernetes dashboard and edit the yaml file in your browser. In Azure Kubernetes Service - Getting Started I explain how to use Octant as a dashboard to access your Kubernetes cluster.Open the dashboard and then select the namespace where you have installed Prometheus. If you followed this series, the namespace is loki-grafana. Navigate to the Config Maps and there you can find the configuration for the Prometheus server named loki-prometheus-server. Edit the Prometheus Configuration Select the YAML tab and add the following code under scrape_configs:This tells Prometheus to scrape data from a pod in the customer-api namespace. The pod has the label app=customerapi and the Prometheus data is available under /metrics. Save the config and then let’s check if customerapi is available in Prometheus.Inspect the Prometheus Scrap TargetsIn the Kubernetes dashboard, navigate to the loki-grafana namespace and select the loki-prometheus-server pod. There you can activate the port forwarding to access the pod from your computer. Open the Prometheus Server Pod in the Dashboard Click the button Start Forwarding and you will get a localhost URL that forwards your request to the Prometheus server instance. The URL in my case is http://localhost:53625. Activate the Port Forwarding Open the URL and navigate to /targets. There you should see the customerapi-test target as healthy. You can also see when a scrape happened the last time, how long it took, and if there were any errors. Inspect the Prometheus Scrap Targets Query Prometheus Data in GrafanaIf you don’t know how to access Grafana, see Collect and Query your Kubernetes Cluster Logs with Grafana Loki for information on how to configure the redirect and how to read the admin password from the Kubernetes secret.In Grafana, click on the round (compass?) symbol on the left and select Explore to open the query editor. Open the Grafana Query Editor Make sure that you select Prometheus as the data source on top of the page and then you can create your first queries. Click on metrics to get an overview of a lot of the built-in commands and get familiar with the functionality of the queries. A useful metric is a histogram where you can display a certain quantile, for example, the 95% quantile. You can see a query of the 95% quantile of the request duration. You can see that the requests were fast but at around 18:30 the request duration spiked to up to 100 seconds. This clearly indicates that there was a problem. Create a Histogram The functionality of PromQL is way too vast for this post today but you can find example in the Prometheus documentation.Import Grafana DashboardsGrafana allows you to import dashboards from the community which is a great starting point. You can find a list of all the dashboards on https://grafana.com/grafana/dashboards.Once you found a dashboard you like, for example the Kuberets Cluster (Prometheus) dashboard, copy its id. Copy the id of the Dashboard Next, click on the + button on the left of your Grafana GUI and select Import. Import a Grafana Dashboard On the import page of the dashboard, provide a name (or leave the default one) and select your Prometheus instance from the dropdown. Then click on Import and the dashboard gets imported. Configure the Dashboard Import After the dashboard is imported, it gets immediately displayed and gives you an overview of your cluster health, the cluster’s pods, and containers, and much more. The Dashboard gives an Overview of the Kubernetes Cluster The dashboard already gives you useful information but you can easily edit or remove its components and also can take a look at the definition of each component. This can help you when you try to create your own dashboards.Community dashboards are a good way to get started but you will always have to create your own dashboards to fulfill your requirements.Create your own Grafana Dashboard with Data from PrometheusTo create your own Grafana dashboard, click on the + button on the left side of the Grafana GUI and then select Add Query. Start creating a new Grafana Dashboard Select Prometheus as your data source on the next screen and enter a query. My query creates a histogram for the 99% quantile for the request duration in seconds over the last 10 minutes. Create a Histogram Query The Visualization tab allows you to configure the looks of your dashboard. I added points to the graph and added a label and unit for the left y-axis. Additionally, I added the min, max, and average values of the query to the legend on the bottom left of the graph. Configure the Visualization of the Dashboard There is a wide variety of options to customize your dashboard. Some are more and some are less useful like the following one. Use a Gauge to display your data On the next page, provide a title for the dashboard. You should always name all the axis and dashboards so everyone can easily see what this dashboard displays. Add a Title Lastly, click on the Save button and give your dashboard a name. Save your first Grafana Dashboard Congratulation, you created your first dashboard. You can add as many components as you want to this dashboard in the future. If you take a look at the one we have, you can immediately see that there is something not right with the service. Something is wrong with the monitored Service Between 21:55 and 22:01 the request time was around 5 - 7 seconds which does not look good compared to the very low request time the service usually has. At 22:07 the request duration spikes to around 35 seconds which is definitely not good and shows that something is massively wrong.ConclusionGrafana combined with Prometheus is a powerful tool to visualize your metrics from each component of your Kubernetes cluster and even from single deployments or pods. This post showed how to add new targets to the Prometheus scrap configuration and how to import and create your own dashboards to display the request duration.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Monitor .NET Microservices in Kubernetes with Prometheus", "url": "/monitor-net-microservices-with-prometheus/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure DevOps, Azure, Nginx, YAML, CI-CD, Docker, Helm, AKS, Kubernetes, Monitoring, Prometheus", "date": "2021-07-26 00:00:00 +0200", "snippet": "Knowledge is power is a famous saying which also holds true when running software. You would like to know how many errors occur, how long the response times of your services are, and also how many people are even using your application.In my last post, I added Grafana Loki to log events but it also installed Prometheus which I will use in this post to log messages from my microservices in my Kubernetes cluster.This post is part of “Microservice Series - From Zero to Hero”.What is Prometheus?Prometheus is an open-source monitoring and alerting system and was the second project to join the CNCF (Cloud Native Computing Foundation) in 2016. It has more than 6000 forks and almost 40K stars on GitHub with a very active community. Prometheus collects data using a pull model over HTTP and does not rely on distributed storage solutions. This allows you to create several independent Prometheus servers to make it highly available.The components of Prometheus are the server itself, client libraries, an alert manager, and several other supporting tools.The client libraries allow you to publish log messages which can be scraped by Prometheus. Further down, I will show you how to do that in .NET 5. The Alertmanager can send alerts to a wide range of systems like email, webhooks, Slack, or WeChat.Prometheus can be used as a data source in Grafana where you can easily create beautiful dashboards. I will show you this in my next post.Available Metric TypesPrometheus knows the following four metric types: Counter: A counter whose value can only increase or bet set at zero. Gauge: A number that can go up or down. Histogram: Samples of observations that are counted in buckets. Summary: Like the histogram but can calculate quantiles.Installing the Prometheus Client Library in .NET 5You can find the code of the demo on GitHub.Installing the C# Prometheus client libraries is very simple. All you have to do is to install the prometheus-net and prometheus-net.AspNetCore NuGet packages Install the Prometheus NuGet Packages Next, add the metric server to expose the metrics which can be scraped by Prometheus. Use the following code and make sure you add it before app.UseEndpoints… in the Startup.cs class:Start your application, navigate to the /metrics endpoints and you will see the default metrics which are exposed from the client library. Prometheus default metrics The metrics might look confusing at first but they will be way more readable once you created your first dashboard.Add Prometheus MetricsThe Prometheus client libraries allow you to easily create your own metrics. First, you have to collect the metrics. You can use the following code to create a counter and histogram:The code above will expose the response time for each HTTP code, for example, how long was the response for an HTTP 200 code. Additionally, it counts the total number of requests. To use the code above, create a new middleware that uses the MetricCollector.The last step is to register the new middleware in the Startup.cs class. Add the following code to the ConfigureServices method:After registering the collector, add the middleware to the Configure method:When you start your microservice and navigate to /metrics, you will see the number of different requests and the response time. Test the custom metrics The code for the MetricCollector and the middleware were taken from GitHub.ConclusionPrometheus is the go-to solution for collecting metrics in dockerized environments like Kubernetes. This demo showed how to add the Prometheus client library to a .NET 5 microservice and how to create your own metrics.In my next post, I will show you how to use these metrics to create dashboards with Grafana.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Collect and Query your Kubernetes Cluster Logs with Grafana Loki", "url": "/collect-and-query-kubernetes-logs-with-grafana-loki/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure DevOps, Azure, Nginx, YAML, CI-CD, Docker, Helm, AKS, Kubernetes, Logging, Monitoring", "date": "2021-07-19 00:00:00 +0200", "snippet": "So far I have talked about the development, deployment, and configuration of microservices and Kubernetes in this series. Over the next couple of posts, I want to talk about an essential topic when it comes to running your services in production: logging and monitoring.In this post, I will show you how to install the Grafana Loki stack and how you can query the logs of everything inside a Kubernetes cluster.This post is part of “Microservice Series - From Zero to Hero”.The Components of Grafana LokiGrafana Loki installs various components which are used for different tasks. Let’s have a look at them:GrafanaGrafana is a tool to query, visualize and alert metrics. These metrics can be stored in a wide variety of data sources and can be brought into Grafana via plugins. Grafana is mostly known for its beautiful graphs to display everything you want to know about your infrastructure and applications.You can find more details on the Grafana website. In a future post, I will show you how to create your own dashboards.PrometheusPrometheus is an open-source project for monitoring and alerting and is also one of the oldest projects of the Cloud Native Computing Foundation (CNCF). Some of the main features of Prometheus are: PromQL, its own flexible query language a multi-dimensional data model for time series data collecting logs via a pull modelYou can find more details on the Prometheus website. In my next post, I will show you how to use Prometheus to collect and display metrics from your microservices and Kubernetes cluster.PromtailPromtail is an agent for Loki. It discovers services, for example, applications inside a Kubernetes cluster or nodes of the cluster, and sends the logs to Loki. You can find more details in the Promtail documentation.LokiLoki is a highly available logging solution. It gets described by its developers as “Like Prometheus but for Logs”. It indexes the labels of the logs and allows querying of these logs using LogQL. You can find more details in the Promtail documentation.Installing Grafana Loki using HelmYou can find the code of the demo on GitHub.The Grafana Loki stack can be easily installed with Helm charts. If you are new to Helm, check out my previous posts Helm - Getting Started and Deploy to Kubernetes using Helm Charts for more details.Use the following code to add the Grafana Helm chart, update it and then install Loki:Installing Grafana Loki using an Infrastructure as Code PipelineIn one of my past posts, I have created an Infrastructure as Code (IaC) Pipeline using Azure CLI. If you want to automatically deploy Loki, add the following code after the Kubernetes deployment in your pipeline:The above code adds two variables, for the version of Loki and the namespace where it gets deployed and then does basically the same as the Helm deployment in the section above.Get the Credentials for GrafanaAfter installing Grafana Loki, you have to read the admin password from the secrets. You can do this in PowerShell with the following code:Use the following code when you are on Linux:These commands get the secret for the admin password, decode it from base 64, and then print it to the console.Access GrafanaBefore you can access Grafana, you have to create a port forwarding to access the Grafana service. You can do this with the following code: Configure the port forwarding to access Grafana This configuration enables you to access the Grafana dashboard using http://localhost:3000. Enter the URL into your browser and you will see the Grafana login screen. The Grafana login screen The user name is admin and the password is what you decoded in the previous step.Create your first Query in LokiAfter the successful login, click on the round icon on the left and select Explore to open the query editor. Open the Loki query editor On the top of the page, select Loki as your data source and then you can create a simple query by clicking on Log labels. For example, select pod and then select the loki-grafana pod to query all logs from this specific pod. The query looks as follows: Create your first Loki query The query will display all logs from the grafana-loki pod. For more details about a log entry, click on it. You can see a successful login on the following screenshot. A successful login got logged Filter your LogsDisplaying all the logs is nice but not really useful. Usually, you want to filter your logs and only display problematic entries like errors. You can use the following query to display only log entries with the error level:This query is more useful than displaying all logs but often you also want to search for a specific log message. You can do this easily by extending the query:This query displays all error logs with an Invalid Username or Password in the log message. Query only error logs LogQL Query LanguageAll the queries above use LogQL. The examples above are just scratching the surface of what you can do with LogQL but it should be enough to get you started. For more details about LogQL, check out the documentation.ConclusionAnalyzing logs is essential for every project running in a production environment. Grafana Loki offers a lot of functionality out of the box like automatically collecting logs from each object in Kubernetes and sending it to Loki where you can query and filter these logs. The examples in this post are very simple but should be enough to get you started.In my next post, I will show you how to use Prometheus, which was also installed by Loki, to scrape metrics from the microservices.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Debug Microservices running inside a Kubernetes Cluster with Bridge to Kubernetes", "url": "/debug-microservices-bridge-kubernetes/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure DevOps, Azure, Nginx, YAML, CI-CD, Docker, Helm, AKS, Kubernetes", "date": "2021-07-12 00:00:00 +0200", "snippet": "Kubernetes is a great tool to run and manage our applications. An important aspect of using a tool like Kubernetes is how easy it is to find and fix problems. Tracing and debugging a microservice architecture can be quite hard and is perhaps the biggest downside of it.Microsoft allows developers to route traffic from a Kubernetes cluster to their local environment to debug microservices without the need to deploy a full cluster on your developer machine.This post is part of “Microservice Series - From Zero to Hero”.What is Bridge to KubernetesBridge to Kubernetes, formerly known as Azure Dev Spaces, is a tool that allows to test and debug microservices on a developer’s machine while using dependencies and configurations of an existing Kubernetes cluster. This is especially useful when you have a bug in a production environment which you can not replicate in your development environment. This bug might only occur in high-load scenarios. For more information, see the Bridge to Kubernetes GA announcement.Configure Bridge to KubernetesYou can find the code of the demo on GitHub.In this demo, I will change one of my microservices and debug it with requests from my AKS cluster. If you don’t have a cluster running yet, see “Microservice Series - From Zero to Hero” for all the tutorials to create a microservice, AKS cluster, and how to deploy it.Install the Visual Studio Code ExtensionsOpen the microservice you want to debug in Visual Studio Code and open the Extensions tab. Search for Kubernetes and install the Kubernetes and Bridge to Kubernetes extensions. Install the Bridge to Kubernetes Extension Configure the Kubernetes Namespace for DebuggingAfter installing the Extensions, press CRTL + Shift + P, type in Bridge, and select Bridge to Kubernetes: Open Menu. Open the Kubernetes Menu This opens the Kubernetes menu where you can see your clusters. Open the Namespace tab of the cluster you want to debug and make sure the right namespace is selected. The selected namespace has an asterisk (*) at the beginning. If the wrong namespace is selected, right-click the namespace you want to use and select “Use Namespace” Configure the Namespace Configure Bridge to KubernetesAfter setting the namespace, press CRTL + Shift + P again and type Kubernetes: Debug. From the drop-down, select Kubernetes: Debug (Local Tunnel). Start configuring the Bridge to Kubernetes This opens the configuration assist for the connection into the Kubernetes cluster. First, select the service you want to debug. Since my namespace only has one service, I only see the customerapi service in the drop-down. Select the service to debug Next, enter the port on which your microservice is running on your local machine. You can either start your application and check the port, or you can open the launchSettings.json file in the Properties folder and check there.The port is 5001 in my case. Configure the port of your local microservice In the next step, select a launch config for your microservice. If you use my demo, you can find this config under CustomerApi/CustomerApi/.vscode. Select a launch config In the last step, configure if you want to redirect all requests to your machine or only specific ones. In a production environment, you would want to only redirect your requests but for this demo, select No to redirect all requests to your machine. Configure what requests get redirected Start debugging your MicroserviceLet’s imagine that there is a new feature for the microservice and from now on the PrimeNumber controller should not calculate a prime number anymore but rather return the double of the entered number (I know it is a stupid example but that’s fine for this demo). Change the code of the Index action in the PrimeNumber controller to return the entered number * 2 and set a break-point to debug the code later.Now you are ready to start the microservice. Press CRTL + Shift + D or select the debug tab and then start the application with the launch setting with the Kubernetes configuration. Start with the Kubernetes settings If Visual Studio Code can establish a connection to the Kubernetes cluster, the bar on the bottom will turn orange. The bar turns orange when connected to K8s Debug Requests from your Kubernetes ClusterAfter starting the microservice, open the public URL of the application running in your Kubernetes cluster. In my case that’s test.customer.programmingwithwolfgang.com. This request displays the Swagger UI. There, open the PrimeNumber method, enter a number and execute the request. The request was redirect to your local machine You can see that the breakpoint in the application running locally on your machine was hit with that request. When you continue the execution, you will also see that the return value is what you expected from your new feature (the user entered 11, 11 * 2 = 22). The request was processed in your microservice running locally When you stop the application on your machine and execute the request again, you will see that the return value changed to 31 (which is the 11th prime number). The request without debugging ConclusionModern applications become more and more complex and therefore get harder to debug. Bridge to Kubernetes allows developers to redirect requests inside a running Kubernetes cluster onto their own machine to debug microservices. This allows debugging code without the need to set up a whole environment and also allows using the dependencies and configuration of the running K8s cluster.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Fixing NuGet.targets(131,5) error The local source doesn't exist", "url": "/nuget-targets-local-source-does-not-exist/", "categories": "Miscellaneous", "tags": "NuGet", "date": "2021-07-05 00:00:00 +0200", "snippet": "Today, I was trying to add a new NuGet package to my .NET 5 project and I got the following error message: C:\\Program Files\\dotnet\\sdk\\5.0.301\\NuGet.targets(131,5): error : The local source ‘C:\\Users\\Wolfgang\\Desktop\\poc-microservice-main\\ProductService\\ProductService.Catalog’ doesn’t exist. I know that the file doesn’t exist because I deleted it weeks ago after helping a reader of my blog. Installing a NuGet package failed I was quite annoyed by the error message because and I couldn’t find out what was wrong. Also checking the mentioned NuGet.targets file didn’t helpt me. Fortunately the solution was quite simple.Open a PowerShell window and enter the following code:This creates a new nuget.config file and fixed the problem for me. Create a new NuGet config file " }, { "title": "Use Infrastructure as Code to deploy your Infrastructure with Azure DevOps", "url": "/use-infrastructure-as-code-to-deploy-infrastructure/", "categories": "DevOps, Cloud", "tags": "DevOps, Azure DevOps, Azure, Nginx, YAML, CI-CD, Docker, Helm, AKS, Kubernetes, IaC", "date": "2021-06-28 00:00:00 +0200", "snippet": "Back in the day developers had to go through a lengthy process to get new hardware deployed. Often it took several weeks and then there was still something missing or the wrong version of the needed software installed. This was one of my biggest pet peeves and it was a major reason why I left my first job.Fortunately, we have a solution to these pains, Infrastructure as Code. This post will explain what it is and how you can easily set up all the services needed.This post is part of “Microservice Series - From Zero to Hero”.What is Infrastructure as Code (IaC)?As the name already suggests, Infrastructure as Code means that your infrastructure and its dependencies are defined in a file as code. Nowadays the configuration is often saved in a JSON or YAML file. IaC has many advantages over the old-school approach of an operation person creating the infrastructure: The definition can be reviewed and saved in version control Infrastructure can be deployed fast and reliable Deployments can be repeated as often as needed No (less) communication problems due to developers writing the configuration themselves Many tools are availableIf you are working with Azure, you might be familiar with ARM templates. There are also many popular tools out there like Puppet, Terraform, Ansible and Chef. My preferred way is Azure CLI. In the following sections, I will show you how to create an Azure DevOps YAML pipeline using Azure CLI and Helm to create an Azure Kubernetes Cluster with all its configurations like Nginx as Ingress Controller, Azure SQL Database, Azure Function, and Azure Service Bus.Azure CLI DocumentationI like using Azure CLI because it is easy to use locally and it is also quite intuitive. All commands follow the same pattern of “az service command”, for example, az aks create or az sql server update. This makes it very easy to google how to create or update services. Additionally, the documentation is very good. You can find all commands here.Create your first Infrastructure as Code Pipeline in Azure DevOpsYou can find the code of the demo on GitHub.Create a new pipeline and define the following variables:The variables should be self-explanatory when you look at their usage later on. Also if you followed this series, you should have seen all names and services before already. You need an existing Azure Service Connection configured in Azure DevOps. If you don’t have one yet, I explain in Deploy to Azure Kubernetes Service using Azure DevOps YAML Pipelines how to create one.Next, install Helm to use it later and create a resource group using the Azure CLI. This resource group will host all new services.If you are unfamiliar with Helm, see Helm - Getting Started and Deploy to Kubernetes using Helm Charts for more information.Create an Azure Kubernetes ClusterCreating an AKS cluster is quite simple due to the names of the parameters. For example, you can configure the VM size, what Kubernetes version you want to install, or the node count of your cluster. The full command looks as follows:Install the Cert-Manager AddonThe Cert-Manager adds SSL certificates to your services running inside AKS to allow the usage of HTTPS. If you read Automatically issue SSL Certificates and use SSL Termination in Kubernetes, then you will be familiar with the following code since it is identical.Add the Certificate Cluster IssuerThe SSL certificates need to be issued using the Cluster Issuer object. I am using the same YAML file as in Automatically issue SSL Certificates and use SSL Termination in Kubernetes except that this time it is applied as inline code and with the variable for the email address.Install Nginx and configure it as Ingress ControllerIn Set up Nginx as Ingress Controller in Kubernetes, I added Nginx and configured it as Ingress Controller of my AKS cluster. To install Nginx in the IaC pipeline, add its Helm repository, update it and then install it with the following commands using Azure CLI:Create a new Azure SQL ServerAfter the deployment of the AKS cluster is finished, let’s add a new Azure SQL Server with the following command:You might miss the variables SqlServerAdminUser and SqlServerAdminPassword. Since these values are confidential, add them as secret variables to your Azure DevOps pipeline by clicking on Variables in the top-right corner of your pipeline window. Add the database variables as secret variables By default, the Azure SQL Server does not allow any connections. Therefore you have to add firewall rules to allow access to the SQL Service. The following code enables Azure resources like Azure DevOps to access the SQL Server.Feel free to add as many firewall rules as you need. All you have to do is to edit the start and end IP address parameters.Deploy an Azure Service Bus QueueTo create an Azure Service Bus Queue, you also have to create an Azure Service Bus Namespace first. I talked about these details in Replace RabbitMQ with Azure Service Bus Queues.To allow applications to read or write to the queue, you have to create shared access signatures (SAS). The following commands create both a SAS for listening and sending messages.Create an Azure FunctionThe last service I have been using in my microservice series (“Microservice Series - From Zero to Hero”) is an Azure Function. Before you can create an Azure Function using Azure CLI, you have to create a Storage Account and an App Service Plan.With the Azure Storage Account and App Service Plan set up, create the Azure Function.Create all your Infrastructure using the IaC PipelineRun the pipeline in Azure DevOps and all your services will be created in Azure. Run the IaC pipeline As you can see, the pipeline ran for less than 10 minutes and deployed all my services. This is probably faster than you can click and configure all the services in the Azure Portal. If you want to deploy the services to a different Azure subscription or with different names, all you have to do is to change the variables and run the pipeline again.This makes it very safe and fast to set up all the infrastructure you need for your project.ConclusionInfrastructure as Code (IaC) solves many problems with deployments and enables development teams to quickly and reliably deploy the infrastructure. You can choose between many tools like Ansible, Terraform, or Chef. Alternatively, you can keep it simple like I did in the demo and use Azure CLI. The advantage of the Azure CLI is that you can easily use it locally for testing.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Deploy every Pull Request into a dedicated Namespace in Kubernetes", "url": "/deploy-every-pull-request-into-dedicated-namespace-in-kubernetes/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure DevOps, Azure, Nginx, YAML, CI-CD, Docker, Helm, AKS, Kubernetes", "date": "2021-06-21 00:00:00 +0200", "snippet": "Kubernetes enables developers to deploy quickly into test and production environments. Following the principles of DevOps of deploying fast and often, we should not stop there. Rather DevOps should enable developers to deploy each feature to a dedicated environment for it to be tested and therefore be deployed into production as fast as possible.This post will show you how to deploy every pull request into its own Kubernetes namespace with its own unique URL. Doing so will bring projects one step closer to continuous deployments.This post is part of “Microservice Series - From Zero to Hero”.Deploy the Helm package during a Pull Request BuildYou can find the code of the demo on GitHub.If you have not enabled CI builds during pull requests, see Run the CI Pipeline during a Pull Request for more information on how to run your pipeline.Deploying the Helm package during the pull request is almost the same as the deployment to the test environment:The most notable change is the condition that checks the source branch of the build. Since the CI and CD pipeline are separated and the CD pipeline is triggered by the CI pipeline, you can not access the source branch directly. Therefore you have to reference the source branch of the CI build. A pull request branch always starts with refs/pull. You can check this with the following condition:The next change to the regular deployment is obtaining the pull request id. The pull request id is great to distinguish between pull requests and therefore will be used as part of the URL and namespace. Since the CD pipeline is triggered by the CI pipeline and not the pull request directly, it is not possible to access the pull request id through the built-in variable System.PullRequest.PullRequestId. Instead, I use the following PowerShell command which is placed inside the GetPrId.yaml template:For the Kubernetes namespace, use a meaningful name and add the pull request id, for example, customerapi-pr-123. The URL almost works the same way, for example, pr-123.customerapi.programmingwithwolfgang.com. If you followed this series and have the DNS already configured for subdomains, then you do not have to change anything in the DNS settings. If you haven’t made the DNS configuration, see Configure custom URLs to access Microservices running in Kubernetes for more information.Cleaning up the Pull Request DeploymentsAs you have seen, deploying pull requests is quite easy. It is also important to not forget to clean them up after someone reviewed them. Otherwise, your Kubernetes cluster will be full soon.Deleting the previously created namespace inside the Azure DevOps YAML pipeline is quite simple:This code reads the pull request id to find the correct namespace name and then uses kubectl to delete the Kubernetes namespace. The more interesting question is how you can control when the namespace gets deleted. The code above runs immediately after the deployment is finished. This means that you create a new namespace and in the next step delete it again.Having control over when the stage runs can be achieved with approvals in Azure DevOps.Add an Approval before deleting the Pull Request NamespaceTo add an approval go to Pipelines and then Environment in your Azure DevOps project. There you can see your environment for deleting the namespace, in my case customerapi-PR-Delete, when you ran the deployment already at least once. If you can’t see it, click on New environment and add it with the same name as in your pipeline.Click on the environment and then click on the three dots on the top right and select Approvals and checks. Open the Environment Settings This redirects you to a new page where you can click on Approvals. Add an Approval before the deployment Select who is allowed to approve and optionally configure the timeout and if the approver is allowed to approve their own deployment. Configure the Approval When you create a new pull request, you will see that after the deployment succeeded, an approval is necessary to execute the deletion step. Often the approver is someone from QA or marketing. This person can check if the feature looks as requested and if the basic functionality is as expected. More in-depth tests should be automated. The Deployment needs to be approved For more detailed explanations about approvals in Azure DevOps, see Approvals for YAML Pipelines in Azure DevOps.Testing the Pull Request DeploymentCreate a new pull request to trigger the CI pipeline. Create a new Pull Request As you can see on the screenshot above, the pull request id is 61.After the Helm deployment is finished, the task also prints the URL of the new service. As you can see the URL pr-61.customer.programmingwithwolfgang.com matches the pull request id. The Pull Request got deployed to an unique URL Click on the URL and your browser opens the previously deployed microservice. As you can see, the URL works and also has a valid SSL certificate that was created automatically. For more information about creating SSL certificates inside your Kubernetes cluster, see Automatically issue SSL Certificates and use SSL Termination in Kubernetes Everything works as expected When you open your Kubernetes dashboard or check your namespaces in the Azure portal, you will see the Kubernetes namespace corresponding to the pull request id. The namespace matches the PR id Further ImprovementsAs you can see, this demo uses a very simple pull request deployment and only uses the Helm package for the microservice. This microservice has no connection to a queue or a database. You have to consider how many connections or dependencies the PR deployment in your project needs.In case you want a full application with queue and database, then add them to the PR deployment. The steps are basically the same as in the other deployments and you can use the PR id again to distinguish the PR database from the test or production database.If you add additional services, make sure to not forget to delete them. Especially databases can be expensive in Azure if you never delete them.ConclusionModern DevOps environments should strive to deploy as fast and as often as possible. To achieve this, deploy every feature into a dedicated environment and if it passes the tests, deploy it to your production environment. This allows developers to deliver features faster and also helps to narrow down bugs due to the small scope of each deployment.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Deploy Microservices to multiple Environments using Azure DevOps", "url": "/deploy-microservices-to-multiple-environments-azure-devops/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure DevOps, Azure, Nginx, YAML, CI-CD, Docker, Helm, AKS, Kubernetes", "date": "2021-06-14 00:00:00 +0200", "snippet": "A nice feature of Kubernetes is that you can easily set up new environments like test, QA, or production. Especially when you already have a CD pipeline, it is easy to extend it and deploy it to additional environments.In the following post, I will add a deployment to the production environment, set up a unique URL, and also deploy the database for the environment.This post is part of “Microservice Series - From Zero to Hero”.Update the existing Deployment to the Test EnvironmentYou can find the code of the demo on GitHub.So far, I have used the CD pipeline to deploy to a stage called test. I plan to change the URL of the test environment and then add a deployment of the Helm package and the database to a production environment.First, add a new subdomain to the URL of the test environment. You don’t have to use a subdomain but this makes the configuration of the DNS settings easier. For the test environment, I will use test as a subdomain which means the URL will be teast.customer.programmingwithwolfgang.comAdditionally, add the environment name also to the database so you can distinguish the test and production databases. That is everything you have to update for the test environment.Add the Production environment to the CD PipelineAdding a new environment is quite easy. Copy the deployment of the test environment and copy it beneath it. Next up replace test with prod, for example, the DeploymentEnvironment variable is now prod.Since I don’t want to use a subdomain for my production URL, remove it which means the URL will be customer.programmingwithwolfgang.comLastly, we only want to deploy to production when the deployment to the test environment is finished and was successful. To achieve that, add the dependsOn keyword and the following condition.In the future, you could add an approval before the production gateway. This means that the production deployment is only executed when a human approves the deployment.The finished CD PipelineThe finished CD pipeline looks as follows:Configure the DNS for the Test EnvironmentYou have to add the new subdomain in your DNS settings before you can use it. To make changes easier for the future, add a wildcard subdomain in your DNS settings. You can see my settings for both my microservices on the following screenshot. Configure the DNS settings for the subdomain For more information about setting up the DNS configuration, see my post Configure custom URLs to access Microservices running in Kubernetes.Testing the new DeploymentRun the pipeline and after the Helm upgrade task, you will see the URL of the test environment printed to the output window in Azure DevOps. Azure DevOps prints the URL after the deployment Click on the URL and you will see that the microservice is running and also using HTTPS with a valid SSL certificate. The test environment is up and running For more information about setting up HTTPS and creating SSL certificates automatically using Let’s Encrypt, see Automatically issue SSL Certificates and use SSL Termination in Kubernetes.Azure DevOps gives you a nice overview of the different environments and stages which gives you a nice overview of the status of all the deployments. As you can see, everything finished successfully. The deployment was successful Enter your URL without the subdomain into your browser and you will see that your production microservice is also running. The Microservice is running Lastly, connect to your SQL Server and you will see all four databases after the deployment (the OrderApi and CustomerApi have two databases each). All databases got deployed For more information on how to deploy databases in a CD pipeline, see Automatically Deploy your Database with Dacpac Packages using Linux and Azure DevOps.ConclusionHaving a CD pipeline in place enables you to easily add new environments to your deployment. A typical CD pipeline consists of test and production deployments but you can easily add QA or other environments.In my next post, I will show you how to deploy every pull request into a new environment and automatically create a unique URL.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Split up the CI/CD Pipeline into two Pipelines", "url": "/split-up-the-ci-cd-pipeline-into-two-pipelines/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure DevOps, Azure, Nginx, YAML, CI-CD, Docker, Helm, AKS, Kubernetes", "date": "2021-06-07 00:00:00 +0200", "snippet": "Modern DevOps is all about delivering high-quality features in a safe and fast way. So far, I am using a single pipeline for all of my CI/CD. This works fine but also makes it harder to make changes in the future. In software development, we have the Single Responsibility Principle (SRP) which states that a class or method should only do one thing. This makes the code simpler and easier to understand. The same principle can be applied to the CI/CD pipeline.This post shows how to split up the existing CI/CD pipeline into a CI and a CD pipeline. The separation will make the pipelines simpler and therefore will promote the ability to make changes quickly in the future.This post is part of “Microservice Series - From Zero to Hero”.Things to consider before splitting up the CI/CD PipelineYou can find the code of the demo on GitHub.Splitting up the pipeline into a separate CI and CD pipeline is quite simple and barely needs any changes. If you are new to this series, check out Improve Azure DevOps YAML Pipelines with Templates first and read about the templates used in the CI/CD pipeline.The only thing you have to consider before splitting up the CI/CD pipeline is how the CD pipeline knows that it should run and how it can access files from the CI pipeline.Edit the Pipeline for Continuous IntegrationContinuous Integration or CI means that you want to integrate new features always into the master branch and have the branch in a deployable state. This means that I have to calculate the build version number, build the Docker image, push it to Dockerhub and create the Helm package which will be used in the CD pipeline for the deployment. All this exists already and the only step I have to add is publishing the values.yaml file and the Helm package to the ArtifactStagingDirectory. This is a built-in variable of Azure DevOps and allows the CD pipeline to access the Helm package and the config file during the deployment.The changes are made in the CreateHelmPackage template. The whole template looks as follows:Splitting up the CI and CD part also helps to make the pipeline smaller. For example, only around half of the variables are needed for the CI pipeline. The whole CI pipeline looks as follows:Creating the Continuous Deployment PipelineThe Continuous Deployment (CD) pipeline might look a bit complicated at first, but it is almost the same as it was before. The first step is to create a new file, called CustomerApi-CD in the pipelines folder and then configure a trigger to run the pipeline after the CI pipeline. This can be achieved with the pipelines section at the top of the pipeline.This code references the CustomerApi-CI which is the name of the CI pipeline and runs when there are changes on the master branch or if a pull request triggered the CI pipeline. Next, change the path to the Helm chart package and the values.yaml file. These files were uploaded by the CI pipeline and can be found in the Pipeline.Workspace now. This is a built-in variable of Azure DevOps and allows you to access files uploaded by the CI pipeline. The path to the files looks as follows:The next step is to download the Helm package and the database. This section stays almost the same, except that you have to download the Helm package from the CI pipeline. This is done with the following code.The finished CD PipelineThe finished CD pipeline looks as follows.Test the new CI and CD PipelinesBefore you can use the new CD pipeline, add it to your Azure DevOps pipelines. To do that, open the Pipelines in your Azure DevOps project, click New pipeline and then select the location where your code is stored. On the next page, select Existing Azure Pipelines YAML file and then select the path to the new CD pipeline from the drop-down menu. Add the new CD Pipeline to Azure DevOps If you used any secret variables, for example, for the database password, do not forget to add them to the new CD pipeline before you start the deployment.Run the CI pipeline and after the build is finished, the CD pipeline will kick off and deploy the application. After the deployment is finished, open your browser and enter your URL (configured in the URL variable in the CD pipeline) and you should see the Swagger UI of the microservice. The Microservice is running To access the microservice using the URL, you have to configure the DNS accordingly. See Configure custom URLs to access Microservices running in Kubernetes to learn how to do that using an Nginx ingress controller.ConclusionUsing two pipelines, one for CI, and one for CD can be achieved quite easily and helps you to keep the complexity inside the pipelines at a minimum. This allows you to add new features, for example, new environments fast and with a smaller chance of making any mistakes.In my next post, I will use the newly created CD pipeline and add a production stage with its own URL and database deployment.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Automatically issue SSL Certificates and use SSL Termination in Kubernetes", "url": "/automatically-issue-ssl-certificates-and-use-ssl-termination-in-kubernetes/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure DevOps, Azure, Nginx, YAML, CI-CD, Docker, Helm, AKS, Kubernetes, TLS, SSL", "date": "2021-05-31 00:00:00 +0200", "snippet": "In my last post, I created an Nginx ingress controller and assigned different URLs to its public URL. The Nginx controller analyses the URL and routes the traffic automatically to the right application. The solution presented worked but only used HTTP. Nowadays, browsers show a warning when the connection is not using HTTPS and users also expect to have secure connections.This post will show you how to configure a cert-manager and automatically issue certificates for your applications.This post is part of “Microservice Series - From Zero to Hero”.Install Cert-ManagerYou can find the code of the demo on GitHub.Cert-Manager is a Kubernetes add-on that issues automatically TLS certificates for your applications. You can find it on GitHub. To install the cert-manager using Helm charts, execute the following commands:I use the ingress-basic namespace also for Nginx. If you want, use a different one for the cert-manager.Install a Let’s Encrypt Certificate IssuerAfter installing the cert-manager, install a certificate issuer to generate the tls certificates for your applications.Save the code in a file and then apply the file to your Kubernetes clusterThis example uses Let’s Encrypt as the issuer but you can use any CA issuer you want. Before you deploy the code, add your email so you can get emails about the certificates. At the beginning of the code, you can see that the kind of object is ClusterIssuer. A ClusterIssuer can create certificates for all applications, no matter in what namespace they are. The second option is Issuer which works only in a single namespace. An issuer might be useful if you want to use a different CA issuer.Update the Microservices to use the TLS CertificateThere is not much to update in the configuration of the microservice to use the TLS secret. All you have to do is add the TLS secret and the host before the rules section in the ingress.yaml file of each microservice. The ingress.yaml file is part of the Helm chart. If you don’t know Helm, see my posts Helm - Getting Started and Deploy to Kubernetes using Helm Charts.The ingress.yaml file looks as follows:The OrderApi ingress file looks the same, except that the name is orderapi instead of customerapi.Next, add the TLS secret name and the host to the values.yaml file.The variables, for example, __TlsSecretName__ are defined in the CI/CD pipeline and will be replaced by the Tokenizer. For more information about the Tokenizer, see Replace Helm Chart Variables in your CI/CD Pipeline with Tokenizer.You can use whatever name you want for the TLS secret.The last step is to add an additional annotation to the ingress of the microservice. Add the following line to the annotations section of the values.yaml file:This is all you have to configure to automatically use HTTPS and also use SSL termination in the Nginx ingress controller. This means that the traffic inside the cluster uses only HTTP and therefore doesn’t use any compute power to decrypt the connection.Using HTTPS to access the MicroserviceDeploy the Microservice and then call their URL, in my case, customer.programmingwithwolfgang.com. You should see that HTTPS is used and that a valid certificate is used. A valid SSL Certificate got created TroubleshootingIf something did go wrong, you might see a warning when you try to access the microservice using HTTPS. The SSL Certificate is not valid If you see this message, check if you added the letsencrypt annotation. I forget this one almost always. If this didn’t fix the problem check if there is a certificate in your namespace with the following command:This should display your certificate. The SSL Certificate got added to the Namespace ConclusionA cert-manager creates SSL certificates automatically in your Kubernetes cluster and helps you to reduce the time to fully configure your application. This is especially useful when you use multiple test environments.In my next post, I will show you how to separate the CI/CD pipeline into two pipelines which will enable you to make changes faster and with fewer errors.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "How to pass the AZ-303 and AZ-304 Certification Exams", "url": "/how-to-pass-az-303-and-az-304-certification-exams/", "categories": "Cloud, Miscellaneous", "tags": "DevOps, Azure DevOps, Azure, Azure Functions, YAML, Azure App Services, Cosmos DB, Azure CLI, Azure SQL, Azure VM, Learning, Certification, Exam", "date": "2021-05-24 00:00:00 +0200", "snippet": "Two weeks ago, I passed the AZ-304 exam and today I passed the AZ-303 exam giving me the Azure Solutions Architect Expert certification.This post will give you an overview of the exams what methods I used to study and my tips to pass the certification.Exam PreparationThe first step of your exam journey should be figuring out what topics are covered in the exam. Download the skills measured for the AZ-303: Microsoft Azure Architect Technologies or for the AZ-304: Microsoft Azure Architect Design exam.These documents give you an overview of the needed skills and outline any changes or updates to the exam. Do not be fooled by the simple document though. During the exam, you are expected to have in-depth knowledge of the needed technologies. You can find more details about the questions in the Difficulty of the Exam section further down.PluralsightIf you are fairly new to Azure or some of the topics covered in the exam, get a Pluralsight subscription and go through the videos in the Microsoft Azure Architect Technologies (AZ-303) and the Microsoft Azure Architect Design (AZ-304) paths.I started with the AZ-304 exam and therefore also started the AZ-304 path on Pluralsight. All courses in this learning path are remarkable and give a great overview of all the topics covered in the exam and furthermore give some additional more in-depth knowledge about the technologies.After finishing the AZ-304 path, I started to watch the videos of the AZ-303 path but I did not learn anything new from them. It is possible that I was slightly burned out after all the videos or that the topics were too similar. If you study for the exam, I suggest giving them a shot and so that you may see for yourself if they help or not.Microsoft Learning PathsAfter getting a great overview of all technologies, I wanted to go more into the details of every technology and used the learning paths from Microsoft. These learning paths cover all technologies in the exam and are a combination of theory and practical tasks, whereby you get a sandbox into Azure and must apply what you learned before. This could be, for instance, creating two new VNets with VMs in them. Then peer these VNets and check the connection between the VMs. The great thing about this is that you do not need to pay anything and do not even have to provide your credit card information. You can find the learning paths of the AZ-303 and AZ-304 exams on their certification page.WhizlabsWith the Pluralsight courses finished and reading through hundreds of pages of documentation, I felt ready to take some practice questions. I decided to use Whizlabs, mostly because of their price of around $20 USD for four practice tests which you can retake as often as you want and also write comments to the questions.I took the first practice test and felt not too bad but was surprised to see that I had only achieved around 60%. I went through each question and also read the comments of other test-takers. This led me to realize that around half of the questions are wrong and often my answers were actually correct. Many comments pointed that out and the support of Whizlabs acknowledged the issues and promised to fix them. But months later, the questions are still wrong.Despite how cheap Whizlabs is, I can not recommend using it due to all the wrong answers and confusion that they are causing.MeasureUpMeasureUp is the test platform officially recommended by Microsoft. The AZ-304 exam has 162 test questions and the AZ-303 one has 166 questions. The questions are in the same format as the actual certification, meaning that there are case studies, single-choice, multiple-choice and drag-and-drop questions.The practice tests are expensive ranging from 91€ for the 30-day access to 110€ for the one-year or download version. If you pass the test on MeasureUp, they give you a passing guarantee and if you fail your exam twice, you are refunded the full amount of the practice tests. They also often have deals where you can get 20% or 30% off.The website can be awful, for example, Chrome didn’t work most of the time for me and there are a couple of buttons that are only clickable if you hit the right pixel. However, once you started the practice test, everything is great. You can even configure the test exam to either have random questions, or questions that you got wrong the last time, or questions that you have not seen in a while.The first time I took the AZ-304 test exam I achieved 74% and for the AZ-303, I only received 64%. But it showed me what was expected from me during the exam and helped me to dig even deeper into the technologies covered by the exams.MeasureUp is expensive but totally worth the price and I highly recommend getting it before taking the exam.How to studyEveryone studies differently. Some prefer videos, some prefer reading and others like hands-on experience.My recommendation is to try everything and see what works best for you. For me, watching the Pluralsight videos was a great starting point. Afterward, I copied the whole learning paths of the AZ-303 and AZ-304 exams into a Word file. I printed the document and highlighted it by hand. When I found something I didn’t know, I added it at the end of the document under notes. I already studied this way during my university time and it works great for me. You can find the document in different formats on GitHub.You should also get enough hands-on experience. I had this experience before I studied the documentation and it helped me greatly in understanding the topic. I would recommend that you create most of the services which are covered in the exam and play around a bit and look at all the panes of the service in the Azure portal.The last recommendation I can give you is to purchase the MeasureUp practice exams. They are worth it but also give you an insight into the way the questions will be and may furthermore boost your confidence when you can achieve a high score in the practice tests.AZ-303 vs AZ-304The topics covered in the two exams are very similar but you have to know different aspects of them. The AZ-303 exam is more about how to set up something and its technical limitations. For example, you have to be able to set up a VNet, add VMs and then configure VNet peering using the Azure CLI and Azure PowerShell modules. Additionally, you must know what VM Skus you should use for specific workloads and what database sizes you can use with the different Azure SQL offerings.The AZ-304 focuses more on what service you should use to build a solution. For example, you have to know if you should use Azure Front Door or Azure Traffic Manager to implement the requirements. Usually, you don’t have to write code here but be prepared to know at least basic commands.I started with the AZ-304 because it fits better with my professional experience and I was more interested in it. I never had any doubt of failing the exam and got a score of 880 out of 1000. Then I started preparing for the AZ-304 which was way harder for me, mostly because of all the Azure PowerShell which I have never used before. Usually, I use Azure CLI but the exam forces you to be proficient in all technologies. Before the exam I was a bit doubtful of passing it but surprisingly to me, I browsed through the exam and got a score of 930 out of 1000.Taking the ExamThere are two options for taking the exam: at an official test center or at home. In the past, I took an exam at the test center but for the AZ-303 and AZ-304, I decided to take the online exam. Both options offer exactly the same exams, so you can select whatever you are more confident with.The exam itself takes 130 minutes but plan for three hours with the ID check and some survey questions from Microsoft. This is plenty of time to go through your answers a couple of times. I finished both exams in under an hour.The AZ-303 exam had 48 questions for me and the AZ-304 had 62. You should expect the number of questions to be in that range. To pass you need a score of at least 700 out of 1000. Note that Microsoft uses a dynamic scoring system which means that 700 out of 1000 doesn’t equal 70%.After finishing both exams, you get the Azure Solutions Architect Expert certification and badge. Azure Solutions Architect Expert Badge Taking the Exam at a Test centerI took a test at a test center a couple of years ago and it wasn’t the most pleasant experience. First, you have to travel to their location, make sure you are one time, and bring an official ID. Next, you have to place all your belongings like your wallet, phone, and watch into a locker and then get escorted into a room full of computers where you take your exam. My test center had a mouse from the 90s and it was quite a hassle working with it. I didn’t feel comfortable there but there is nothing wrong with taking the exam at a test center.Taking the Exam OnlineDue to the pandemic, there were no test centers open in my location and therefore I took both exams online. To take the exams from home, you have to have a stable internet connection, a webcam, and a microphone. You are not allowed to use headphones though.Taking the exam from home was way more comfortable for me. First, you don’t have to travel there and additionally, you can use your computer, mouse and keyboard, and your chair which you are already familiar with. The check-in starts 30 minutes before the exam is scheduled. You get a link on your phone and have to take a picture of your ID and yourself. Then take four pictures of your surroundings and you get placed into the waiting queue. After a couple of minutes, a proctor will contact you either via voice message or chat and they may ask you some questions, such as, to show them your desk. Then they start the exam for you and the official exam begins.After you are done with the exam, you will immediately get your score and how you did in each of the areas of the exam.Difficulty of the ExamWhen I started with the practice exams, I was surprised by how hard the questions were. It is important to focus on the question and read it word for word. Sometimes a single word can change the answer to the question. I had two questions in my exam where I had no clue about the answer. In this situation, take a breath and try to rule out false answers. Often you have one or two answers which are completely wrong and so you are left with only two possible answers. If you still have no clue, take a guess. You don’t lose any points for a wrong answer. Never leave a question unanswered!During the practice exams, I felt way more at ease with the AZ-304 compared to the AZ-303 ones. Funnily enough, the AZ-303 exam felt easier than the AZ-304 one. I guess that I got a bit lucky with my questions there.ConclusionThe AZ-303: Microsoft Azure Architect Technologies and AZ-304: Microsoft Azure Architect Design certification exams require a lot of studying and hands-on experience but if you prepared diligently, you will pass the exams to earn your certification and learn a lot about the different technologies in Azure." }, { "title": "Configure custom URLs to access Microservices running in Kubernetes", "url": "/configure-custom-urls-to-access-microservices-running-in-kubernetes/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure DevOps, Azure, Nginx, YAML, CI-CD, Docker, Helm, AKS, Kubernetes", "date": "2021-05-17 00:00:00 +0200", "snippet": "In my last post, I created an Nginx Ingress controller and added rules to route to my two microservices. This solution worked but it was far from optimal. First, I had to add routing rules for my microservices, and even worse using an IP address is very ugly in intuitive for the users.Today, I will add URLs to my microservices to make them easily accessible for my users.This post is part of “Microservice Series - From Zero to Hero”.Add a DNS Entry for your DomainBefore you can use your own URL, you have to create an Azure DNS zone. In the Azure portal, search for DNS zone and click create. Provide a subscription, resource group, and the name of your domain. Add a new DNS zone After the DNS zone is created, click on + Record set and add a new A record pointing to the URL of your Nginx controller. If you don’t have a public IP yet, see Set up Nginx as Ingress Controller in Kubernetes to configure one. Since I have two microservices, I added two subdomains, customer.programmingwithwolfgang and order.programmingwithwolfgang. Add A records for your subdomains pointing to your Kubernetes cluster Note: I had a different public IP address in my post Set up Nginx as Ingress Controller in Kubernetes. If you are following the series, make sure to add the IP address of your Nginx controller.If you are hosting your domain outside of Azure, add the Azure DNS zone namespaces to your domain provider. In my provider, I have the following GUI to configure the nameservers: Add the Azure DNS server It might take a bit until the DNS servers are updated. You can check the DNS entries with nslookup:Once the DNS entries are updated, open your URL in your browser and you should see the Nginx 404 page.Route Traffic according to the URLYou can find the code of the demo on GitHub.The DNS settings are configured and now you want to tell Nginx to route customer.programmingwithwolfgang to the customer microservices and order.programmingwithwolfgang to the order microservice. This can be done easily with a change in the ingress configuration. The ingress controller is defined using Helm. If you don’t know Helm, see Helm - Getting Started and Deploy to Kubernetes using Helm Charts.All you have to do is to add the - host: parameter above the http section and provide your URL:After adding the host line in the ingress.yaml file, add your URL to the values.yaml file. Additionally, change the path from /customerapi-test/?(.*) to /. Since the microservices are using different URLs, you don’t need different paths anymore. The values.yaml file should now look as follows:The URL is defined as a variable in the CI/CD pipeline and will be added by the Tokenizer. For more information about the Tokenizer, see Replace Helm Chart Variables in your CI/CD Pipeline with Tokenizer.In my last post, I had a small workaround in the Startup.cs class to get Swagger working. This workaround is not needed anymore due to the usage of the URL and therefore can be removed:That’s it. Deploy both microservices and let’s test them.Testing Microservices using custom URLsAfter the deployment is finished, navigate to customer.programmingwithwolfgang (obviously you have to use your own URL) and you should see the Swagger UI. The UI wasn’t working when using the IP instead of the URL but this problem is now fixed. The Swagger UI of the Customer Microservice with the custom URL The order microservice is working as well with its custom URL. The Swagger UI of the Order Microservice with the custom URL ConclusionUsing custom URLs or subdomains is surprisingly easy with Nginx and Kubernetes. Setting up the URLs is fast and gives your users or testers a better experience than the IP address I used before. Additionally, the Swagger UI problems were also fixed. Using a custom URL is a better solution than using an IP address but it is still not optimal.In my next post, I will automatically create certificates enabling the use of HTTPS and also will implement SSL termination in the Nginx ingress controller.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Set up Nginx as Ingress Controller in Kubernetes", "url": "/setup-nginx-ingress-controller-kubernetes/", "categories": "Kubernetes, Cloud", "tags": "DevOps, Azure DevOps, Azure, Nginx, YAML, CI-CD, Docker, AKS, Kubernetes", "date": "2021-05-10 00:00:00 +0200", "snippet": "So far I have used a Service with the type LoadBalancer for my microservices. The service can be set up quickly and also gets a public IP address assign from Azure to access the microservice. Besides the load balancing, it has no features though. Therefore, I will replace the Service object with an Nginx ingress controller.Today, I will implement the ingress controller and set up some basic rules to route the traffic to my microservices. In the next post, I will add SSL termination and the automatic creation of SSL certificates.This post is part of “Microservice Series - From Zero to Hero”.Install Nginx as Ingress ControllerThe idea is to have an Nginx controller as the only entry point to my Kubernetes cluster. This controller will have a public IP address and route the traffic to the right microservices. Additionally, I will change the existing Service of the microservices to the type ClusterIP which removes the public IP address and makes them accessible only through the Nginx Ingress controller.To install Nginx, I will use a Helm chart. If you don’t know Helm, see my posts Helm - Getting Started and Deploy to Kubernetes using Helm Charts.First, create a new namespace that will contain the Nginx controller. This will help with future maintenance.Next, add the Nginx Ingress Helm chart to your AKS cluster and then install it with the following code:For this demo, it is enough to have two replicas but for your production environment, you might want to use three or more. You can configure it with the –set controller.replicaCount=2 flag. The installation of the Nginx ingress might take a couple of minutes. Especially the assignment of a public IP might take a bit. You can check the status with the following code:This command also gives you the public (External) IP address of the controller: Nginx got installed with a public IP Open the public IP and your browser and you should get the Nginx 404 page. The default Nginx 404 page Configure the Microservices to use a ClusterIP ServiceYou can find the code of the demo on GitHub.I have two existing microservices in my demo repo which both use a Service of the type LoadBalancer. Before you can use the Nginx Ingress controller, you have to change the Service type to ClusterIP. Since the microservices use Helm, you can easily override values using the values.yaml file and therefore, I added the following code there:That is all that is needed for the Service. Next, you have to create an Ingress object for each microservice to tell Nginx where the traffic should be routed. You can find the Ingress object definition inside the Helm chart:This might look complicated at first glance but it is a simple Nginx rule. It starts with an if condition which lets you control whether the Ingress object should be created. Then it sets some metadata like name and namespace and inside the rules, it defines the path and the port of the application. If you use Kubernetes 1.14 to 1.18, you have to use apiVersion: networking.k8s.io/v1beta1. I am using Kubernetes 1.22.6 and therefore can use the GA version of the API.Lastly, add the values for the Ingress object to the values.yaml file:The K8sNamespace variable is defined in the Azure DevOps pipeline and will be replaced when the Tokenizer task is executed. You can read more about it in Replace Helm Chart Variables in your CI/CD Pipeline with Tokenizer.Deploy the application, open the URL of the Nginx controller and you will see the Swagger UI of the microservice: The Swagger UI of the microservice This works fine but when you try to deploy the second microservice, you will get the following error message: Error from server (BadRequest): error when creating “orderapi-ingress.yaml”: admission webhook “validate.nginx.ingress.kubernetes.io” denied the request: host “_” and path “/” is already defined in ingress customerapi-test/customerapi-test. If you have two rules with / as the path, Nginx won’t know which one to take and therefore the deployment fails.Add individual Paths to the MicroservicesThe solution to this problem is to add different paths for each microservice. For example, for the CustomerApi, I use /customerapi-test/?(.) and for the OrderApi /orderapi-test/?(.). Additionally, you have to configure this path in the Startup class of each microservice with the following code inside the Configure method:The values.yaml file will contain the following code at the end:Testing the Microservice with the Nginx Ingress ControllerAfter you deployed both microservices, check if you can access the swagger JSON with the following URL: /customerapi-test/swagger/v1/swagger.json. This should display the JSON file: Accessing the Swagger json file Next, try to call one of the methods, for example, the PrimeNumber method: Calling the PrimeNumber method When you try to access the Swagger UI, you will get an error message though: Error message of Swagger UI It is inconvenient that this is not working but I will leave it as it is for now. In my next post, I will replace the IP address with a DNS entry which should fix the problem. Lastly, check if the second microservice is also working with the following URL: /orderapi-test/swagger/v1/swagger.json The OrderApi works too ConclusionNginx can be used as an Ingress controller for your Kubernetes cluster. The setup can be done within minutes using the Helm chart and allows you to have a single entry point into your cluster. This demo used two microservices and provides basic routing to access them.In my next post, I will map a DNS name to the IP and access the microservices using different DNS names.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Deploy a Docker Container to Azure Functions using an Azure DevOps YAML Pipeline", "url": "/deploy-docker-container-azure-functions/", "categories": "DevOps, Cloud", "tags": "DevOps, Azure DevOps, Azure, Azure Functions, YAML, CI-CD, Docker", "date": "2021-05-03 00:00:00 +0200", "snippet": "In my last post, I created a YAML pipeline to build and deploy an Azure Function to Azure. Today, I will build the same Azure Function inside a Docker container and deploy the container to Azure.This post is part of “Microservice Series - From Zero to Hero”.Add Docker to the Azure FunctionYou can find the code of the demo on GitHub.To add Docker to the Azure Function, right-click the project in Visual Studio and click Add –&gt; Docker Support. Visual Studio automatically creates the Dockerfile for you.If you want to learn more about adding Docker to a .NET 5 project, see my post Dockerize an ASP .NET Core Microservice and RabbitMQ.Build a Docker Container inside a YAML Pipeline in Azure DevOpsIn Azure DevOps, create a new pipeline (or edit the one from my last post) and add the following variables:Additionally, add DbUser, DbPassword, and QueueConnectionString as secret variables to the pipeline. These variables contain the database user and password and the connection string to the Azure Service Bus Queue. If you want to deploy your own Azure Function without a connection to other services, then you obviously don’t have to add the variables.Next, add a job inside a stage and create a build version using GitVersion. If you want to learn more about versioning, see my post Automatically Version Docker Containers in Azure DevOps CI.With the version number in place, add two more tasks to build the Docker container and then push it to a registry. In this demo, I am pushing it to Dockerhub.The last step is to deploy the previously created Docker container to the Azure Function and then pass the database and queue connection string to its settings. For more information about deploying an Azure Function with Azure DevOps see my last post, Deploy Azure Functions with Azure DevOps YAML Pipelines. Note that the Azure Function must exist, otherwise the deployment will fail.The full pipeline looks as follows:Save the pipeline and run it. The pipeline ran successfully You can see the Docker container with its two tags on DockerHub after the build is finished. The Azure Function Container got pushed to DockerHub Testing the Azure Function is exactly the same as in my last post, Deploy Azure Functions with Azure DevOps YAML Pipelines.ConclusionThis short post showed how to create a Docker container of an Azure Function inside an Azure DevOps YAML pipeline. The Docker image was published to DockerHub and then deployed to an existing Azure Function.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Deploy Azure Functions with Azure DevOps YAML Pipelines", "url": "/deploy-azure-functions-azure-devops-pipelines/", "categories": "DevOps, Cloud", "tags": "DevOps, Azure DevOps, Azure, Azure Service Bus, Azure Functions, YAML, CI-CD", "date": "2021-04-26 00:00:00 +0200", "snippet": "In my last post, I created an Azure Function that reads messages from an Azure Service Bus Queue and then updates a database. The deployment of the Azure Function was manual using Visual Studio.Since I am a big fan of DevOps, I will create an Azure DevOps pipeline today to automate the deployment.This post is part of “Microservice Series - From Zero to Hero”.Create a basic YAML pipeline in Azure DevOpsYou can find the code of the demo on GitHub.Create a basic YAML pipeline in Azure DevOps where you restore the NuGet packages, build the solution, and publish it. For more information about Azure DevOps YAML pipelines, see my post Build .NET Core in a CI Pipeline in Azure DevOps.Your basic pipeline should look something like this:The pipeline above runs automatically when changes are detected on the master branch inside the AzureFunctions/OrderApi.Messaging.Receive/ folder and uses the newest Ubuntu VM to run the build. Next, I configure a couple of variables and then run the restore, build and publish of the solution.Deploy the Azure FunctionBefore you can deploy to Azure, you need a service connection. If you don’t have one yet, see Deploy to Azure Kubernetes Service using Azure DevOps YAML Pipelines to set up one.With the service connection in place, add the following task to the pipeline:This task takes the previously created .zip file (during the publish) and deploys it to an existing Azure Function. All you have to do is replace the appName with your Azure Function name. Note that the Azure Function has to exist, otherwise the task will fail. If your Azure Function runs on Windows, use functionApp as the appType.Update the Azure Function SettingsThe Azure Function can be deployed but it won’t work because it has no access to the Azure Service Bus Queue or database. To provide access, you have to update the service settings. First, add secrets to your Azure Pipeline by using the Variables button on the right side. Add secret variables to the pipeline Next, add the following variables to your pipeline:Lastly, add the Azure App Service Settings task to update the service settings:For more information about the Azure Service Bus Queue, see my post Replace RabbitMQ with Azure Service Bus Queues. The pipeline is finished. Deploy the Azure Function and test it.Testing the deployed Azure FunctionIf you followed this series (“Microservice Series - From Zero to Hero”), you know that I am using a Customer microservice that writes updates into a queue and the Azure Function takes these messages to update the database of the Order microservice.To test that the Azure Function works, first check the orders, especially the names in the Order database. The orders before the update Next, update a customer name, for example, from Wolfgang Ofner to Name Changed. The Customer microservice writes this update in the queue (or you add the message manually into the queue). The Azure Function sees the new messages, reads them, and then updates the orders in the database. After the Azure Function ran, check the orders in the database and you will see the changed name. The name was updated ConclusionDeploying Azure Functions with Azure DevOps only takes a couple of lines of code and should never be done manually. Additionally, it is very easy to update the existing settings without exposing any sensitive data.In my next post, Deploy a Docker Container to Azure Functions using an Azure DevOps YAML Pipeline, I will show you how to deploy an Azure Function inside a Docker container.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Use Azure Functions to Process Queue Messages", "url": "/azure-functions-process-queue-messages/", "categories": "Cloud", "tags": "Azure, Azure Service Bus, Azure Functions", "date": "2021-04-19 00:00:00 +0200", "snippet": "Azure Functions is a serverless compute offering and lets developers execute code without thinking about the infrastructure it is running on. This helps to efficiently develop event-driven solutions.This post will show how to use Azure Functions to read messages from an Azure Service Bus Queue and how to write data to an Azure SQL Database.This post is part of “Microservice Series - From Zero to Hero”.What are Azure Functions?Azure Functions is a serverless compute platform that allows for quick development without the need to manage the underlying infrastructure. Serverless means that Microsoft abstracts the infrastructure away and fully manages its operation and scaling.An Azure Function is usually one method that can be triggered by an event like an HTTP call or a message being placed in a queue. Microsoft offers a wide variety of triggers and programming languages like C#, Java, or Phyton for your function. You can use an App Service Plan which you pay monthly or you can use a Consumption plan and only pay when the function is running.Azure Functions are great when you want to execute for background services like processing items on queues but it is not that great when users are waiting for the result because it might take some time to start the function.Explaining the Demo ApplicationYou can find the code of the demo on GitHub.The demo application contains two microservices called OrderApi and CustomerApi. Every time a customer is updated, the CustomerApi places a message with the new information on either a RabbitMQ or Azure Service Bus Queue, depending on its Startup configuration. The OrderApi has a background service that checks the RabbitMQ queue for new messages and then takes these messages to update the customer names in its database.The problem with this solution is that every container running in Kubernetes has this background service running. The background service always scales with the OrderAPI, even though it might not be necessary and therefore wastes resources and as a result, increases the costs. Additionally, there is no insight into the background service and it would be nice to have some information about the queue processing logic, like how many messages were processed or how many errors occurred.I want to create an Azure Function to process the queue messages independently from the containers running in Kubernetes and then will deactivate the background service entirely (I will leave it in the code in case you want to use it). The Azure Function will read the queue message, load the orders of this customer, update the customer name and then save the orders in the database.Create your first Azure FunctionYou can find the code of the demo on GitHub.To create a new Azure Function, start Visual Studio, search for Azure Functions and click Next. Create a new Azure Function Enter a name and then select a trigger. As you can see, the template offers a wide variety of triggers such as HTTP, RabbitMQ, or SignalR. Select Service Bus Queue trigger and on the right side enter a name for the connection string and the queue name. Configure the Function Trigger Click on Create and your first Azure Function gets created.In the Azure Portal, go to your Service Bus Namespace and select your CustomerQueue. There select the Shared access policies pane and create a new policy with Listen. After you saved the policy, click on it and you can see the Primary Connection String. If you want to learn how to create an Azure Service Bus Queue and more details about Shared access policies, see my post Replace RabbitMQ with Azure Service Bus Queues. Copy the Connection String to your Azure Service Bus Queue Copy the Primary Connection String and add the following line inside the Values to the local.settings.json file of your Azure Function:Note that some versions of the Azure Service Bus SDK have a problem with the connection string. If you run into a problem, remove the Entity Path at the end of the connection string. In my case the is “EntityPath=customerqueue”. Set a breakpoint in the function, start the solution, and then add something to your queue. As soon as you added a message to the queue, you will hit the breakpoint because the Azure Function was triggered. This confirms that your connection string and trigger are working.Add Entity Framework Core to an Azure FunctionInstall Entity Framework Core 3.1.14 via NuGet or add the following line to your project file:Note that Entity Framework Core 5 currently has a dependency that is incompatible with Azure Functions. Azure Functions should be updated to .NET 5 soon.Next, create a new file and add the database context:Then create the Order and UpdateCustomerFullNameModel which you will need to load and update the orders.Lastly, add the following line inside the Values section of the local.settings.json file to configure the connection string:Replace &lt;YourDatabaseConnectionString&gt; with your connection string and you are good to go. Now let’s set up dependency injection and inject the database context into the function.Add Dependency Injection to an Azure FunctionTo add dependency injection to an Azure Function, install the Microsoft.Azure.Functions.Extensions NuGet package. Then create a new class, called Startup with the following code:This code reads the connection string from the settings file and adds it to the database context. Now you can use constructor injection to inject the database context into your function. The whole code for the function looks as follows:The function deserializes the message into an UpdateCustomerFullNameModel object, then searches for all orders with the customer id from the deserialized object and updates all names. If something goes wrong, messageReceiver.DeadLetterAsync(locktoken) places the message in a dead letter queue. To use DeadLetterAsync() you have to install the Microsoft.Azure.WebJobs.Extensions.ServiceBus NuGet package.Deploy the Code to Azure FunctionsTo publish your code, right-click the project and select Publish. This opens a new window where you select Azure and click Next. Publish the Azure Function to Azure On the next window, select the environment you want to use. Note that Windows has more options for editing the function in the Azure Portal but for this demo it doesn’t matter if you use Windows or Linux. Deploy the Function to Linux Next, provide a name, subscription, resource group, plan, and location for the new Azure Function. Provide basic settings After providing the settings, click on Finish and the publish profile will be created. Before you publish the function, you have to provide the connection string to the database and Service Bus Queue. On the right side, click on the three dots and then select Manage Azure App Service settings. This opens a new window with all Application settings. You can either copy the local values for the connection strings or paste the desired values into the text boxes. Configure the remote Connection Strings Note to keep your connection strings secret and never check them into source control.After inserting the connection strings, publish the Azure Function.Testing the Azure FunctionOpen the CustomerAPI solution, add your database and queue connection strings in the appsettings file and start the application. In the Swagger UI update an existing user with the PUT operation. Update an existing Customer Updating the customer will also create a message on the Azure Service Bus Queue. For more information about the Azure Service Bus Queue, see my post Replace RabbitMQ with Azure Service Bus Queues. The Customer was added to the Service Bus Queue Note you might not see the message if your Azure Function is already running because the message might be already processed. Open your Azure Function in the Azure Portal and on the Overview page, you can see that the function was executed once. The Azure Function was executed once To make sure everything worked you can either check the Service Bus Queue to see that there is no message left or in the DeadLetter Queue or you can check the changed value in the database. No message in the DeadLetter Queue The Customer Name got updated ConclusionAzure Functions is a great tool to implement event-driven solutions without worrying about the underlying infrastructure. You only pay when your function runs if you select the Consumption plan and can connect a wide variety of different services like RabbitMQ, Azure Event Hub, or Azure Service Bus Queues. This post showed how to deploy an Azure Function from Visual Studio.In my next post, I will show you how to create a CI/CD pipeline in Azure DevOps to automate the deployment.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Replace RabbitMQ with Azure Service Bus Queues", "url": "/replace-rabbitmq-azure-service-bus-queue/", "categories": "Cloud, DevOps", "tags": "DevOps, CI-CD, Azure DevOps, Azure, Kubernetes, AKS, Helm, Azure Service Bus, RabbitMQ", "date": "2021-04-12 00:00:00 +0200", "snippet": "RabbitMQ is a great tool to connect microservices asynchronously but it also comes with a couple of downsides. The biggest downside is that you have to take care of running, monitoring, and updating it. Running a RabbitMQ Docker image in Kubernetes is quite easy but still requires some management. One of the best features of cloud provides are the platform as a service (PaaS) offerings. This means that the cloud provider, for example, Azure runs the platform for you and you don’t have to take care of updating or patching the software.In this post, I will add a new class to my microservice so I can switch between RabbitMQ and Azure Service Bus.This post is part of “Microservice Series - From Zero to Hero”.What is Azure Service BusAzure Service Bus is an enterprise messaging PaaS solution with many useful features. Some of these features are: Queues Topics (one sender, multiple subscribers) Transaction Duplication filter Dead message queues Geo-disaster recoverySince it is a PaaS offering, Azure is managing the infrastructure which means that you as a developer can focus on implementing it and don’t have to think about maintaining, upgrading, or monitoring it. For this demo, I am using the Basic tier which has very limited features but only costs 0.043€ per million operations. In your production environment, you most likely will use the Standard tier.Difference between Azure Service Bus Queue and Azure QueueBoth queue solutions are very similar but Azure Service Bus comes with a First-In-First-Out guarantee. Additionally, it has more enterprise features in the Standard and Premium tier. For a more detailed comparison between Azure Service Bus and Azure Queue, see the documentation.Create a new Azure Service Bus QueueFirst, you have to create an Azure Service Bus Namespace. In the Azure Portal, search for Service Bus and click on Create. Create a new Service Bus Namespace Select your subscription, resource group, location, and pricing tier, and then provide a unique name. Click on Create and the Service Bus Namespace gets created. Configure the new Azure Service Bus Namespace After the deployment, click on Queues and then add a new queue. Create a new Queue Give the queue a name and leave all settings as they are. Configure the new Queue Configure the Access to the Queue using Shared Access Policies (SAS)After the queue is created, click on it and then select the Shared access policies tab. There click on Add, select the Send checkbox and click on Save. Create a SAS for the Queue We will use this SAS to allow the microservice to send messages to the queue. You can also give the manage permission if you want to allow your microservice to create queues but I prefer as few permissions as possible. Additionally, I will show in a future post how to create the infrastructure using an Azure DevOps CI/CD pipeline.Send Messages to the Azure Service Bus Queue from a MicroserviceYou can find the code of the demo on GitHub.The Azure Service Bus Queue is created and configured and now we can configure the microservice to send messages to the new queue. First, we add the following settings to the appsettings.json file:These settings contain the Azure Service Bus queue name, the connection string, and also a switch to use RabbitMQ or Azure Service Bus Queue. I used the in-memory switch already in a previous post, Use a Database with a Microservice running in Kubernetes. The queue switch will work the same way and either register the RabbitMQ service or the Azure Service Bus Queue service. In the appsettings.Development.json file, add the UserabbitMq attribute and set it to true.Next, create a new class that will contain the Azure Service Bus options:In the ConfigureServices method of the Startup.cs class, read the AzureServiceBus section into the previously created AzureServiceBusConfiguration class:Additionally, read the UserabbitMq variable and either register the already existing RabbitMQ service or the Azure Service Bus service:That is all the configuration needed. Lastly, go to the CustomerApi.Messaging.Send project and install the Azure.Messaging.ServiceBus NuGet package and then create a new class called CustomerUpdateSenderServiceBus. This class inherits from the ICustomerUpdateSender interface and takes a customer and sends it to the Azure Service Bus Queue. The full class looks as follows:This code is very simple but will throw an exception if the queue does not exist.Test the Azure Service Bus integration in the MicroserviceStart the microservice and update a Customer using the Swagger UI. Update an existing customer After updating the customer, open the Service Bus Queue in the Azure Portal and select the Service Bus Explorer tab. There, you should see one message in the queue. One message got sent to the Queue Select the Peek option (look at the first message without deleting it) and you should see your previously edited customer. The updated customer got sent to the Queue Deploy the Microservice to Kubernetes and use the Azure Service BusTo use the Azure Service Bus Queue in Kubernetes, you have to tell your microservice the connection string. This works the same way as providing a database connection string which I described in Use a Database with a Microservice running in Kubernetes.First, click on Variables and add a new variable inside your Azure DevOps pipeline. Name the variable AzureServiceBusConnectionString and past the connection string from the previously created SAS as the value. Add the Service Bus Queue Connection String After adding the variable, add the following code to the values.yaml file.This code creates a secret in Kubernetes with the value of the previously created variable. This secret will overwrite the appsettings of the microservice and therefore allow it to access the Azure Service Bus Queue and no sensitive information was exposed in the pipeline.This code is using Helm to create the secret and Tokenizer to replace __AzureServiceBusConnectionString__ with the value of the variable AzureServiceBusConnectionString. For more information on these topics see Deploy to Kubernetes using Helm Charts, Override Appsettings in Kubernetes and Replace Helm Chart Variables in your CI/CD Pipeline with TokenizerConclusionCloud providers, especially Azure, offer a wide range of services. This allows developers to use different PaaS solutions which help them to focus on their implementation, rather than running and maintaining the infrastructure. Azure Service Bus is an enterprise queueing solution and can replace RabbitMQ if you want to take advantage of the PaaS offering instead of running RabbitMQ yourself. I added a switch to the microservice, so you can easily switch between RabbitMQ and Azure Service Bus Queue.In Automatically set Azure Service Bus Queue Connection Strings during the Deployment, I have replaced the variable with some automation to automatically read the connection string. This allows for a more flexible and robust deployment process.In my next post, I will show you how to replace the background service in the OrderApi with Azure Functions to use a serverless solution to process the messages on the queue.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Use Azure Container Registry in Kubernetes", "url": "/azure-container-registry-kubernetes/", "categories": "Kubernetes, DevOps", "tags": "DevOps, CI-CD, Azure DevOps, Azure, Kubernetes, AKS, Helm, ACR, Azure Container Registry", "date": "2021-04-05 00:00:00 +0200", "snippet": "Dockerhub is like GitHub for Docker containers. You can sign up for free and get unlimited public repos and one private repo. This is great for developers like me who want to make their containers easily accessible for everyone. Enterprises probably don’t want to have their containers on a public share. They can either buy an enterprise plan or they can use a private registry like Azure Container Registry (ACR).In this post, I will talk about the advantages of Azure Container Registry and show you how to configure your microservice and Kubernetes cluster to run images from ACR and run them in your cluster.This post is part of “Microservice Series - From Zero to Hero”.What is Azure Container Registry (ACR)Azure Container Registry is a private container registry that allows you to build and store your images, replicate them around the globe and also scan for vulnerabilities. ACR comes in three pricing tiers: Basic, Standard, and Premium. The main difference between them is the amount of included storage and the number of webhooks you can use. Additionally, the premium tier supports geo-replication. This means that your images are still available when a data center goes down. Furthermore, it allows for faster startup times because the needed image is closer to the destination. This is only the case if you operate globally.Microsoft recommends using the premium tier. At first glance, you might think that the price difference is massive (0.141€ for Basic, 0.563€ for Standard, 1.406€ for Premium per day) but in the end, you pay 4.37€ for the Basic tier compared to 43.59€ for the Premium tier.Create an Azure Container Registry in the Azure PortalIn the Azure Portal, search for Container registries and then click on Create container registry. Create a new Azure Container Registry Select a subscription, resource group, location, and SKU. For the demo, I am using Basic. Provide a unique name for your ACR and then click on Create. Configure the new ACR Upload an Image to the Azure Container RegistryTo upload an image to the new ACR, you have two options: importing the image from Dockerhub or uploading it from a CI/CD pipeline (or manually).Import an Image from Dockerhub into ACRImporting an image is a good way to get started fast but I would recommend using the CI/CD pipeline approach in the next section. To import the image, use the following Azure CLI command:The first line logs you into your Azure subscription and the second one takes the name of your ACR, the source image from Dockerhub, and the image name which will be created in ACR.Upload a Docker Image using Azure DevOps PipelinesYou can find the code of the demo on GitHub.The better approach to uploading is an automated Azure DevOps pipeline. In my previous demos, I already created tasks to build and publish the image to Dockerhub. I will update these tasks to a new version (v2 instead of v1) and add more parameters, so you can easily switch between ACR and Dockerhub.Create a Service Connection to your ACRBefore you can upload images to ACR, you have to create a Service Connection. In your Azure DevOps project, click on Project settings and then on Service connections. Create a new Service Connection On the Service connections tab, click on New service connection and search for docker registry. Add a Docker registry connection On the next tab, select Azure Container Registry, your subscription, and your ACR. Provide a name and remember it because you will need it later in your pipeline. Configure the ACR Service Connection Update the CI/CD Pipeline to use the new Service ConnectionYou can find the Docker tasks in the DockerBuildAndPush.yml template. The original Docker build task looks as follow:I replaced the parameters and the Docker v1 task with the following code:As you can see, I added a couple of new parameters and updated the Docker task to v2. The v2 task takes a containerRegistry instead of dockerRegistryEndpoint and also a repository. Additionally, it supports multiple tags so you can build the same image as before and additionally always update the latest tag.The Docker publish looked originally as follows:I updated this task also to version 2 and added the new parameters.Lastly, I have to update my pipeline to pass values for the new parameters:Additionally, I renamed the ImageName variable to Repository. If you want to deploy to Dockerhub instead, use ‘Docker Hub’ for the containerRegistry and wolfgangofner/customerapi for the repository (you have to replace wolfgangofner with your Dockerhub repository name). For more information about pushing images to Dockerhub, see my post Build Docker in an Azure DevOps CI Pipeline.Update the Image Source in the Helm packageThe microservice uses Helm for the configuration. For more information about Helm, see my post Helm - Getting Started.The values.yaml file contains the configuration for the image and tag Kubernetes should use. Currently, this is:Since I renamed ImageName to Repository, you also have to rename it here:That’s it. The pipeline is configured to push the image to your new ACR and Kubernetes should then pull the new image from ACR and run. Let’s see if everything works.Test the Switch to ACRRun the pipeline and you should see that the build and push of the Docker image works. Go to your ACR and under Repositories, you will see the customerapi repository. But the pipeline will fail due to a timeout in the Helm upgrade release task. Helm upgrade timed out Connect to the K8s dashboard and check the pod to see the error message. For more information about accessing the dashboard, see my previous post Azure Kubernetes Service - Getting Started. ACR authentication required As you can see on the screenshot above, the pull failed because Kubernetes is not authorized to pull the image. This is because ACR is a private container registry and you have to give Kubernetes permission to pull images.Allow Kubernetes to pull ACR ImagesTo allow Kubernetes to pull images from ACR, you first have to create a service principal and give it the acrpull role. Use the following bash script to create the service principal and assign it the acrpull role.Don’t worry if the creation of the role assignment needs a couple of retries. Create a new service principal with the acrpull role Next, create an image pull secret with the following command:Note that you have to use the username and password from the previously created service principal. The namespace is the Kubernetes namespace in which your microservice is running. If you also want to deploy the OrderApi microservice from the demo repository, you have to repeat the command for the orderapi-test namespace.Use the Image Pull Secret in your MicroserviceAfter the image pull secret was created, you have to tell your microservice to use it. The image pull secret is part of the deployment but it is empty. In my last posts, I used the values.yaml file for values that I want to set during the deployment. Therefore, I add the name of the secret there:If you used a different name for your secret in the kubectl create secret docker-registry, then you have to use your name instead of acr-secret.Deploy the Image from ACR to KubernetesRun the pipeline again and this time it will succeed. The deployment using ACR succeeded Connect to the dashboard and check the events to make sure that the image was pulled from your ACR. The image was successfully pulled from ACR ConclusionAzure Container Registry (ACR) is a private container registry and a great alternative to Dockerhub, especially for companies. ACR allows you to build your images but also to distribute them globally. Due to the private nature of ACR, Kubernetes needs an image pull secret to allow deployments to access ACR and pull images from there.Note: By default, my demo code is using Dockerhub because it is more accessible and I want to make it easier for people who want to try the demo. I left the code for ACR as comments in the pipeline.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Readiness and Liveness Probes in Kubernetes", "url": "/readiness-health-probes-kubernetes/", "categories": "Kubernetes, DevOps", "tags": "DevOps, CI-CD, Azure DevOps, Azure, Kubernetes, AKS, Helm", "date": "2021-03-29 00:00:00 +0200", "snippet": "Kubernetes automatically checks if a pod is healthy and also when it is ready to accept traffic. These checks are done using a readiness probe and liveness probe respectively. This post shows how to configure these probes in a .NET 5 microservice and how to configure them in Kubernetes using Helm charts.This post is part of “Microservice Series - From Zero to Hero”.Liveness ProbeKubernetes regularly checks whether a pod is still alive or not. To check that, Kubernetes sends a request to the URL and port configured in the liveness section of the deployment. If the request returns an HTTP code greater or equal to 200 but less than 400, the pod is considered healthy. In every other case, the pod is considered dead and will be restarted. A liveness probe looks as follows:The code above tells Kubernetes to perform the liveness probe on the URL /health on port 80 (port 80 is HTTP). By default, K8s checks every 10 seconds but you can change this value using the periodSeconds parameter.Readiness ProbeThe readiness probe works the same way as the liveness probe except that it is only executed to determine whether a pod is ready to receive traffic after startup. A readiness probe looks as follows:This is a very simple probe and also checks the /health endpoint. Your application might execute some logic like warming up the cache which takes a couple of minutes. Then Kubernetes will wait until this is done and only then start routing traffic to the pod.Configuring Health Checks in .NET 5 and .NET CoreYou can find the code of the demo on GitHub.Health checks were introduced in .NET Core 2.2 and can be configured in the Configure method of the Startup class. To create a simple health check, you can use the MapHealthCheck extension and provide the name of the endpoint..NET Core and .NET 5 provide a wide variety of options to configure the health checks. For example, you can customize the return codes or even check if the database is accessible. For more details, take a look at the great documentation.Configuring Health and Readiness Probes with HelmFor more information about Helm and its configuration, see my previous post Helm - Getting Started.Open the deployment in the Helm charts folder. There you can see the liveness and readiness probe already.As you can see, the liveness probe checks the /health endpoint and the readiness probe the /ready endpoint. Since the /ready endpoint doesn’t exist, the pod won’t be able to start. The probes are only added when the probes.enabled value is set to true. by default, this value is already set to true. To change the value, go to the values.yaml file.Testing the Health and Readiness ProbesDeploy the microservice using the CI/CD pipeline and you will see that it fails during the Helm upgrade release task. This task times out after five minutes because the pod was not able to start (more precisely it started but the readiness probe failed). The deployment failed When you connect to the dashboard and open the details of the pod, you can see that the readiness probe failed. For more information about accessing the dashboard, see my previous post Azure Kubernetes Service - Getting Started. The readiness probe failed You also might see a warning that the liveness probe failed. This might be caused by Kubernetes checking the liveness of the pod before its port was opened. To prevent Kubernetes from checking too fast, use the initialDelaySeconds parameter to tell K8s to wait a certain amount of time before checking the first time.Fixing the broken Readiness ProbeChange the path in the readiness probe from /ready to /health. Additionally, I added the initialDelaySeconds parameter and set it to 15 seconds. This tells Kubernetes to wait 15 seconds before it executes its first check. The finished liveness and readiness probe looks as follows:Run the CI/CD again and this time the deployment will succeed and the pod will start successfully. The pod started successfully ConclusionReadiness probes are used to check if a pod is ready to receive traffic. Only after a successful probe, traffic is routed to the pod. Liveness probes work the same way as readiness probes and check periodically if a pod is still alive. If a pod is not alive anymore, Kubernetes restarts it. .NET 5 and .NET Core 2.2+ allow to easily create health checks with only a handful of lines of code.Today’s demo was very simple but it should show you enough to get started and create more complex probes.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Use a Database with a Microservice running in Kubernetes", "url": "/microservice-with-database-kubernetes/", "categories": "DevOps, Kubernetes", "tags": "DevOps, SQL, CI-CD, Azure DevOps, Azure, Docker, Kubernetes, AKS", "date": "2021-03-22 00:00:00 +0100", "snippet": "I showed in my last post how to automatically deploy database changes to your database. In this post, I will extend my microservice to use this database and also extend the deployment to provide a valid connection string.This post is part of “Microservice Series - From Zero to Hero”.Update the Microservice to use a DatabaseYou can find the code of the demo on GitHub.So far, the microservice uses an in-memory database. I want to keep the option to use the in-memory database for local debugging. Therefore, I add the following value to the appsettings.Development.json file:If you want to run the microservice locally with a normal database, set this value to false. Next, I add the database connection string to the appsettings.json file:It is a best practice to use User Scripts when you are dealing with sensitive data in your local environment. To add User Secrets, right-click on your project and select Manage User Scripts. Add User Secrets After adding the settings, we only need one more change in the Startup.cs class. Here we change the configuration of the in-memory database to either configure a real database connection or the in-memory one, depending on the settings:To use an SQL Server, you have to install the Microsoft.EntityFrameworkCore.SqlServer NuGet package. Install the SQL Server NuGet package Additionally, comment out the code in the constructor of the CustomerContext. The data was used as initial values for the in-memory database.If you want, run the application locally and test the database connection.Pass the Connection String in the CI/CD PipelineProviding the connection string in the CI/CD pipeline is simpler than you might think. All you have to do is add the following code to the values.yaml file inside the Helm chart folder of the API solution.This code sets the connection string as a secret in Kubernetes. Since the hierarchy is the same as in the appsettings.json file, Kubernetes can pass it to the microservice. The only difference is that the json file uses braces for hierarchy whereas secrets use double underscores (__). The value for __ConnectionString__ will be provided by the ConnectionString variable during the tokenizer step in the pipeline.Test the Microservice with the databaseDeploy the microservice and then open the Swagger UI. Execute the POST operation and create a new customer. Add a new customer to the database Connect to the database, for example, using the SQL Management Studio and you should see the new customer there. The new customer was added to the database ConclusionUsing a database with your microservice works the same way as with all .NET 5 applications. Due to the already existing CI/CD pipeline which is using Helm to create the deployment package, there were barely any changes necessary to pass the connection string to the microservice during the deployment.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Replace Helm Chart Variables in your CI/CD Pipeline with Tokenizer", "url": "/replace-helm-variables-tokenizer/", "categories": "DevOps, Kubernetes", "tags": "AKS, Helm, Kubernetes, YAML, Azure", "date": "2021-03-15 00:00:00 +0100", "snippet": "Helm is a great tool to deploy your application into Kubernetes. In my post, Helm - Getting Started, I also mentioned the values.yaml file which can be used to replace variables in the Helm chart. The problem with this approach is that the values.yaml file is hard-coded.In this post, I want to introduce Tokenizer which is a simple Azure DevOps extension with which you can replace variables in the values.yaml file.This post is part of “Microservice Series - From Zero to Hero”.Why would I replace variables in my Helm Charts?Currently the values.yaml file looks as follows:As you can see, the replica count or the tag is hard-coded as 1 and latest respectively. One replica might be fine for my test environment but definitely not for my production environment. In my post Automatically Version Docker Containers in Azure DevOps CI, I talked about the disadvantages of using the latest tag and that it would be better to use a specific version number. Though, this version number is changing with every build and therefore needs to be inserted automatically.Another use case of dynamic variables would be the connection string to your database. The connection string will be different for each of your environments.Install Tokenizer in Azure DevOpsYou can download the Tokenizer extension for free from the Marketplace. To download the extension, open the page of the extension in the marketplace and click on Get it free. Get the Tokenizer extension This opens a new page where you can either select your Azure DevOps Services organization to install it or download the extension if you want to install it on an Azure DevOps server. Install the Tokenizer extension This extension looks for variables starting and ending with a double underscore, for example, __MyVariable__ and replaces it with the value of the variable MyVariable.Add the Tokenizer Task to the Azure DevOps PipelineYou can find the code of the demo on GitHub.Add the following task before the HelmInstall task to your pipeline:Note that I am using templates in my pipeline and added the task to the HelmInstall.yaml file. You can find more information about templates in Improve Azure DevOps YAML Pipelines with Templates. These templates also use parameters. The task looks as follows:It is the same code as above except that the source file pattern is passed as a parameter.All files matching the sourceFilesPattern will be searched for tokens to be replaced. In my post Helm - Getting Started, I talked about overriding Helm chart values using the values.yaml file. For now, all I want to update is the repository and tag variable of the values.yaml file:Here I want to replace the repository with the ImageName variable and the tag with the BuildNumber variable.Testing the TokenizerI have the following variables in my pipeline:The GitVersion variable sets the version number. You can read more in my post Automatically Version Docker Containers in Azure DevOps CI. Run the pipeline and then check what image got deployed. You can use kubectl or a dashboard to check if the right image got loaded. For more information about using a dashboard, see my post Azure Kubernetes Service - Getting Started. The correct image got loaded ConclusionAutomatically replacing configuration values for different environments is crucial. This post showed how to use the Tokenizer extension and how to easily replace values in your CI/CD pipeline.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Auto-scale in Kubernetes using the Horizontal Pod Autoscaler", "url": "/auto-scale-kubernetes-hpa/", "categories": "Kubernetes, DevOps", "tags": "AKS, Helm, Kubernetes, YAML, Azure", "date": "2021-03-08 00:00:00 +0100", "snippet": "Running a performant, resilient application in the pre-cloud era was hard. Especially with unpredictable user traffic, it was often necessary to use more hardware than needed, just to make sure the application can handle an increased load. Kubernetes makes our life a lot easier and can automatically scale your application out and in, depending on the usage of your application.Today, I will show how to use the Horizontal Pod Autoscaler (hpa) to automatically scale your application out and in which helps you to offer a performant application and minimize your costs at the same time.This post is part of “Microservice Series - From Zero to Hero”.How the Horizontal Pod Autoscaler (HPA) worksThe Horizontal Pod Autoscaler automatically scales the number of your pods, depending on resource utilization like CPU. For example, if you target a 50% CPU utilization for your pods but your pods have an 80% CPU utilization, the hpa will automatically create new pods. If the CPU utilization falls below 50%, for example, 30%, the hpa terminates pods. This ensures that you always run enough pods to keep your users happy but also helps you not waste money by running too many pods.Besides CPU utilization, you can also use custom metrics to scale. These custom metrics can be, for example, response time, queue length, or hits-per-second. In my last post, Manage Resources in Kubernetes, I set resource requests for pods. If you don’t set them, the hpa won’t be able to scale based on CPU utilization.In the hpa, you can configure the minimum and maximum amount of pods. This prevents the hpa from creating new pods (until you run out of resources) when your application goes haywire but also ensures a bottom line to guarantee high availability. The Horizontal Pod Autoscaler checks by default the metrics every 15 seconds. You can configure the interval with the -horizontal-pod-autoscaler-sync-period flag.Create a Horizontal Pod AutoscalerYou can find the code of the demo on GitHub.In my demo, I am using Helm to deploy my application to Kubernetes. You don’t have to use Helm though and can just apply the yaml file I will create to your Kubernetes cluster.To create the Horizontal Pod Autoscaler, create a new yaml file named hpa inside the templates folder inside the Helm charts folder and paste the following code into the file:This config creates a Horizontal Pod Autoscaler if the hpa.enabled flag is set to true. Then it configures the specification with the maximum and minimum amount of replicas and at the end the target metric. In this example, the target metric is CPU utilization. All values starting with .Values are provided by the values.yaml file. Using the values.yaml file allows you have one file where you can override the configuration of your Helm charts. In my next post, I will show how to use a Tokenizer to apply dynamic values during your deployment.If you use the values.yaml file, add the following section:If you don’t use the values file, you can replace the placeholders in the hpa with actual values:This value can be configured using the –horizontal-pod-autoscaler-downscale-stabilization flag, which defaults to 5 minutes. This means that scaling down will occur gradually, smoothing out the impact of rapidly fluctuating metric values.Note that you should never run only one pod for production applications. I would recommend running at least 3 pods to ensure high availability.Deploy the Horizontal Pod AutoscalerDeploy the hpa to your Kubernetes cluster. If you want to learn how to deploy the Helm charts to Kubernetes, check out my post Deploy to Kubernetes using Helm Charts. After the deployment is finished, check that the hpa got deployed correctly. You can use kubectl or a dashboard to check if the hpa values are set correctly. For more information about using a dashboard, see my post Azure Kubernetes Service - Getting Started. The hpa got deployed Load testing a MicroserviceThere are many load testing tools out there. I wrote a super simple one myself and added it to the root of the GitHub repository inside the AutoscalingDemo folder. The code looks as follows:The test application creates 500 threads and calls my microservice to calculate a prime number. This is quite CPU heavy and will trigger the hpa to scale out my microservice. If you run this code, replace the string for the GetStringAsync method with your URL.Load test the Microservice without auto-scalingFirst, I run the load test without the hpa. This means that only one pod processes all requests which should take some time because the pod will run at 100% capacity. Run the load test with one pod The average response time is 508 milliseconds and when I open the Swagger UI of the microservice, it feels unresponsive.Load test the Microservice with auto-scaling using the HPAAfter deploying the hpa, I run the test again. Run the load test with auto-scaling Using the hpa to scale out the microservice decreased the average response time to 198 milliseconds. Also, the Swagger UI feels responsive and usable.Scaling using the Horizontal Pod AutoscalerSo far we have seen that the response time during the load time was way better when using a hpa. Let’s check what happened behind the scenes. If you open the hpa in the dashboard, you can see its events. There you can see that the hpa first scaled to four pods and then to seven. The hpa scaled to seven pods When you check the pods of the microservice, you will see that seven pods are running. Seven pods of the microservice are running After all, requests are processed (and a cooldown phase), the hpa scales in the pods. Since there is no load at all, it removes all pods but one. If you configured the minimum replicas to three, the hpa would scale to three pods. The hpa scaled in to one pod More Horizontal Pod Autoscaler ConfigurationThe hpa offers a wide range of additional features and configurations. In this section, I will shortly highlight some of them.Scale in DelaySome workloads are highly variable which would lead to a constant scaling (in or out). To prevent this and have a more stable number of pods, use the –horizontal-pod-autoscaler-downscale-stabilization flag to set a timeframe between scale in operations. The default value is 5 minutes. This means if the hpa scales in, the next scale in can happen in the earliest 5 minutes.Scaling PoliciesScaling policies allow you to configure for how long a certain value has to be reached until scaling happens. This could be for example, only scale out if the CPU utilization is higher than 70% for more than 30 seconds and only scale in if the CPU utilization is below 30% for 30 seconds. The code for this looks as follows:Policies can also be used to limit the rate of downscale, for example, only removing 3 pods per minute when scaling down.Stabilization WindowThe stabilization window restricts the hpa from scaling out or in too frequently. For example, if you set the stabilization window to 3 minutes (180 seconds) the timespan between scaling operations is at least 180 minutes. You can configure the stabilization window for scaling out and in independently. The following code shows a stabilization window for scaling down:ConclusionThe Horizontal Pod Autoscaler allows you to configure automatic scaling. This can be scaling out to increase the throughput and performance of your application or scaling in to reduce the used resources and therefore the costs. Scaling can be performed on simple metrics like CPU utilization or on more complicated metrics like response time or hits per second. Additionally, metrics can be combined.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Manage Resources in Kubernetes", "url": "/manage-resources-kubernetes/", "categories": "Kubernetes, DevOps", "tags": "AKS, Helm, Kubernetes, YAML, Azure", "date": "2021-03-01 00:00:00 +0100", "snippet": "Kubernetes is a great tool but it also has a steep learning curve. Today, I want to talk about how to limit the resources a container can use and how containers can request a minimum of resources on a node in the Kubernetes cluster.This post is part of “Microservice Series - From Zero to Hero”.Resource Requests vs. Resource LimitsResource requests describe how many resources, for example, CPU or RAM a node has to have. If a node doesn’t satisfy the requests of the container, the container (more precisely the pod) won’t be scheduled on it. If no node satisfies the request in your cluster, the pod won’t be scheduled at all.Resource limits describe how many resources, for example, CPU or RAM, a pod can use. For example, if the resource limit is set to 512MB RAM, the pod can only use 512 MB of RAM, even if it needs more, it won’t get more. This is important to prevent a pod go haywire and using all the resources of your node. Additionally, without a resource limit set, Kubernetes can’t perform auto-scaling of your pods. I will describe how auto-scaling using the horizontal pod autoscaler works in my next post, Auto-scale in Kubernetes using the Horizontal Pod Autoscaler.Pods are the smallest unit that can be created in Kubernetes. If a pod contains multiple containers, the resource requests of all containers are added up for the pod. Then Kubernetes checks where and whether it can schedule the pod. The resource limit can never be lower than the resource request. If you do so, Kubernetes won’t be able to start the pod.Resource and request limits can be set on a container or namespace level. In this post, I will only demonstrate the container-level approach. The namespace level approach works the same way though.Units for Resource Requests and LimitsCPU resources are defined in millicores. If your container needs half a CPU core, you can define it as 500m or 0.5. You will later see that Kubernetes converts 0.5 to 500m. Make sure to never request more CPU cores than your biggest node has. Otherwise, Kubernetes won’t be able to schedule your pod. It is a best practice to use 1 CPU core (1000m) or less for your pod except if you have a specific use case where your application can use more than one CPU core. It is usually more effective to scale out (create more pods) rather than use more CPU power.When your pod hits the CPU limit, Kubernetes will throttle the pod. This means that it restricts the CPU the pod can use. This may lead to worse performance of your application, but it prevents it from being evicted.Memory is defined in bytes. The default value is mebibyte which equals more or less a megabyte. You can configure anything from bytes up to petabytes. If you request more RAM than your biggest node has, Kubernetes will never schedule your pod.Unlike the CPU limit, when you hit the RAM limit, Kubernetes won’t be able to throttle the memory usage of the container. Instead, it will terminate the pod. If the pod is managed by a controller like a DeamonSet or Deployment, the controller will tell Kubernetes to create a new pod to replace the terminated one.Configure Resource Request Limits in Helm for a Container.You can find the code of the demo on GitHub.In my previous posts, I created two microservices and used Helm charts to deploy them to Kubernetes. For more information on the implementation see “Microservice Series - From Zero to Hero”.By default, Helm adds an empty resources section to the values.yaml file. This means that no resource request or limit is set.Uncomment the code and remove the empty braces. Then set values for the limits and requests, for example:This code configures that a container requests 100 millicores (0.1 CPU core) and 64 MB of RAM. It also limits the resources to a maximum of 128 MB RAM and 0.3 CPU cores. You will later see that Kubernetes converts these 0.3 CPU cores to 300m.That’s all you have to configure because Helm adds the reference to the values file automatically to the container section in the deployment.yaml file.Now you can deploy your application and check if the resource values are set correctly. If you want to learn how to deploy the Helm charts to Kubernetes, check out my post Deploy to Kubernetes using Helm Charts.Test that the configured Resource Values are setYou can use kubectl or a dashboard to check if the resource values are set correctly. For more information about using a dashboard, see my post Azure Kubernetes Service - Getting Started. The resource requests and limits are set correctly ConclusionResource limits should always be set to make sure that a pod can’t eat up all the resources of your Kubernetes node. Without resource limits set, Kubernetes won’t be able to automatically scale your pods using the horizontal pod autoscaler. I will show in my next post, Auto-scale in Kubernetes using the Horizontal Pod Autoscaler, how to use it to make your application more resilient and perform better under heavy load.Resource requests allow you to configure how many resources like CPU and RAM a node has to have available. Always make sure that you don’t configure too much RAM or CPU, otherwise, your pod will never be scheduled.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Automatically Deploy your Database with Dacpac Packages using Linux and Azure DevOps", "url": "/deploy-dacpac-linux-azure-devops/", "categories": "DevOps, Cloud", "tags": "DevOps, SSDT, SQL, Dacpac, CI-CD, Azure DevOps, Azure, Docker", "date": "2021-02-22 00:00:00 +0100", "snippet": "I showed in my last post how to use SSDT to create a dacpac package and how to deploy it locally. The SSDT project uses .NET Framework 4.8 which means that it runs only on Windows. Azure DevOps has a task to deploy dacpac packages, but it also only supports Windows. To be able to use a Linux environment, I will create a .NET Core project to build the dacpac package and build my own Docker container with the sqlpackage installed to deploy the dacpac to an SQL Server.This post is part of “Microservice Series - From Zero to Hero”.Use .NET Core to create the Dacpac PackageYou can find the code of the demo on GitHub.I have created a new folder in my solution, Database, which contains the SQL project. To build this project in a Linux environment, I add a new .NET Core 3.1 Class Library called CustomerApi.Database.Build. Next, I replace the SDK type with MSBuild.Sdk.SqlProj/1.11.4 and set the SQLServerVersion SqlAzure because I want to deploy it to an Azure SQL database.This references the MSBuild.Sdk.SqlProj project which can be found on GitHub. Unfortunately, this project doesn’t support .NET 5 yet, that’s why I use .NET Core 3.1.Add Scripts to the Deployment ProjectThe created Dacpac package should execute SQL scripts before and after the deployment. To achieve that, I create a folder Scripts which contains two subfolders called, PostScripts and PreScripts. The folders contain the Script.PostDeployment, respectively the Script.PreDeployment script. Structure of the scripts These scripts contain all SQL scripts which should be executed. The PostScripts folder contains the 00_AddCustomers script and therefore, I have added it to the Script.PostDeployment file.Additionally, I have to add the following code to the .csproj file:This code configures the execution of the scripts before and after the deployment.Be aware that the scripts are executed every deployment. If your script inserts data, you have to make sure that it checks the data before it inserts it. I am using a merge statement to update existing data or create it if it doesn’t exist. This script looks complicated but is quite simple.Add Tables to the Deployment ProjectLastly, I have to add the tables for my database. You can either add the SQL script in the root folder, or you can reference the SSDT project. Since I am lazy, I reference the SSDT project with the following code in the .csproj fileThis adds the Tables folder with the Customer table to the solution. Referencing the tables Finished Project and Build ProblemsThe finished .csproj file looks as follows:If you get an error building the project, add the following code to the MSBuild.exe.config:You can find the file under C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\MSBuild\\Current\\Bin (The path will vary, depending on your version, e.g. Professional, Enterprise, etc.). This is a known MSBuild bug which exists for around a year already.Build the Database Project in the Dockerfile to create a Dacpac PackageAfter the database build project is finished, it is time to include it in Dockerfile so it gets built in the CI/CD pipeline. The Dockerfile is located in the CustomerApi folder and contains already all statements to build the projects and run the tests. First, add a copy statement to copy the project inside the container:Next, add the following section:This code creates an intermediate container and labels it so you can access it later. Additionally, it builds the project and generates the dacpac package this way. If you were following this series, then you have seen the same method to collect the test results and code coverage.Upload the Dacpac PackageAfter building the Dacpac package, you have to extract it from the Docker container and upload it as the build artifact. This is done in the DockerBuildAndPush template which is located under pipelines/templates. Add the following code after the PublishCodeCoverage task:This code takes a container with the dacpac label and extracts the dacpac file (This is the same as the extraction of the code coverage and test results). Then it uploads (publishes) it for the Azure DevOps agent.Create a Linux Container to Deploy the Dacpac packageThe Azure DevOps team has an open GitHub issue from 2018 about deploying dacpac on Linux. Unfortunately, we haven’t gotten any news if and when they will implement it. Therefore, I will use a Linux Docker container with sqlpackage installed to deploy the database. The Dockerfile was originally created by a colleague of mine. I have created a new folder, Infrastructure, in the root folder and added the Dockerfile there.I also have uploaded the container to Dockerhub and will use it in the CI/CD pipeline.Deploy your Database using the Dacpac PackageTo deploy the dacpac package to the database, you should use a deployment task in your CI/CD pipeline. This task runs as a container job using the previously mentioned Linux container with the sqlpackage installed. This deployment also runs only after the build succeeded and only if the build reason is not a pull request The code for that looks as follows:This deployment executes the DatabaseDeploy template which takes two parameters, connectionString, and dacpacPath. The template looks as follows:The deployment downloads the previously published dacpac artifact and then executes a sqlpackage publish with the connection string to the database and the path of the dacpac package.Testing the Database DeploymentI added the database deployment also to the OrderApi. You can find the code of the demo on GitHub.Before you can run the pipelines, you have to replace the content of the SQLServerName variable with your server URL:Additionally, you have to add the DbUser and DbPassword variables in the pipeline. You can add the DbUser variable as a normal variable if you want. The DbPassword variable must be added as a secret variable though, otherwise, everyone can see the password. Add secret variables to your pipeline Run both (CustomerAPi and OrderApi) pipelines and you should see both databases deployed on your database server. Deploying the database in the CI-CD pipeline The databases got deployed on the server ConclusionDacpac is great to automate the database deployment including schema updates and the execution of scripts. Currently, the deployment is only supported on Windows but with a bit of tweaking, I was able to use .NET Core 3.1 and my own Linux Docker container to build and deploy it using Linux. Setting up everything is a bit of work but after you have done it once, you can easily copy and paste most of it into future projects.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Automatically deploy Database Changes with SSDT", "url": "/automatically-deploy-database-changes/", "categories": "DevOps", "tags": "DevOps, SSDT, SQL", "date": "2021-02-15 00:00:00 +0100", "snippet": "In my last post, I talked about deploying database changes automatically. Today, I will show how to use SSDT (SQS Server Data Tools) to generate a Dacpac package and how to deploy it to your SQL server.This post is part of “Microservice Series - From Zero to Hero”.Prepare your EnvironmentBefore you can get started, you have to download the custom SSDT Tools from GitHub. These custom tools are an extension of the MSBuild.Sdk.SqlProj project and are necessary to be able to open the SSDT project file (*.sqlproj) with Visual Studio.Getting to know the SSDT Project StructureYou can find the code of the demo on GitHub.Open the .sqlproj file with Visual Studio and you will see the project structure. There are two relevant folders inside the project, Scripts, and Tables.The Scripts foldersThe Scripts folder contains the PostScripts, PreScripts, and ReferenceDataScripts subfolders. The Scripts folder contains SQL scripts Scripts in the PreScripts folder are executed before the deployment whereas scripts in the PostScripts and ReferenceDataScripts folders are executed after the deployment. There are two folders after the deployment for a better separation of concern but it would also be fine if you put all your scripts into the PostScripts folder. You can execute any SQL script you want, except schema changes. Schema changes are defined in the Tables folder.The Tables folderThe Tables folder contains the definition of your tables. The definitions of all tables The MigrationScriptsHistory table is needed to store all executed migrations. All other tables are customer-defined and you can add whatever tables you need, for example, the Product table:If you double-click on the SQL file in Visual Studio, you can see the script file and the designer to edit the file. The SQL Designer in Visual Studio Generate the Database using SSDTI have added two tables, Customer and Product, and want to deploy this new database to my database server. Since Docker is awesome, I use a Docker container for my SQL Server. You can start an SQL Server 2019 with the following command:If you connect to your server, using tools like the SQL Server Management Tool, you will see that there is no database yet. No database is on the server To deploy the new database, right-click the .sqlproj file in Visual Studio and select Publish. Publish the database This opens the Publish Database window. Configure the database deployment As you can see, the connection string is empty. Click on edit and enter your database server information. Provide the connection settings This creates the connection string and now you can click on Publish to deploy your database. Deploy the database The publish process should only take a couple of seconds and Visual Studio will tell you when it is finished. The publish succeeded That’s already it. Refresh your SQL server and you will see the new database with its three tables. The database and tables got created Applying changes to an existing DatabaseIf you already have an existing database and want to apply changes, for example, adding a new column, you can simply add it to the existing table. Open the Product table and add a price column as a decimal. The code looks as follows:That’s already all you have to do. Publish the project again to update your SQL server to see the new column in the Product table. The Product table got a new column Configure the Target PlatformIf you don’t use an SQL Server 2019 then you have to configure your target platform before you can publish the project. Right-click the .sqlproj file and select Properties. Select the desired target platform in the Project Settings tab. Configure the target platform ConclusionSSDT offers a simple solution to automating your database deployments. It automatically checks the schema on the deployed database and compares it with the schema in your SSDT project. If there is a change, these changes are applied. This allows for fast changes and even allows for setting up environments for new developers fast and easily. The demo was very simple but should give you enough knowledge to get started.In my next post, I will show how to build the SSDT project in your CI/CD pipeline and how to deploy it to an Azure SQL Database.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Automate Database Deployments", "url": "/automate-database-deployments/", "categories": "DevOps", "tags": "DevOps, SSDT, CI-CD", "date": "2021-02-08 00:00:00 +0100", "snippet": "DevOps has been around for some years now and most developers know what it means. In simple terms, it stands for a culture where you automate all your steps from code merges to tests and deployments. Doing this for an application is often quite simple. It gets way harder when database changes are involved though.In this post, I will show you three ways how to automatically apply your changes and why one method is better than the other ones.This post is part of “Microservice Series - From Zero to Hero”.Why you should automate Database ChangesIn a modern DevOps culture, you want to deploy fast and often, with high quality. Achieving fast and qualitative high deployments can only be done with automation. Usually, when developers get started with DevOps, they start automating the build and release process of their application. Deploying an application, for example, in .NET or a Docker container is fairly simple but it gets complicated when database changes come into play. Since it is not as straightforward as deploying an application, database changes are often done by hand or at the start of the application. Both approaches are bad and in the following sections, I will give examples and explain why these approaches are bad in my eyes.Difficulties of Database ChangesI mentioned a couple of times that database changes are hard, but are they really?Yes and no. If you have experience with deploying database changes, they are simple. If you are new to this topic, it might be hard and confusing. It is crucial that you know the base concepts of database changes. The most important rule is to never deploy breaking changes. Let’s look at an example where you deploy breaking changes.You have an online shop with a Customer table and want to remove the FaxNumber column. You change the C# code and prepare the database migration. Then you run the database migration and suddenly your application stops working. This is because the old code is still deployed and this code expects the Customer table to have a FaxNumber.Keep in mind that you always have to support two versions of your application. The current deployed one and the future one. If your application runs in Kubernetes and you use rolling updates, you will have both versions running until all old pods are replaced with new ones.How to deploy Breaking ChangesBreaking changes need to be deployed over two deployments. In the first deployment, you stop using the FaxNumber column (make it nullable), and in a second deployment delete it. This also makes rollbacks easier since you don’t have to restore deleted data.Execute Entity Framework Core Migrations on StartupEntity Framework allows developers to write migrations. These can be to make changes (Up) or to roll previous changes back (Down). This could be, for example, creating a new table in the Up method and deleting the table in the Down method. These migrations can be triggered in the Startup method of the application. EF then checks the history of the migrations and executes each migration that wasn’t executed previously.The migrations are executed automatically during the startup of the application but it comes with a couple of downsides. If you run your application in a modern environment, like a cloud environment or Kubernetes, you don’t know when your application is evicted and restarted. This could mean that your application restarts every minute, which means that it checks every minute to execute the migration. Especially with containers, we want as fast as possible startup times. Another downside is that you have no control over the time when the migrations are applied. Maybe you want to deploy a feature but hide it behind a feature flag and then activate the feature and make the database changes later. This would require a second deployment and would make the whole process way more complex.Due to the downsides, try to avoid the Entity Framework migrations.Using Tools like FluentMigratorIf you don’t have much experience with migrations and look at tools like FluentMigrator, you might think that it is something great and will help you. The FluentMigrator gives you many helper methods to write SQL code with C# and also executes the migrations during the startup of the applications. You can find the project on GitHub.These helper methods are, for example, .Table(“”) or .PrimaryKey().Using the FluentMigrator makes it easy to see all migrations and also makes it fairly easy to write SQL code with C#. Though, it has the same downsides as the Entity Framework migrations and therefore shouldn’t be used in modern applications.SQS Server Data Tools (SSDT)SSDT is a project type for relational databases and is well integrated into Visual Studio. It enables IntelliSense support and build time validation to find problems fast and help developers to make changes easier. Its main feature is the schema comparison between an existing database and its own definition. For example, if your database has a table Customer with the column FaxNumber and the definition of the Customer table in your SSDT project doesn’t have this column, the SSDT project will delete the column automatically during the deployment. The developer has to write no code for that. On the other hand, if the SSDT project contains tables or even a database that doesn’t exist, the SSDT project will create these tables or even create a new database during the deployment (publish) process.Another big advantage is that SSDT allows new developers to set up new environments or a local test system very fast. The SSDT project automatically handles the versioning of the migration. This means no developer has to manually provide a version number and it guarantees that no more conflicts between multiple migrations with the same version number will occur.Additionally, SSDT can execute SQL scripts before or after the deployment. This means that DBAs can write complex SQL scripts and then only have to add the script to the right folder in the project.Since the database changes are deployed, it is very easy to integrate this deployment into your CI/CD pipeline. In my next two posts, I will show you how to create an SSDT project and how to deploy it locally, and then how to deploy it using a CI/CD pipeline in Azure DevOps.ConclusionThere are many ways to automatically apply database changes but you should strive for a solution that gives you full control over the execution of the changes. Entity Framework and NuGet packages like the FluentMigrator don’t give you these options and therefore shouldn’t be used. I recommend using SQS Server Data Tools (SSDT) to easily apply your schema changes or even execute SQL scripts.“In my next post”, I will show you how to apply changes using SSDT locally.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Analyze Software Architecture with NDepend", "url": "/analyze-software-architecture-ndepend/", "categories": "Miscellaneous", "tags": "Software Architecture, Tools, NDepend, .NET Core 3.1, C#, ASP.NET Core MVC, Visual Studio", "date": "2021-02-01 00:00:00 +0100", "snippet": "A part of my job as a software consultant is to analyze existing software to find room for improvement to make the software better or to avoid these mistakes if the software is rewritten soon. The focus areas of your analysis vary depending on the requirements and there are also many good tools out there to help you.Today, I will give a high-level overview of how I approach the analysis of a software project using NDepend.NDependNDepend is a tool for .NET developers that gives deep insight into the codebase. The tool can be installed as a Visual Studio extension and analyzes your code and then gives you different graphs and dashboards to dig deeper into found issues. NDepend comes with a lot of pre-configured rules to analyze the code. These rules can be easily modified or extended to fit your needs. For this post, I will stick with the default rulesNote: NDepend provided me with a free license but they didn’t get a preview version of this article nor do I accept any change requests in this article from them. Everything written here is my independent opinion.Install NDependYou can download a 14 day trial version from the NDepend website. After your download is finished, extract the zip file and run the installer. This will install the Visual Studio extension.Unfortunately, you have to keep this folder. If you delete it, the extension won’t work anymore. Also if you use Visual Studio to install the extension, you only get redirected to the download page. I would like it better if I could install the Visual Studio extension and don’t have to keep the folder.Analyze a .NET Core Software ProjectIf you are a reader of my blog, you know that I am a big fan of microservices. One big advantage of them is that they are small and simple. “Unfortunately” they are not great to showcase an analysis tool because there is not much to analyze. Instead, I will use the open-source NopCommerce. You can download the project from GitHub. Unpack the zip file and then open the solution which is inside the src folder.Create a new NDepend projectBefore you can create an NDepend project to analyze the solution, you have to build it. After the build is finished (it might take quite some time) go to Extensions –&gt; NDepend and select Attach New NDepend Project to Current VS Solution. Create a new NDepend project This opens a new window where you can select the solution file you want to analyze. By default, the Nopcommerce solution file should already be selected. There you can also see all selected dlls. NDepend analyzes dlls rather than the solution file. That’s the reason why you have to build the solution before you can analyze it. Select the solution file Click on Analyze 22 .NET Assemblies and NDepend will analyze them and then create a dashboard with its findings.The NDepend DashboardThe dashboard might look quite overwhelming at first but it is quite simple. One thing I don’t like is that you have to add the code coverage results after the dashboard is created and then run the whole analysis process again. It would be nice if you could add the code coverage results before analyzing the project the first time.To add a code coverage you have to create it first. In Visual Studio select Test –&gt; Analyze Code Coverage for all Tests (or the shortcut CRTL+U. CRTL+K). This runs all tests and then adds the Code Coverage Results panel. There you can export the results.In the NDepend dashboard click on the code coverage and then select the previously exported file. Run the NDepend code analysis again and you should see the code coverage in the dashboard. The NDepend dashboard The dashboard gives you a nice overview of the project so you know what you are getting into. For example, you can see that the solution has 78755 lines of code but only 7.56% code coverage. This is already a very bad sign. The technical debt is displayed as 38.16% which equals to 856 days of work according to NDepend. I don’t know how NDepend comes up with this estimation but it gives you a good overview that the code of NopCommerce is probably in pretty bad shape.A nice feature of the dashboard is that you can basically click everywhere to get more information. For example, click on the issues and you will get a list of all the issues with an explanation of what’s wrong there. It also navigates you directly to the corresponding class or method.The graphs of the dashboard are more or less empty at the moment because they show the changes over time. I think this is a nice feature. Even if the technical debt is not too accurate, its trend still might be useful. For example, if you are at the beginning of a sprint at 30% and at the end of the sprint at 36%, you can see that you probably created a lot of technical debt and should plan some time soon to fix it.Analyze Dependencies between DLLsWhen you start analyzing an already existing project, it is always hard to get started and to figure out which projects are the center of the solution. NDepend helps you here with an interactive Dependency Graph. You can open it via Extensions –&gt; NDepend –&gt; Dependency Graph. The dependency graph NopCommerce is a big solution but on the graph, you can see that it looks like that the solution is separated into three parts and the core parts are on the right of the graph containing dlls like Nop.Web and Nop.Core. Now that you have a starting point, you can dig deeper into the important dlls. For example, Nop.Web.Framework. You can easily zoom in with your mouse and see all dependencies inside this project. If you click on a namespace, for example, Nop.Web.Framework.Mvc, it gets highlighted in orange and shows you all calls to other namespaces and where it is used. This shows that the namespace is the core of the whole dll. Dependencies of a namespace You can learn a lot about the solution with the Dependency Graph even before you saw a line of code.Cyclomatic ComplexityThe cyclomatic complexity is a metric that is used to indicate the complexity of code. It measures every available path through a method or class. For example, the following method has a cyclomatic complexity of 2 because there are two available paths.A rule of thumb is that a method should not have a cyclomatic complexity of more than 7. More than 7 means that the code is too complicated to efficiently understand and change.NDepend gives you a graphical overview of all namespaces and their cyclomatic complexity. Click on Extensions –&gt; NDepend –&gt; Code Metrics. There you can select the level, for example, method or namespace, and what you want to display. Select Cyclomatic Complexity and then Top 10. This highlights the 10 methods with the highest complexity. Code metrics overview I am not a big fan of the optical presentation but it can give you a starting point to look into the most complex methods. The list of the top 10 methods is way more useful to me. You can find it on the right when you use the Code Metrics or you can click on Method Complexity on the dashboard- Cyclomatic complexity top 10 The list tells you the Namespace, class, and method and its cyclomatic complexity. Even before looking into the code, you can see that it’s terrible because the complexity is in the hundreds. Remember, the rule of thumb says not more than 7 and then you see 123 or even 253 here. This is going to be a problem if developers have to touch this code.Technical DebtTechnical debt describes the impact of the cost of additional work when you choose to implement a feature as fast as possible instead of taking a better approach. The more technical debt you acquire, the harder it will be to implement new features. That is the reason why projects which are 3-5 year olds see a drastic increase in time needed per feature.Let’s look at a real-life example. Instead of saving 20 years for a house, you take the shortcut and get a mortgage from the bank. You have to repay the debt with interest over the next 20 years. If you don’t repay the debt, it will increase due to the interest until you are unable to repay the mortgage and the bank will take your house.The same principle applies in software. If you always take shortcuts, the debt will increase until it’s too much and it will be almost impossible to implement new features because you introduce too many new features or have to change too much code.The NDepend dashboard shows the technical debt percentage, gives a rating, and estimates how much work you would need to invest to fix this debt. The NDepend dashboard AS you can see NDepend calculated the technical debt with 38.16%, gave the rating D, and estimates that it will take 856 days, or 3 years to fix it. No technical debt is not the goal and even if these 856 are estimated too high, it gives you an idea about the condition of the solution. With the complex dependencies, the missing code coverage combined with the technical debt, I can say that the NopCommerce solution is in pretty bad shape, even before looking at the code itself.NDepend aims at pointing out problems but also at helping you to resolve them. If you click on debt on the dashboard, you can see all issues listed on the right side. Overview of the technical debt By default it’s sorted by the time it takes to fix the issues but if you quickly go through the list, it gives you another indication about the bad state of the solution. You see a lot of code should be tested, method too big, too many parameters, type too big, too many fields, and so on. This all indicates that the methods and objects are very big and complex. Therefore they are hard to test or extend.Analyze the Software ArchitectureSo far, I used NDepend to get an overview of all the problems and get an indication that the NopCommerce solution is in a pretty bad state. But even if NDepend said that everything is good, the condition could be really bad. In this section, I want to show you a couple of fast indications to get a first opinion of the state of the project.Test QualityCode coverage only tells you a part of the story, 7.56% like in NopCommerce is way too low though. Even if you have a high code coverage, you could have bad tests or are missing the edge cases of methods. Let’s have a look at the following method and its test.This results in 100% code coverage. But if you think about the code, you will quickly see some flaws. If the second parameter is 0, you try to divide by 0 which is not allowed, and end in an exception. This means even with your 100% code coverage, your code produces an exception. Another problem you might see is that the result of the division is an integer. This means that the result is rounded and might lead to unexpected results, besides that it is very inaccurate.Test Quality in NopCommerceAs you have seen in the section above, the code coverage gives you the first indication but you always should look at the tests themselves to classify their quality. NopCommerce has a code coverage of merely 7.56% but let’s have a look at the quality of the existing tests. Since we have an ASP.NET Core MVC project, let’s check the tests of the controllers first. Unfortunately, there is only one test and as you can see on the following code sample, even this test is useless and doesn’t help at all.If you look through the tests, you will find a couple of tests for validators, automapper config, and models. Most of the tests are useless, for example, creating an object and then checking if the properties you set are still the same value. These tests only help you to get your code coverage percentage up but it doesn’t help the business running good software.Missing tests might indicate a problem with the architecture and/or too complicated classes which makes them untestable.Analyze the Project StructureAfter checking the lack of tests and the previously discovered complexity of too big classes, methods, and so on, I want to check if that’s the case. To do that I will start at the controllers and then go from layer to layer. I would expect a three-tier architecture where the controller takes a request and then calls a service. The service contains all the business logic and calls a repository. This repository handles all database operations. There could be more layers like a domain layer or a shared code library but this shouldn’t change too much.Since NopCommerce is an e-commerce shop, I guess that products, customers, and orders should be one of the main controllers. When you open the ProductController or the OrderController you will see that they have 3403 respectively 2826 lines of code. That is way too many!The constructor of the ProductController looks as follows:There are a couple of important principles in software development. My favorite is the Single Responsible Principle (SRP) and Keep it simple stupid (KISS). If you see a constructor like this you already know that this class does more than one thing and is not simple at all. A massive constructor like this makes it also almost impossible to test because you have to fake every parameter before you can even start writing your first test.Complexity of methodsIn the previous section, I analyzed that the controllers are way too complicated. In this section, I will look into a method and try to find the service and repository layer (if they exist) and analyze their complexity. An online store needs customers and without knowing anything about online shops, I would guess that the creation of one is a simple operation and therefore the method should be simple. Unfortunately, that’s not the case in NopCommerce. NDepend analyzed a cyclomatic complexity of 53 which is way higher than the maximum recommended 7. The method has almost 200 lines of code and looks as follows:A controller should only take the request and then call services that handle the business logic. From a first glimpse, it looks like the controller does all the business logic already. This would also mean that you can’t reuse the code to create a product because it is in the controller and not in a service class. Additionally, the controller does way more than creating a product that violates the Single Responsible Principle. Due to its insanely high complexity, it is also almost impossible to write adequate tests for the code.Previously, I said that I will dive down the different layers but after seeing the controller I feel like I have seen enough to give a first verdict.Code Reusability and APIsCode reusability is an important factor when it comes to development speed and maintenance costs in the future. I like microservices and one feature of them is that they have many APIs which allow code to be shared easily.Let’s look at the following use case: you have your NopCommerce online shop and then get the task to develop a mobile app for your store. With a modern microservice architecture, you only have to build the mobile app which then calls all your APIs to load data or create orders. This should be done fairly quickly. What about our existing NopCommerce solution?As far as I have seen there are no APIs in the whole solution and as previously mentioned, all the business logic is in the controller. This means that it is not even possible to create a shared library of the service layer to share the code with the mobile app. While looking over the controller methods, I also saw that security checks like if the user has the right role are done in each controller method. Additionally, these checks read certain cookie values that are tied to the online shop and won’t be available in your mobile app.I have spent maybe half an hour in the code but can already tell you that it’s not possible to reuse the code without a lot of unnecessary extra work.Last Thoughts on Analyzing Software ArchitectureYou can analyze the software architecture of a project with many goals in mind. It totally depends on the requirements of your analyzes. For example, if you should find out why software is running slowly, you would use a profiler or some measurement tools to find out which parts of your software are running slow. Then you can focus on these areas to improve the performance.Another aspect of analyzing a software project could be the deployment process. I haven’t talked about this at all but in a modern DevOps culture, all changes should be automatically tested and deployed (either after a change is committed or at the end of a sprint) and also should be reviewed before it gets merged in your master branch. Analyzing this process could span the build and deployment pipeline, what technologies are used, for example, Docker, and go even as far as analyzing how running the software works. For example, if you run it in Kubernetes with auto-scaling or if your application just crashes when it’s out of resources.Even if it’s not directly analyzing the software architecture, it makes sense to look at the planned and recently finished features. If there are only new features and no refactoring tasks and if the features took quite some time, you can already guess that the software is in a bad shape. Here it is also your job as a consultant or architect to intervene and try to convince the product owner/manager to spend more time refactoring.ConclusionThis post gave you a high-level overview of how you can get started when analyzing the architecture of a software project. Look out for untestable code because this is often an indication of too complex code or a bad software architecture and keep software principles like the Single Responsible Principle, Separation of Concerns, and KISS in mind. Use tools to get a starting point and then start there to dive into the solution in more detail.I used NDepend to analyze the NopCommerce solution and get an overview of its state. If this post sounded like bashing NopCommerce, this was not my intention. I used it because I wanted to show you a realistic piece of software and pointed out my findings. If they were all great I would have mentioned that too. Unfortunately, the negative ones totally outranked the good ones. I hope NDepend improves the installation process in the future so you can install it as an extension in Visual Studio without downloading and keeping the NDepend folder somewhere on your computer.NDepend was easy to use and give me a great overview of the state of the project and provided useful links to dive into the pain points of the solution. For example, I found quickly super complicated methods and could fairly easily make a list of the top 10 things I want to improve. I am not believing too much in the amount of work it tells me to fix the technical debt but it gives me an indication. 856 days is massive and should be definitively lower. Also, the graphs on the dashboard are nice to point out the trend where the project is going. If the graphs show that the technical debt went up by quite a bit over the last month, it might be easier to convince your manager to do fewer new features and fix more of the technical debt in the next sprint.I mentioned a couple of times that I am a big fan of microservices. If you want to learn more about microservices, see my series “Microservice Series - From Zero to Hero”.Note: NDepend provided me with a free license but they didn’t get a preview version of this article nor do I accept any change requests in this article from them. Everything written here is my independent opinion." }, { "title": "Improve Azure DevOps YAML Pipelines with Templates", "url": "/improve-azure-devops-pipelines-templates/", "categories": "DevOps, Kubernetes", "tags": "Azure DevOps, CI, YAML, AKS, Azure, Helm, Docker", "date": "2021-01-27 00:00:00 +0100", "snippet": "YAML pipelines can get quite long and unclear over time. In programming, developers use several files to separate logic to make the code easier to understand. The same is possible using templates in YAML pipelines. Additionally, these templates can be used in several pipelines reducing duplicate code.This post is part of “Microservice Series - From Zero to Hero”.YAML Pipelines without TemplatesIn my last post, I worked on a pipeline that built a .NET 5 application, ran tests, pushed a docker image, and deployed it to Kubernetes using Helm. The pipeline had 143 lines of code in the end. It looked like a wall of text and might be overwhelming at first glance.You can find this pipeline on GitHub. If you go through the history, you will see how it evolved over time.What Pipeline Templates areTemplates let you split up your pipeline into several files (templates) and also allow you to reuse these templates either in the same pipeline or in multiple ones. As a developer, you may know the Separation of Concerns principle. Templates are basically the same for pipelines.You can pass parameters into the template and also set default values for these parameters. Passing parameters is not mandatory because a previously defined variable would still work inside the template. It is best practice to pass parameters to make the usage clearer and make the re-usage easier though.Another use case for templates is to have them as a base for pipelines and enforce them to extend the template. This approach is often used to ensure a certain level of security in the pipeline.Create your first TemplateI like to place my templates in a templates folder inside the pipelines folder. This way they are close to the pipeline and can be easily referenced inside the pipeline.Create Templates without ParametersThe first template I create is for the build versioning task. To do that, I create a new file, called BuildVersioning.yml inside the templates folder and copy the BuildVersioning task from the pipeline into the template. The only additional step I have to take is to use “step:” at the beginning of the template and intend the whole task. The finished template looks as follows:Create Templates with ParametersCreating a template with parameters is the same as without parameters except that parameters get placed at the beginning of the file. This section starts with the parameters keyword and then lists the parameter name, type, and a default value. If you don’t have a default value, leave it empty.After the parameters, add the steps keyword and add the desired tasks.Use Templates in the Azure DevOps YAML PipelineI placed all tasks in a couple of templates. To reference these templates use the template keyword and the path to the file:If a template needs parameters, use the parameters keyword and add all needed parameters:I put all tasks into templates and tried to group what belonged together. The pipeline looks as follows now:The pipeline has now 51 instead of 143 lines of code and I think that it is way easier to find certain parts of the code now.Running the PipelineAfter you added your templates, run the pipeline and you will see that it works the same way as before. The pipeline works with the templates ConclusionTemplates are great to simplify Azure DevOps YAML pipelines. Additionally, they are easy to reuse in multiple pipelines and help so to speed up the development time of new pipelines.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Deploy to Azure Kubernetes Service using Azure DevOps YAML Pipelines", "url": "/deploy-kubernetes-azure-devops/", "categories": "DevOps, Kubernetes", "tags": "Azure DevOps, CI, YAML, AKS, Azure, Helm", "date": "2021-01-25 00:00:00 +0100", "snippet": "Microservices are becoming more and more popular these days. These microservices run most of the time in Kubernetes. A goal we want to achieve with microservices is a quick and reliable deployment.In this post, I will show how to deploy your application to Kubernetes (more precisely Azure Kubernetes Service (AKS)) using Helm and Azure DevOps pipelines.This post is part of “Microservice Series - From Zero to Hero”.Create a Kubernetes Cluster in AzureIf you are new to Kubernetes or want instructions on how to install an Azure Kubernetes Service (AKS) cluster, see my post “Azure Kubernetes Service - Getting Started”.Create a Service Connection in Azure DevOpsBefore you can deploy to AKS, you have to create a service connection so Azure DevOps can access your Azure resources. To create a new service connection go to Project settings –&gt; Service connections and click on New service connection. Create a new service connection This opens a flyout where you have to select Azure Resource Manager and then click Next. Choose a service connection type Select Service principal (automatic) as your authentication method and click Next. Service connection authentication method On the next step, select a scope level, your subscription, the resource group, and provide a name. For example, you could configure that the service connection is only allowed to access the subscription ABC and in this subscription access only the resource group XYZ. I want my service connection to access all resource groups. Therefore, I don’t select any. Configure the service connection Click on Save and the service connection gets created. Note that the service connection name will be used in the pipeline to reference this connection.Configure the Azure DevOps YAML Pipeline to Deploy to Azure Kubernetes ServiceI created already a YAML pipeline in my previous posts which I will extend now. You can find this pipeline on GitHub.Since I already have a Docker image on Docker hub, I only have to add the creation of the Helm chart and a couple of variables to the pipeline.Define Variables for the DeploymentFirst, I add the following variables at the beginning of the pipeline:The variables should be self-explaining. They configure the previously created service connection, set some information about the AKS cluster like its name, resource group, what namespace I want to use, and some information for Helm. For more information about Helm see my post “Helm - Getting Started”.Deploy to Azure Kubernetes ServiceSince I am using Helm for the deployment, I only need three tasks for the whole deployment. First I have to install Helm in my Kubernetes cluster. I use the HelmInstaller task and provide the Helm version which I previously configured in a variable.Next, I have to create a Helm package from my Helm chart. To do that, I use the HelmDeploy task and the package command. For this task, I have to provide the service connection, the information about my Kubernetes cluster, the path to the Helm chart, and a version. I calculate the version at the beginning of the pipeline and set it in the Build.BuildNumber variable. Therefore, I provide this variable as the version.The last step is to install the Helm package. Therefore, I use HelmDeploy again but this time I use the upgrade command. Upgrade installs the package if no corresponding deployment exists and updates it if a deployment already exists. Additionally, I provide the –create-namespace argument to create the Kubernetes namespace if it doesn’t exist.That’s already everything you need to deploy to Kubernetes. Run the pipeline to test that everything works as expected. The deployment to Kubernetes was successful For practice, try to add the deployment to another namespace, for example, prod.Test the deployed MicroserviceUse the dashboard of Kubernetes (see here how to use Octant) or use the Azure portal to find the URL of the previously created microservice. Find the external URL of the microservice Open the external URL in your browser and you will see the Swagger UI of the microservice. Swagger UI of the microservice running in AKS The finished PipelineThe full YAML pipeline looks as follows:Shortcomings of my ImplementationThis implementation is more a proof of concept than a best practice. In a real-world project, you should use different stages, for example, build, deploy-test, and deploy-prod. Right now, every build (if it’s not a pull request) deploys to test and prod. Usually, you want some tests or checks after the test deployment. The pipeline is also getting quite long and it would be nice to move different parts to different files using templates.I will implement all these best practices and even more over the next couple of posts.ConclusionUsing an Azure DevOps pipeline to deploy to Kubernetes is quite simple. In this example, I showed how to use Helm to create a Helm package and then deploy it to an Azure Kubernetes Service cluster. Over the next couple of posts, I will improve the pipeline and extend its functionality to follow all best practices.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Approvals for YAML Pipelines in Azure DevOps", "url": "/deployment-approvals-yaml-pipeline/", "categories": "DevOps", "tags": "Azure DevOps, CI", "date": "2021-01-21 00:00:00 +0100", "snippet": "DevOps is all about automation and making the process faster and more reliable. Sometimes a human must check a change or feature before it is deployed. This could be someone from QA or even marketing checking if the new design is as they wanted it. Adding approvals in the classic (old) editor in Azure DevOps is pretty simple but it’s not as straightforward with the new YAML pipelines.In this post, I will show how you can add approvals to your deployments when you use the modern YAML pipelines.This post is part of “Microservice Series - From Zero to Hero”.Add Approvals in the Classic Editor PipelineIf you still use an old classic editor pipeline, it is very easy to add an approval before the deployment. All you have to do is to click on the icon on the left side of your task. Note that this icon only appears when you hover over the task. After you clicked on the icon, select Pre-deployment approvals and add the users who are allowed to approve the deployment. Add approvals in the classic editor When you start the deployment, all approvers get an email to approve the deployment. Once the required approver approved the deployment, the task continues.Add Approvals when using YAML PipelinesYou can find my YAML pipeline on GitHub.Adding approvals when using YAML pipelines is a bit more work but not complicated. In your Azure DevOps project open Pipelines –&gt; Environments. If you ran my NuGet pipeline, you will already have two environments, nuget-publish-internal and nuget-publish-public. The environments are created (or referenced if they already exist) with the environment property in the pipeline as shown in the following code:If you don’t have an environment yet, create one by clicking on New environment.Configure an EnvironmentI want to add an approval to the publish task to nuget.org because I want only certain versions published. To do that, click on nuget-publish-public. If you ran the deployment already, you will see a list of all runs. If you haven’t run the deployment yet, you will see a get started page: Configure an environment Click on the three dots on the right side and select Approvals and checks. Open the approvals page On this page, you can add approvals, branch control, and business hours. Branch control lets you configure which branches are allowed to start a deployment. For example, you could configure that deployments are only allowed from the master branch. Business hours allows you to configure the hours a deployment can start. For example, if you set your business hours from 9 am to 5 pm, a deployment started at 6 pm won’t run until 9 am. This allows you to ensure that there is always someone available in case something goes wrong.Click on the three dots on the right side and select Approvals and checks. Add an approval Click on Approvals and the config flyout appears. I add myself as an approver, to keep it simple and allow me to approve my own runs. I leave the timeout at 30 days. If you add instructions to the approvers, they will get them in the notification email. Click on Create and the approval gets created. Configure the approval Test the Approval in the YAML PipelineRun the YAML pipeline and the build stage will run as always. The deployment stops before the publish to nuget.org task. When you open the deployment, you can see that an approver has to review it. Review the deployment Click on Review and then on Approve. Once you approved, the deployment starts. Approve the deployment ConclusionAdding approvals was very easy with the classic editor. With the new YAML pipelines, it is still pretty easy but you need a bit more clicks to create the process. Approvals are a great tool to ensure that the deployment gets reviewed before its deployed to a critical environment.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Publish NuGet Packages to Nuget.org using Azure DevOps Pipelines", "url": "/azure-devops-publish-nuget/", "categories": "DevOps", "tags": "Azure DevOps, NuGet, CI", "date": "2021-01-18 00:00:00 +0100", "snippet": "In my last posts, I published my NuGet package to a private NuGet feed. This is a good solution if you want to use your NuGet packages only internally. If you want to share them, you need a public NuGet feed. The by far biggest one (also the default feed) is nuget.org.In this post, I will extend my previously created Azure DevOps pipeline and deploy the NuGet package to nuget.org so everyone can download and use it.This post is part of “Microservice Series - From Zero to Hero”.Create a Nuget.org API KeyTo be able to push NuGet packages to nuget.org, you have to obtain an API key first. Go to nuget.org and signup or log in and then click on your profile and select API Keys. Create a NuGet API key This opens the API keys page of your account. There click on + Create and enter a name and expiry date. Additionally, select which packages you want to associate with your key. I uploaded the NuGet package by hand before, therefore I can see it in the list. Click Create and the API key gets generated. Generate a NuGet API key Make sure that you click Copy after the API key is generated. This is the only time you can access the key. If you don’t copy it, you have to refresh it to get a new one. Copy the API key Create a NuGet Connection in Azure DevOpsThe next step is to use the previously created API key to connect Azure DevOps with nuget.org. In your Azure DevOps project, click on Project settings and then Service connections. There click on New service connection and select NuGet. Select ApiKey as the authentication method, enter https://api.nuget.org/v3/index.json as the feed URL and paste the previously create NuGet API key in the ApiKey textbox. Provide a name and then click Save. Create a NuGet service connection Publish to Nuget.org using an Azure DevOps PipelineYou can find the code of the demo on GitHub.The last step is to use the previously created service connection to extend the Azure DevOps pipeline and push the NuGet package to nuget.org. Publishing to nuget.org is almost the same as publishing to a private NuGet feed.First, I create a new stage in which I download the previously created NuGet package.Next, I push the NuGet package to nuget.org using the dotnet core nuget push command. The only difference to push to the internal feed is that I use as feed type external and as publishFeedCredentials the previously created service connection.For more details about the steps see my post Publish to an Internal NuGet Feed in Azure DevOps.That’s all you have to do. Save the pipeline and run it. You can find the finished pipeline on GitHub.DotNetCore currently does not support using an encrypted Api KeyWhen you run the pipeline, you will get an error while publishing the NuGet package to nuget.org. Publishing to nuget.org failed Everything is set up correctly but there is a bug (or missing feature) in Dotnet Core and therefore you can’t use an API key to push packages. There is an open GitHub issue which was created in 2018 but Microsoft has ignored the problem so far.Bug WorkaroundLuckily there is a workaround until (if ever) Microsoft fixes the problem. You can use a dotnetcore custom command and provide the appropriate arguments to push to nuget.org.You tell the command which packages it should push, -s declares the destination, and -k provides the nuget.org API key. The best practice for secret variables is to create a new variable inside the pipeline by clicking on Variables in the top right corner and then click + on the pop-out. Provide a name and previously copied API key. Additionally, check “Keep this value secret” so no user can read the value. Click on OK and run the pipeline again. Publishing the NuGet package worked This time the publish worked.ConclusionPublishing a NuGet package to nuget.org works almost as the publish to a private Azure DevOps feed. Unfortunately, Microsoft has a bug in their dotnetcore task and therefore it is not working as expected. This post showed how you still can use an Azure DevOps pipeline to push a NuGet package to nuget.org using a small workaround.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Microservice Series - From Zero to Hero", "url": "/microservice-series-from-zero-to-hero/", "categories": "Cloud, Kubernetes", "tags": "AKS, Kubernetes, Docker, YAML, Azure, .NET 5, Helm, CI-CD, Azure DevOps, C#, Azure, .NET 6, Istio, Prometheus, ACR, Azure CLI, Azure Container Registry, Azure Functions, Azure Service Bus, Cloud, Continuous Integration, CQRS, DevOps, Dacpac, Docker-Compose, Grafana, IaC, KEDA, Kiali, MediatoR, Microservice, Linkerd, Managed Identity, AAD, Prometheus, Monitoring, Pull Request, RabbitMQ, SSDT, SSL, Swagger, TLS, xUnit, YAML", "date": "2021-01-17 00:00:00 +0100", "snippet": "I am working as a consultant and software architect on different projects that focus on microservices, DevOps, and Kubernetes. Many of my consulting jobs consist of explaining microservices, why they are great, and how to use them efficiently. Microservices is a buzzword that’s been around for a couple of years and almost every developer knows it. However, not many of them know how to implement those.Therefore, I decided to start this series with the intent of explaining what microservices are and to show how they communicate. After a couple of posts, I received some comments from my readers asking about more advanced topics such as deployments, monitoring, and asynchronous communication. This motivated me to continue this series and it grew to a total of 65 posts over almost two years. All posts use the same demo application. I tried my best not to change too much but the code of earlier posts might be somewhat different than it is now.Since these blog posts cover such a variety of topics, I will try to categorize them in such a way that they are easily found. I tried to add to add the posts to the relevant category and sometimes they may appear in several categories. Additionally, I will add the full list in chronological order at the end of this page.Getting Started with MicroservicesThe following posts explain the theory behind microservices and how to set up your first two .NET 6 (originally .NET Core 3.1) microservices. Both microservices use the mediator pattern and communicate via RabbitMQ. Microservices - Getting Started Programming a Microservice with .NET Core 3.1 Document your Microservice with Swagger CQRS in ASP .NET Core 3.1 Mediator Pattern in ASP .NET Core 3.1 RabbitMQ in an ASP .NET Core 3.1 Microservice Dockerize an ASP .NET Core Microservice and RabbitMQ Set up Docker-Compose for ASP .NET Core 3.1 Microservices Upgrade a Microservice from .NET Core 3.1 to .NET 5.0 Upgrade a Microservice from .NET 5.0 to .NET 6.0 Continuous Integration and Unit Tests in Azure DevOpsThis section is all about automated builds using YAML pipelines in Azure DevOps. Starting with a simple .NET Core pipeline, the pipeline switches to building Docker images and then runs xUnit tests inside the Docker container. Another topic of this section is automated versioning of Docker images and lastly splitting up the pipeline into smaller chunks using templates. Build .NET Core in a CI Pipeline in Azure DevOps Run the CI Pipeline during a Pull Request Build Docker in an Azure DevOps CI Pipeline Run xUnit Tests inside Docker during an Azure DevOps CI Build Get xUnit Code Coverage from Docker Automatically Version Docker Containers in Azure DevOps CI Improve Azure DevOps YAML Pipelines with Templates Split up the CI/CD Pipeline into two Pipelines Continuous Deployment with Azure DevOpsAfter building the Docker images, let’s focus on deploying these images to Kubernetes. Each pull request gets deployed into a new namespace, SSL certificates, and a unique URL is generated and Helm is used to manage the deployment. Using Helm allows overriding configuration values. Furthermore, this section explains how to deploy Azure Functions, and SQL Databases and push NuGet packages to an internal or public feed. Approvals for YAML Pipelines in Azure DevOps Deploy to Azure Kubernetes Service using Azure DevOps YAML Pipelines Improve Azure DevOps YAML Pipelines with Templates Replace Helm Chart Variables in your CI/CD Pipeline with Tokenizer Split up the CI/CD Pipeline into two Pipelines Deploy Microservices to multiple Environments using Azure DevOps Deploy every Pull Request into a dedicated Namespace in Kubernetes Automatically set Azure Service Bus Queue Connection Strings during the Deployment Deploy Azure Functions with Azure DevOps YAML Pipelines Deploy a Docker Container to Azure Functions using an Azure DevOps YAML Pipeline Publish NuGet Packages to Nuget.org using Azure DevOps Pipelines Automatically Deploy your Database with Dacpac Packages using Linux and Azure DevOps Deploy KEDA and an Autoscaler using Azure DevOps Pipelines Create Custom Roles for Azure DevOps in Azure Update DNS Records in an Azure DevOps Pipeline Kubernetes with HelmThe following posts explain Microsoft’s Azure Kubernetes Service and why Helm is useful for your deployments. After the basics, more advanced topics like Ingress controller, automated SSL certificate installation, and KEDA are discussed. Azure Kubernetes Service - Getting Started Helm - Getting Started Deploy to Kubernetes using Helm Charts Override Appsettings in Kubernetes Run a Kubernetes Cluster locally Manage Resources in Kubernetes Auto-scale in Kubernetes using the Horizontal Pod Autoscaler Readiness and Liveness Probes in Kubernetes Use Azure Container Registry in Kubernetes Automatically scale your AKS Cluster Debug Microservices running inside a Kubernetes Cluster with Bridge to Kubernetes Set up Nginx as Ingress Controller in Kubernetes Configure custom URLs to access Microservices running in Kubernetes Automatically issue SSL Certificates and use SSL Termination in Kubernetes KEDA - Kubernetes Event-driven Autoscaling SSL Configuration in KubernetesKubernetes helps to automatically create Let’s Encrypt SSL certificates and Nginx as Ingress controller allows the creation of unique URLs for each microservice. Set up Nginx as Ingress Controller in Kubernetes Configure custom URLs to access Microservices running in Kubernetes Automatically issue SSL Certificates and use SSL Termination in Kubernetes Create NuGet PackagesNuGet packages allow sharing of code between microservices. Additionally, the versioning of these packages gives developers full control over the version they are using and when they want to upgrade to newer versions. Create NuGet Packages in Azure DevOps Pipelines Publish to an Internal NuGet Feed in Azure DevOps Restore NuGet Packages from a Private Feed when building Docker Containers Publish NuGet Packages to Nuget.org using Azure DevOps Pipelines Database Deployments with Azure DevOpsDeploying database changes has always been a pain. Using dacpac packages allows developers or database administrators to easily deploy their changes to an existing database, or create a new one with a pre-defined schema and optionally with test data. Docker also comes in handy when trying to deploy the dacpac package using Linux. Since Azure DevOps doesn’t support deploying dacpacs on Linux, a custom Docker container is used to deploy the dacpac. Automate Database Deployments Automatically deploy Database Changes with SSDT Automatically Deploy your Database with Dacpac Packages using Linux and Azure DevOps Use a Database with a Microservice running in Kubernetes Azure Container Registry and Azure Service BusThe Azure Container Registry is a private repository in Azure. Since it is private, Kubernetes needs an Image pull secret to be able to download images from there. Additionally, this section shows how to replace RabbitMQ with Azure Service Bus Queues and how to replace the .NET background process with Azure Functions to process messages in these queues. Use Azure Container Registry in Kubernetes Replace RabbitMQ with Azure Service Bus Queues Use Azure Functions to Process Queue Messages Azure FunctionsAzure Functions can be used to process messages from queues and can be deployed as a Docker container or as .NET 6 application. Deploy Azure Functions with Azure DevOps YAML Pipelines Deploy a Docker Container to Azure Functions using an Azure DevOps YAML Pipeline Replace RabbitMQ with Azure Service Bus Queues Use Azure Functions to Process Queue Messages Infrastructure as Code, Monitoring, and LoggingInfrastructure as Code (IaC) allows developers to define the infrastructure and all its dependencies as code. These configurations are often stored in YAML files and have the advantage that they can be checked into version control and also can be deployed quickly using Azure DevOps. Another aspect of operating a Kubernetes infrastructure is logging and monitoring with tools such as Loki or Prometheus. Use Infrastructure as Code to deploy your Infrastructure with Azure DevOps Collect and Query your Kubernetes Cluster Logs with Grafana Loki Monitor .NET Microservices in Kubernetes with Prometheus Create Grafana Dashboards with Prometheus Metrics Service MeshBig Kubernetes clusters can be hard to manage. Service Mesh like Istio helps administrators to manage Kubernetes clusters with topics such as SSL connections, monitoring, or tracing. All that can be achieved without any changes to the existing applications. Isitio also comes with a bunch of add-ons such as Grafana, Kiali, and Jaeger to help administrate the cluster. Service Mesh in Kubernetes - Getting Started Istio in Kubernetes - Getting Started Use Istio to manage your Microservices Add Istio to an existing Microservice in Kubernetes KEDA - Kubernetes Event-Driven AutoscalingApplications have become more and more complicated over the years and often rely on external dependencies these days. These dependencies could be an Azure Service Bus Queue or a database. KEDA allows applications to scale according to these dependencies. KEDA - Kubernetes Event-driven Autoscaling Deploy KEDA and an Autoscaler using Azure DevOps Pipelines AAD AuthenticationUsing Azure Active Directory authentication allows users to authenticate their applications using Azure identities. The advantage of this approach is that no passwords need to be stored and managed for the connection. Use AAD Authentication for Pods running in AKS Implement AAD Authentication to access Azure SQL Databases Use AAD Authentication for Applications running in AKS to access Azure SQL Databases All Posts in Chronological OrderThe following list consists of all blog posts in chronological order: Microservices - Getting Started Programming a Microservice with .NET Core 3.1 Document your Microservice with Swagger CQRS in ASP .NET Core 3.1 Mediator Pattern in ASP .NET Core 3.1 RabbitMQ in an ASP .NET Core 3.1 Microservice Dockerize an ASP .NET Core Microservice and RabbitMQ Set up Docker-Compose for ASP .NET Core 3.1 Microservices Build .NET Core in a CI Pipeline in Azure DevOps Run the CI Pipeline during a Pull Request Build Docker in an Azure DevOps CI Pipeline Run xUnit Tests inside Docker during an Azure DevOps CI Build Upgrade a Microservice from .NET Core 3.1 to .NET 5.0 Get xUnit Code Coverage from Docker Azure Kubernetes Service - Getting Started Helm - Getting Started Deploy to Kubernetes using Helm Charts Override Appsettings in Kubernetes Run a Kubernetes Cluster locally Automatically Version Docker Containers in Azure DevOps CI Create NuGet Packages in Azure DevOps Pipelines Publish to an Internal NuGet Feed in Azure DevOps Restore NuGet Packages from a Private Feed when building Docker Containers Publish NuGet Packages to Nuget.org using Azure DevOps Pipelines Approvals for YAML Pipelines in Azure DevOps Deploy to Azure Kubernetes Service using Azure DevOps YAML Pipelines Improve Azure DevOps YAML Pipelines with Templates Manage Resources in Kubernetes Auto-scale in Kubernetes using the Horizontal Pod Autoscaler Replace Helm Chart Variables in your CI/CD Pipeline with Tokenizer Automate Database Deployments Automatically deploy Database Changes with SSDT Automatically Deploy your Database with Dacpac Packages using Linux and Azure DevOps Use a Database with a Microservice running in Kubernetes Readiness and Liveness Probes in Kubernetes Use Azure Container Registry in Kubernetes Replace RabbitMQ with Azure Service Bus Queues Use Azure Functions to Process Queue Messages Deploy Azure Functions with Azure DevOps YAML Pipelines Deploy a Docker Container to Azure Functions using an Azure DevOps YAML Pipeline Set up Nginx as Ingress Controller in Kubernetes Configure custom URLs to access Microservices running in Kubernetes Automatically issue SSL Certificates and use SSL Termination in Kubernetes Split up the CI/CD Pipeline into two Pipelines Deploy Microservices to multiple Environments using Azure DevOps Deploy every Pull Request into a dedicated Namespace in Kubernetes Use Infrastructure as Code to deploy your Infrastructure with Azure DevOps Debug Microservices running inside a Kubernetes Cluster with Bridge to Kubernetes Collect and Query your Kubernetes Cluster Logs with Grafana Loki Monitor .NET Microservices in Kubernetes with Prometheus Create Grafana Dashboards with Prometheus Metrics Service Mesh in Kubernetes - Getting Started Istio in Kubernetes - Getting Started Use Istio to manage your Microservices Add Istio to an existing Microservice in Kubernetes KEDA - Kubernetes Event-driven Autoscaling Deploy KEDA and an Autoscaler using Azure DevOps Pipelines Create Custom Roles for Azure DevOps in Azure Update DNS Records in an Azure DevOps Pipeline Automatically scale your AKS Cluster Use AAD Authentication for Pods running in AKS Implement AAD Authentication to access Azure SQL Databases Automatically set Azure Service Bus Queue Connection Strings during the Deployment Upgrade a Microservice from .NET 5.0 to .NET 6.0 Use AAD Authentication for Applications running in AKS to access Azure SQL Databases " }, { "title": "Restore NuGet Packages from a Private Feed when building Docker Containers", "url": "/restore-nuget-inside-docker/", "categories": "Docker, DevOps", "tags": "Azure DevOps, Docker, NuGet, CI", "date": "2021-01-11 00:00:00 +0100", "snippet": "In my last posts, I created a private NuGet feed and automatically uploaded a NuGet package with my Azure DevOps pipeline. Using Visual Studio to restore the NuGet package from the private feed works fine because my user has access to the feed. When I try to restore the package when building a Docker image, it will crash with a 401 Unauthorized error. The Docker build failed Today, I will show you how to create an access token for your private Azure DevOps NuGet feed and how to pass it to your Dockerfile to build Docker images.This post is part of “Microservice Series - From Zero to Hero”.Create a Personal Access Token (PAT) in Azure DevOpsTo create a personal access token in Azure DevOps, click on user settings, and select Personal access tokens. Create a Personal access tokens This opens a new flyout window. There you can enter a name, your organization, and the expiration date. If you select Custom defined as the expiration date, you can create a PAT which is valid for one year. Select custom defined scope and then select Read in the Packaging section. This allows the PAT to read the NuGet packages from the feed. Configure the PAT After you click Create, the new PAT is displayed. Make sure to copy it because you won’t be able to see it again after you closed the window. The PAT got created Pass the PAT to build the Dockerfile locallyYou can find the code of the demo on GitHub.Add a nuget.config fileThe first step to restoring the NuGet package from the private feed is to add a nuget.config file to the root folder of the CustomerApi project. This file contains the URLs for the nuget.org and private feed. Since this file gets committed to source control, I don’t add the PAT there because I want to keep it private. The file looks as follows:Use the PAT in the DockerfileDocker supports build arguments which I will use to pass the PAT inside the Dockerfile in the CI pipeline. Locally, I don’t pass a PAT but I can set a default value. Additionally, I have to add the PAT and a username to the nuget.config to be able to access the private NuGet feed. the code looks as follows:Instead of localhost, use the previously created PAT. Make sure to not commit it to your source control though. Additionally, you have to copy the previously nuget.config file inside the Docker image. You can do this with the following code:To use the nuget.config file during the restore, use the –configfile flag and provide the path to the nuget.config file.You can find the finished Dockerfile on GitHub.Run the docker build again and it will finish successfully this time.Pass the PAT in the Azure DevOps PipelineIn the Azure DevOps pipeline, create a new secret variable for the PAT. To do that, open the pipeline and then click on Variables on the right top. Click on the + symbol and then provide a name for the variable and the value of the PAT. Make sure to enable Keep this value secret to hide the actual value from users. Create a secret variable for the PAT Next, add the build-arg parameter to the docker build task and provide the previously created variable as PAT. The whole task looks as follows:That’s already all to restore the NuGet package from a private feed. Run the pipeline it will finish successfully.ConclusionYou can create a private access token (PAT) to access NuGet packages of a private NuGet feed in Azure DevOps. This token can be passed to the Dockerfile as a build argument and then inside the Dockerfile be added to the nuget.config file. This allows you to restore private NuGet packages locally and in your Azure DevOps pipeline without committing your secret PAT to source control.Note: If you run this demo, it will work without my private feed because I uploaded the NuGet package to nuget.org.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Publish to an Internal NuGet Feed in Azure DevOps", "url": "/publish-internal-nuget-feed/", "categories": "DevOps", "tags": "Azure DevOps, NuGet, CI", "date": "2021-01-04 00:00:00 +0100", "snippet": "In my last posts, I showed how to create NuGet packages in Azure DevOps pipelines. To easily distribute them, you have to publish these NuGet packages to a NuGet feed. This can be done by using nuget.org which is publicly accessible or you can use a private one in Azure DevOps.In this post, I will show how to create a private NuGet feed in Azure DevOps and how to automatically publish your NuGet packages there using an Azure DevOps pipeline.This post is part of “Microservice Series - From Zero to Hero”.Create a private NuGet FeedTo create a private NuGet Feed, open the Artifacts tab in your project in Azure DevOps and then click on + Create Feed. Create a new NuGet feed This opens a flyout where you can configure the new feed. Provide a name, its visibility, and the scope. Click Create and your private feed gets created. Create the NuGet feed That’s it. Now let’s edit the pipeline to upload the previously created NuGet package to the new feed.Upload NuGet Packages to a private Feed in an Azure DevOps PipelineYou can find the code of the demo on GitHub.I already created a pipeline that creates a NuGet package, in my last posts. I will extend this pipeline to upload the created NuGet package to the previously created NuGet feed. Publishing the NuGet package is pretty simple. I will create a new stage that will only run when the build stage was successful and also will only run if the build is not triggered by a pull request. This ensures that only builds from the master branch (or any feature branch) will be uploaded to the feed.Next, I create a deployment where I download the previously published artifacts. These artifacts got published by the build and contain the NuGet packages. I have to download it because the stages are independent of each other. If the upload was in the build stage, I could directly reference the NuGet package without the need to download it.Lastly, I use a DotNet Core task to push the NuGet package. Use push as the command, the path where you downloaded the NuGet package, and as feed type internal. Don’t care too much about the publish Vsts Feed id. This id is generated by Azure DevOps after you select your previously created feed.That’s already everything you need to publish the NuGet package to the private feed. The whole stage looks as follows:You can find the finished pipeline on GitHub.Publish the NuGet PackageRun the pipeline and after the build is finished, open your NuGet feed in the Artifacts tab. You will see your NuGet published there. The NuGet package got published If your pipeline doesn’t have enough permissions to publish the NuGet package, open the settings of your feed, and navigate to the Permissions tab. There, add the Build Service as Contributor. Authorize the build service Save the changes and run the pipeline again. Your NuGet package should get published now.Install NuGet Packages from your Private NuGet FeedTo install a NuGet package from your private feed, right-click your project or solution in Visual Studio, and select Manage NuGet Packages… On the right top of the window, you should see “Package source” and the settings button on the right of it. Click the settings button and a new window opens. Click on the + symbol and then add a name and the URL to your feed. Add the NuGet feed as package source Click OK and you can select your feed as your package source. Search for the previously uploaded NuGet package and install it. Install the NuGet package Note: If you try to add my feed URL, you will get a 401 error because your user is not allowed to access my feed. Nevertheless, the demo will work because I also uploaded the NuGet package on nuget.org which is a public feed. I can access the feed because I am logged into Visual Studio and therefore my credentials are passed. If I tried to use Docker to restore the NuGet package from my private feed, I would also get the 401 error.In my next post, I will show how to pass an access token to the Dockerfile to access the private NuGet feed.ConclusionCreating and using a private NuGet feed in Azure DevOps is easy. In this post, I showed how to create one and how to extend an Azure DevOps pipeline to automatically publish NuGet packages to the private feed. Then, I added the new feed to my project and used Visual Studio to install the NuGet package. In my next post, I will show how to pass an access token to the Dockerfile to access the private NuGet feed.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Create NuGet Packages in Azure DevOps Pipelines", "url": "/create-nuget-azure-devops/", "categories": "DevOps", "tags": "Azure DevOps, NuGet, CI", "date": "2020-12-28 00:00:00 +0100", "snippet": "I think the most important rule of a microservice is that it has to be deployed independently. When your project or company grows, it is very likely that you want to share code between projects or microservices. This leads to the question of how to share the code? Should you copy it into your microservice, reference dlls, or should every team develop its own code independently?In this post, I will show how to share code using NuGet packages and how to automatically create them in an Azure DevOps pipeline.This post is part of “Microservice Series - From Zero to Hero”.How to share Code between MicroservicesThere are several methods of sharing code between microservices but I think most of them are bad. Let’s have a look at the available options.Don’t share CodeNot sharing code between microservices helps to keep them independent but this means that each microservice must develop all features by itself. This can lead to a lot of unnecessary development time and therefore costs a lot of money. Therefore, not sharing code is not an option.Referencing DllsI prefer having all microservices in a big repository. This allows you to reference code from a different project. You could create a “Shared” folder and create in there projects which are shared between several microservices. The problem with this approach is that when one team changes the code, it changes the code for all microservices that reference the code. This will certainly lead to unintended and unexpected behavior. Since we want bug-free code, this option is not good to share code.Create Code and then copy it to MicroservicesAnother option would be to create a new project with the code and use this code as the base for further developments. If a microservice wants to use this code, then copy it into their own project and then can edit it without interfering with other microservices. This solution sounds better than the previous one but this leads to a lot of overhead. All these versions will go in different directions, therefore it will take a lot of time to do simple changes, for example, updating the .NET version of the project. This has to be done in each implementation instead of in one central one. Therefore this solution is also not good.Share Code with NuGet PackagesIf you want to get external code and have control over it, NuGet packages are perfect. You can install the version you need and also can update whenever you have to. If the NuGet package gets updated but you don’t want to update yours, it’s no problem at all. The source code of the NuGet package exists only once, therefore it can be updated or modified quickly. Once a new version is released, every microservice can update it whenever they want.Create the Source for the NuGet PackageYou can find the code of the whole demo on GitHub.For this Demo, I will create a simple NuGet package that offers one method which takes an integer and then calculates a prime number, depending on the input. Prime numbers are, for example, 2, 3, 5, 7, 11, and so on. The input parameter indicates the number you want, for example, if you input 2, the second prime number, 3, is returned. If you input 4, the fourth prime number, 7 is returned. I know this is not going to be the most useful NuGet package but calculating prime numbers is quite CPU intensive and I will use this method in a later example to demonstrate autoscaling if the CPU limit is reached.I created a new folder in the root of my repository and called it NuGet. Inside this folder, I created a new folder called Prime Number which contains a new .NET 5 class library project. The class library consists only of one class with one method to calculate the prime number.To make sure that the code does what I expect it to do, I created a test project to test it.That’s already all the code you need for the NuGet package. You could create the NuGet package manually by executing nuget pack in the root folder of the project but in the next section, I will show you how to create it automatically in an Azure DevOps pipeline.Create NuGet Package in Azure DevOps PipelineI created a new pipeline in Azure DevOps to create the NuGet package. For more information about the basics of build pipelines read my post Build .NET Core in a CI Pipeline in Azure DevOps.The pipeline is going to be very simple but let’s have a look step-by-step.The first part is configuring when the pipeline should run, what agent it uses, and two variables. I run the pipeline every time something is changed inside the NuGet folder, except if it’s inside a Test folder.The next section defines a stage called build and creates a version number. If you want to learn more about the build versioning see Automatically Version Docker Containers in Azure DevOps CI.After the version number is calculated, I execute dotnet restore and then dotnet build on all csproj files inside the NuGet folder.The next step is to run the unit tests and publish the code coverage results. If a test fails, I will stop the pipeline and don’t create the NuGet package.The last two tasks create the NuGet package using dotnet pack and then publish the generated artifacts. Dotnet pack is executed on all project files inside the NuGet folder which don’t end with Test.csproj. This means that the test project doesn’t get packed into a NuGet package. The publish of the artifacts will be used in the next stage to publish the artifact to a private NuGet feed and in another stage to publish it to nuget.org. You can read about publishing in my next post.You can find the finished pipeline on GitHub.Test the NuGet from the PipelineRun the pipeline and after it is finished successfully, you can find the NuGet package as an attachment of the build. In your build click on 2 published. Find the NuGet package in the build Open the packages-nuget folder and there you can find the created NuGet package. The created NuGet package Download it and install it in your microservice. In my next post, I will show you how to automatically upload it to a NuGet feed in the Azure DevOps pipeline and how to install it from there.ConclusionSharing code between microservices can be tricky but with NuGet packages, it is very simple and organized. In this post, I showed you how to automatically create a NuGet package using Azure DevOps pipelines. In my next post, I will show how to upload the NuGet package to a NuGet feed with the same Azure DevOps pipeline and how to use it in your microservices.You can find the code of the whole demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Automatically Version Docker Containers in Azure DevOps CI", "url": "/automatically-version-docker-container/", "categories": "DevOps, Docker", "tags": "Azure DevOps, Docker, CI", "date": "2020-12-21 00:00:00 +0100", "snippet": "Most examples of building Docker containers just use the latest tag to deploy or Docker images. This is simple but shouldn’t be done in a production environment.Today, I will show how to add semantic versioning to your CI/CD pipeline in Azure DevOps to automatically create a meaningful version for your Docker images.This post is part of “Microservice Series - From Zero to Hero”.Why you shouldn’t use the latest TagThe latest tag is simple and there is nothing wrong with using it if you want to try out something. In a production environment, you shouldn’t use it because you want to have full control and knowledge of what version you are running at any given time.Let’s say you have a test and production Kubernetes cluster and both run images using the latest tag. If a developer finishes a new feature the CI/CD pipeline creates a new image with the latest tag and deploys it to the test environment. That’s fine but what if a pod in your production cluster gets restarted? Kubernetes starts the image with the latest tag and suddenly you have the new feature running on one pod in your production cluster. If this new feature also needs changes to the database schema, the pod won’t start because the changes aren’t deployed yet. This will lead to more and more failing pods until your application is offline.Another problem you might run into when using the latest tag is the caching mechanism of Docker. Docker is caching quite aggressively. By default, Docker checks if the image exists locally, and if it does, Docker starts it. This means that once you have the image with the latest tag, Docker will not update it and always start the outdated version.There are ways around these problems but the version number is also there to inform your users or customers of a new version of your software/product. For example, my software ABC got updated from version 1.2 to 2.0 is way more meaningful than my software got a new latest version.Automatically create Semantic VersioningSemantic versioning is the most used versioning method and consists of three numbers divided by a period, for example, 1.4.35. The first number (1) indicates the major version, the second number (4) the minor version, and the last number (35) the patch version. Major version changes contain breaking changes whereas minor or patch changes are backward compatible.As of March 2021, the originally used BuildVersioning extension stopped working. I don’t know why and since the last update was years ago, I decided to use GitTools instead. This extension is constantly updated, has simpler usage and more downloads. I will leave the original post for reference at the bottom of this post.Install GitTools in Azure DevOpsYou can download the GitTools extension for free from the Marketplace. To download the extension, open the page of the extension in the marketplace and click on Get it free. Get the GitTools extension This opens a new page where you can either select your Azure DevOps Services organization to install it or download the extension if you want to install it on an Azure DevOps server. Download the GitTools extension This extension calculates the version and sets it in many different variations in different variables. On the following screenshot, you can see all available variables and what their values look like: Available variables with GitTools Add the Build Versioning Task to the PipelineYou can find the code of the demo on GitHub.Add the following two tasks as the first tasks of your job in your CI pipeline.These tasks install the GitVersion tool and calculate the build number variables.Additionally, you have to add a new file called GitVersion.yml to the root folder of your repository with the following content:The last step you have to take before using semantic versioning is to ensure that the pipeline does not perform a shallow fetch when checking out the code. You can check this by clicking on the three dots in the top right corner and then select “Triggers”. This opens a new page where you select the “YAML” tab and then “Get sources”. Uncheck shallow fetch on the bottom of the window. Disable shallow fetch Use the Semantic VersionAll you have to do to use the semantic version is to add the desired variable as the tag to your image name. I already have a variable for the image name and add $(GitVersion.FullSemVer) as the tag:Testing the Semantic VersionSetting up the semantic version is pretty simple. Run the CI pipeline with the master branch and you will see the Build Versioning task creating a new build number. On the following screenshot, you can see that version 0.1.130 was created. Determine the build version If you commit changes to the master branch and run the pipeline again, it will create version 0.1.129. If you don’t have any changes, it will create 0.1.128 again. After the CI/CD pipeline is finished, the new image gets pushed to Docker Hub. There you can see the new version and the old one. New vs old version Having a semantic version like 0.1.128 is way more meaningful than 358 and helps users of the image identify changes easily.Create a Version Number for Feature BranchesUsually, only the master branch is used to create new versions of an application. Sometimes a developer wants to create an image to test new changes before merging the feature into the master branch. When you run the pipeline with a feature branch (every branch != master), the Build Versioning task will create a preview version. This means that you still get a semantic version but the task adds the branch name and a counter to the end of the version, for example, 0.1.131-myFeature0000. Feature branch versioning This version number means that it was run for the branch named versionnumber and it was run the first time. If you commit changes and run the branch again, it will create the version 0.1.131-myFeature0001.ConclusionDevOps is all about automating tasks and processes. A part of this automation is the versioning of your application. Today, I showed how easy it is to use the Build Versioning extension in Azure DevOps to automatically create semantic version numbers and how to add them to a Docker image.You can find the code of the whole demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”The following text is the original article about BuildVersioningInstall Build Versioning in Azure DevOpsMy company created a free Azure DevOps extension which you can download from the Marketplace. To download the extension, open the page of the extension in the marketplace and click on Get it free. Get the build versioning extension This opens a new page where you can either select your Azure DevOps Services organization to install it or download the extension if you want to install it on an Azure DevOps server. Download the build versioning extension This extension automatically installs Git and then calculates the semantic version. The calculated version gets set in the Build.BuildNumber variable.Add the Build Versioning Task to the PipelineYou can find the code of the demo on GitHub.Add the following BuildVersioning task as the first task of your job in your CI pipeline.This configures the versioning task to use chocolatey to install Git if it’s not available, and then replace the Build.BuildNumber variable with the calculated semantic version number.Additionally, you have to add a new file called GitVersion.yml to the root folder of your repository with the following content:Use the Semantic VersionAll you have to do to use the semantic version is to add the Build.BuildNumber variable as the tag to your image name. I already have a variable for the image name and add the BuildNumber there:Testing the Semantic VersionSetting up the semantic version is pretty simple. Run the CI pipeline with the master branch and you will see the Build Versioning task creating a new build number. On the following screenshot, you can see that version 0.1.43 was created. Create build version If you commit changes to the master branch and run the pipeline again, it will create version 0.1.44. If you don’t have any changes, it will create 0.1.43 again. After the CI/CD pipeline is finished, the new image gets pushed to Docker Hub. There you can see the new version and the old one. New vs old versioning The new version 0.1.43 is way more meaningful than 358 and helps users of the image identify changes easily.Create a Version Number for Feature BranchesUsually, only the master branch is used to create new versions of an application. Sometimes a developer wants to create an image to test new changes before merging the feature into the master branch. When you run the pipeline with a feature branch (every branch != master), the Build Versioning task will create a preview version. This means that you still get a semantic version but the task adds the branch name and a counter to the end of the version, for example, 0.1.44-versionnumber0000. Version of feature branch This version number means that it was run for the branch named versionnumber and it was run the first time. If you commit changes and run the branch again, it will create the version 0.1.44-versionnumber0001.ConclusionDevOps is all about automating tasks and processes. A part of this automation is the versioning of your application. Today, I showed how easy it is to use the Build Versioning extension in Azure DevOps to automatically create semantic version numbers and how to add them to a Docker image.You can find the code of the whole demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Run a Kubernetes Cluster locally", "url": "/run-kubernetes-cluster-locally/", "categories": "Kubernetes, Docker", "tags": "Helm, Microservice, Kubernetes, Docker", "date": "2020-12-14 00:00:00 +0100", "snippet": "Running microservices in Kubernetes usually requires a cluster running in the cloud or on-premise. During the development or when debugging, developers often need to run their application quickly in Kubernetes. Spinning up a new cluster or configuring the deployment to an existing one might take longer than the time they actually need the cluster.The solution to this problem is to run Kubernetes locally on your development machine using Docker Desktop.This post is part of “Microservice Series - From Zero to Hero”.Installing Kubernetes locallyTo run your microservice in Kubernetes on your Windows developer computer, you have to install Docker Desktop first. After you installed it, open the settings and go to the Kubernetes tab. There you click on Enable Kubernetes. Enable Kubernetes Applying this setting restarts Docker. It may take a couple of minutes but once it’s back up, you have Docker and Kubernetes with one node running.Deploy a Microservice to your local KubernetesDeploying an application to a local Kubernetes works exactly the same way as if Kubernetes was running in the cloud or in your local network. Therefore, I will use Helm to deploy my microservice.Configure the Kubernetes ContextBefore you start, make sure that you have selected the right context of your local Kubernetes. To check the context, right-click on the Docker tray and hover over the Kubernetes tab. By default, the local Kubernetes context is called docker-desktop. If it is not selected, select it. Otherwise, you won’t deploy to your local Kubernetes cluster. Set the Kubernetes context Deploy a Microservice with HelmYou can find the code of the demo on GitHub. If you don’t know what Helm is or if you haven’t installed it yet, see Helm - Getting Started for more information.To deploy the microservice, open the demo application and navigate to the Helm chart of the CustomerApi. You can find it under CustomerApi/CustomerApi/charts. The chart is a folder called customerapi. Deploy this chart with Helm:The package gets deployed within seconds. After it is finished, connect to the dashboard of your cluster. If you don’t know how to do that, see my post Azure Kubernetes Service - Getting Started. There I explain how I use Octant and how to access your Kubernetes cluster with it.Testing the Microservice on the local Kubernetes ClusterOpen the Services tab and you will see the customerapi service with its external IP “localhost” and port 80. Check the CustomerApi Service This means that you can open your browser, enter localhost and the Swagger UI of the CustomerApi microservice will be loaded. The Swagger UI of the CustomerApi microservice Change the Port of the MicroserviceIf you want to change the port your microservice is running on, open the values.yaml file inside the customerapi folder. Change the port in the service section from 80 to your desired port, for example, 22334.If you already have the microservice deployed, use helm upgrade to re-deploy it with the changes, otherwise use helm install.After the package is installed, open your browser and navigate to localhost:22334 and the Swagger UI will be displayed.ConclusionKubernetes is awesome but it can get complicated to test small changes in a cluster. Docker Desktop allows you to install a Kubernetes cluster locally. Combined with Helm, a developer can deploy a microservice to a local Kubernetes cluster within minutes and test the application.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Override Appsettings in Kubernetes", "url": "/override-appsettings-in-kubernetes/", "categories": "Kubernetes, Docker", "tags": "Helm, AKS, Microservice, Kubernetes", "date": "2020-12-11 00:00:00 +0100", "snippet": "Changing configs was difficult in the past. In the .NET framework, we had to do a web.config transformation and we also had to have a web.config file for each environment. It was ok since there were usually only a couple of environments. This got improved a lot with .NET Core and its appsettings.json file which could override files depending on the environment.Nowadays, especially with microservices, we have many different environments and often create them dynamically. It is quite common to create a new Kubernetes namespace and deploy a pull request there, including its own database. We can’t have unlimited config files though. Especially when everything is so dynamic. This is where another neat .NET Core (now just .NET) feature comes in. It is possible to override values in the appsettings.json file using environment variables or even to replace these values using Tokenizer.In my last posts, I talked about Helm and showed how it can be used to easily deploy applications to Kubernetes. Today, I will show you you can dynamically configure your application using environment variables in Helm.This post is part of “Microservice Series - From Zero to Hero”.Deploy a Microservice using HelmYou can find the code of the demo on GitHub. If you don’t know what Helm is or if you haven’t installed it yet, see Helm - Getting Started for more information.Open the demo application and navigate to the Helm chart of the OrderApi. You can find it under OrderApi/OrderApi/charts. The chart is in a folder called orderapi. Deploy this chart with Helm:The package gets deployed within seconds. After it is finished, connect to the dashboard of your cluster. If you don’t know how to do that, see my post “Azure Kubernetes Service - Getting Started”. There I explain how I use Octant and how to access your Kubernetes cluster with it.In the dashboard, open the orderapi pod and you will see that there is an error. RabbitMQ causes an Exception The application wants to open a connection to RabbitMQ. Currently, there is no RabbitMQ running in my Kubernetes cluster. Therefore, the exception occurs.Override Appsettings with Environment VariablesThe settings for RabbitMQ are in the appsettings.json file. There is also a flag to enable and disable the connection. By default, this flag is enabled.Currently, I don’t want to use RabbitMQ, therefore I want to disable it. Microsoft introduced the DefaultBuilder method which automatically reads environment variables, command-line arguments, and all variations of the appsettings.json files. This allows developers to use environment variables to override settings without changing the code.I am reading the RabbitMQ configs in the Startup.cs class and then register the service depending on the value of the enabled flag:The enabled flag is in the RabbitMQ section of the appsettings.json file. To override it with an environment variable, I have to pass one with the same structure. Instead of braces, I use double underscores (__). This means that the name of the environment variable is rabbitmq__enabled and its value is false.Pass the Environment Variable using HelmHelm allows us to add environment variables easily. Add in the values.yaml file the following code:This passes the value as an environment variable into the deployment.yaml file.This code iterates over the envvariables and secrets section and sets the values as environment variables. This section looks different by default but I find this way of passing variables better.Update the MicroserviceUse Helm upgrade to update the deployment with the changes in your Helm package:After the changes are applied, open the dashboard and navigate to the orderapi pod.Testing the overridden ValuesYou will see that the pod is running now. The environment variable is also displayed in the dashboard. If your application is not working as expected, check there first if all environment variables are present. The pod starts and the environment variable is shown To test the application, click either on “Start Port Forwarding” which will give you a localhost URL or you can open the Service and see the external IP of your service there. Check the external IP of your service Open your browser and navigate either to the shown localhost URL or to the external URL of your Service. There you will see the Swagger UI of the OrderApi microservice. The Swagger UI of the microservice Conclusion.NET Core and .NET are great frameworks to override setting values with environment variables. This allows you to dynamically change the configuration during the deployment of your application. Today, I showed how to use Helm to add an environment variable to override a value of the appsettings.json file.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Deploy to Kubernetes using Helm Charts", "url": "/deploy-kubernetes-using-helm/", "categories": "Kubernetes", "tags": "Helm, AKS, Microservice, Kubernetes", "date": "2020-12-08 00:00:00 +0100", "snippet": "In my last post, I explained how Helm works and how to add it to your microservice. This post is going to be more practical. Helm is a package manager that helps you to deploy your application easily to Kubernetes.In this post, I will show you how to deploy an application to Kubernetes using Helm and how to avoid some pitfalls.This post is part of “Microservice Series - From Zero to Hero”.Deploy the Microservice with HelmYou can find the code of the demo on GitHub.Set upTo follow along with this demo, you have to have a Kubernetes cluster running. This can be on your local machine or in the cloud. I am using Azure Kubernetes Service. If you haven’t set up Kubernetes yet, read my post about AKS “Azure Kubernetes Service - Getting Started” to set up a cluster on Azure.Before you can deploy your application with Helm, you have to install Helm. You can use chocolatey on Windows to install it:For all other operating systems, see the Helm download page to download the right version for your system.Deploy your Microservice with HelmAfter you have installed Helm, open to charts folder of your application. In the demo application, the path is CustomerApi/CustomerApi/charts. There you can see a folder named customerapi. This folder contains the Helm package. To install this package use helm install [Name] [ChartName]. For the demo application this can be done with the following code:The package gets deployed within seconds. After it is finished, connect to the dashboard of your cluster. If you don’t know how to do that, see my post “Azure Kubernetes Service - Getting Started”. There I explain how I use Octant and how to access your Kubernetes cluster with it.In the dashboard, open the customerapi pod and you will see that there is an error. The pod can't start The error message reads: “Failed to pull image customerapi:stable”. The image can’t be pulled because it doesn’t exist with the stable tag. Another reason why the image can’t be pulled is that the repository can’t be found. My repository is wolfgangofner/customerapi, not customerapi. Let’s update the repository and tag and update the application with Helm.Change the Configuration of the Helm ChartYou can find the values.yaml file inside the customerapi Helm chart. This file provides a way to override the values of the configuration. Under the image section, edit the repository and the tag to use the correct ones:It is a best practice to always use a version number as the tag and not latest. Using the latest tag might end up in problems with the container cache but more important you can’t exactly know what version of the application you are running. For example, you are running latest but tomorrow I update latest to a new version. The next time your container gets restarted, it loads the new image and this might break your application.Update a Helm DeploymentThe configuration is updated and we can re-deploy the Helm chart. To update an existing deployment use helm upgrade [Name] [ChartName]:Test the deployed ApplicationAfter the Helm upgrade is completed, connect to the dashboard and open the pod again. There you can see that the pod is running now. The pod is running The application is running but how can you access it? The dashboard allows you to enable port forwarding for the pod by clicking the button “Start Port Forward”. Click it and you will get an URL that will forward to your application. Enable port forwarding Open the URL and you will see the Swagger UI of the Customer API microservice. The Customer API Swagger UI That’s nice to see but impractical since our customers won’t connect to our Kubernetes cluster before they access our applications. In my last post, I said that the Service object works as a load balancer and offers an external endpoint to access the application. Go to Services and check the external IP for the customerapi. The Service has no external IP The Service has no external IP. This is because the type of the service is ClusterIP. ClusterIP only publishes an internal IP but no external one. This means we have to update the Service to enable the external endpoint and then publish the application again. This works because Azure creates an Azure Loadbalancer and assignes it a public IP Address. If you run your K8s cluster on-premises, you have to point the public IP to the endpoint yourself.Expose the Application to the OutsideTo update the Service type, open the values.yaml file again and find the service section. Update the type from ClusterIP to LoadBalancer and save the file.Update the deployment with Helm upgrade again.After the upgrade is finished, you can use either use kubectl to get the service IP or look it up in the dashboard. To use kubectl use the following command in Powershell:Alternatively, open the Service in the dashboard again, and there you can see the external IP. The Service has an external IP now Enter his IP address in your browser and you will see the Swagger UI of the Customer API Microservice. The microservice is publicly accessible ConclusionHelm is a package manager for Kubernetes and can be used to easily deploy and update your applications. I showed how to quickly update the configuration with the values.yaml file and how to make your application publicly accessible with the Kubernetes Service object.In my next post, I will show how to override values in the appsetting.json file with environment variables to allow for more dynamic configurations.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Helm - Getting Started", "url": "/helm-getting-started/", "categories": "Kubernetes", "tags": "Helm, AKS, Microservice, Kubernetes", "date": "2020-12-07 00:00:00 +0100", "snippet": "Helm is a package manager for Kubernetes which helps developers quickly deploy their applications. In my last post, Azure Kubernetes Service - Getting Started, I showed how to deploy an application to AKS using the dashboard.In this post, I will talk about the basics of Helm and show how to add it to your application.This post is part of “Microservice Series - From Zero to Hero”.What is Helm?Deploying microservices to Kubernetes, especially if they have dependencies, can be quite complex. This is where Helm comes in. Helm is a package manager for Kubernetes that allows you to create packages and helm takes care of installing and updating these packages. Helm packages are called charts. These charts describe everything your application needs and helm takes care to create or update your application, depending on the provided chart. Helm also serves as a template engine which makes it very easy to configure your charts either locally or during your CI/CD pipeline.Let’s add a Helm chart to our microservice and I will explain every component of the chart. You can find the code of the demo on GitHub.Add Helm Charts to a MicroserviceVisual Studio 2019 comes with great Helm support. Right-click on your API project and select Add –&gt; Container Orchestrator Support. Add Container Orchestration Support This opens a new window where you can select Helm or Docker-compose. Select Kubernetes/Helm and click OK. Add Helm If you don’t have Kubernetes/Helm as an option, make sure that you have the Visual Studio Tools for Kubernetes installed. Add the Visual Studio Tools for Kubernetes Visual Studio creates a new folder called charts and places a folder inside this charts folder with the name of your project. It’s important to only use lowercase because Kubernetes can process only lowercase names. The Helm charts got added Helm Chart TemplatesHelm created another subfolder, called templates, and places a couple of files into it. If you ignore the _helpers and Notes files, the remaining file names should sound familiar if you think about Kubernetes objects. The charts folder contains templates for the Service, Deployment, Ingress, and Secrets. These four objects are needed to run your application in Kubernetes. At a first glance, these files look super complicated and confusing but they are quite simple. You have to know the yaml definition of the Kubernetes objects though. The templates are basically the same, except that the values are added dynamically.Let’s have a look at the service.yaml file:This file defines a service, adds labels, and then configures its ports and protocol. Helm replaces all values inside the two braces. What’s important to notice is that some start with .Values.*. These values come from the values.yaml file which is outside of the templates folder.Override ValuesIn the previous section, I showed the service.yaml file. This file reads two values from the values.yaml file: .Values.service.type and .Values.service.port. You can find the respective files in the values.yaml file in the service section:Take a look at the values.yaml file and you will see that the type of the service is ClusterIP and its port is 80. This approach enables you to configure your application with changes in only one file. The same principle applies to all files inside the templates folder.The fullnameOverride parameter is used to replace the variable customerapi.fullname which serves, for example, as the name of the deployment and service. Usage of the fullnameOverride variable ConclusionIn this post, I gave a very short introduction to Helm. Helm is a package manager for Kubernetes and helps you to deploy your application including all its dependencies to Kubernetes. Helm also serves as a template engine which makes it very easy to change values.In my next post, I will show who to deploy our microservice to Kubernetes using Helm. You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Azure Kubernetes Service - Getting Started", "url": "/azure-kubernetes-service-getting-started/", "categories": "Kubernetes, Cloud", "tags": "AKS, Kubernetes, Docker, YAML, Azure", "date": "2020-11-30 00:00:00 +0100", "snippet": "In my eyes, the two biggest inventions in the last years are the cloud and Kubernetes (K8s). Today, I want to combine both and give you a high-level overview of Kubernetes using Microsoft’s Azure Kubernetes Service (AKS). At the end of this post, you will know why Kubernetes is awesome and how to deploy your first application and even load balance it.This post is part of “Microservice Series - From Zero to Hero”.Introduction to KubernetesKubernetes is a container orchestrator. This means that it runs your containers, usually Docker containers, and also comes with the following nice features: Service Discovery and Load Balancing Self-healing Automated Deployments Certificate Management Declarative ConfigurationKubernetes can be configured with yaml files, which means that you can store all your configuration in your source control. The smallest unit in Kubernetes is not a container, instead, it uses pods. A pod can contain one or more containers. Additionally, K8s can be easily extended with your own objects.Service Discovery and Load BalancingLoad balancing and especially service discovery has always been complicated and required some skills to set up. Both can be achieved in Kubernetes with a Service. This Service object takes all requests for an application (often a microservice) and load balances the request to an available pods. Further down, I will show how to deploy an application with three pods and use the Service to load balance between those pods.Self-healingAnother complex problem for, especially, on-premise applications is self-healing. This means if an application crashes, it gets automatically restarted. Kubernetes does this with health checks. You can provide an URL, often /health and K8s will check if this URL returns a request with the status code &gt;= 200 and &lt; 400. If the code is not within this range, Kubernetes restarts the pod and marks it as unavailable during the restart. Therefore, no user will be routed to the restarting pods which means that from a user’s perspective everything looks fine.Automated DeploymentsKubernetes supports two deployment modes out-of-the-box: rolling deployments and blue-green deployments. Requests are only routed to new pods, once they are marked as running. K8s checks this by using readiness probes. They work the same way as health probes, except that they are only used to check if a pod is ready to serve requests.The rolling deployment mode starts a new pod and once this one is running, it deletes an old one. This process is repeated until all old pods are replaced. You can configure how many pods you want to replace at the same time. This deployment mode is the default.The blue-green deployment starts all pods of the new version. Once all are running, Kubernetes switches the traffic from all old pods to all new ones. Afterwards, K8s deletes the old pods.Certificate ManagementCertificate management has been a problem for a long time. When a new application is deployed, a new certificate needs to be ordered and then installed. Kubernetes does all that for you. In K8s you can run a certificate manager, for example, let’s encrypt which creates certificates during the deployment and applies them to your application.Declarative ConfigurationEvery configuration is done in yaml in a declarative way. This means that you can check-in your files in source control. A declarative configuration means that you tell Kubernetes what you want and it takes care of achieving this. For example, run 10 copies of my application and load balance all incoming traffic. K8s then creates a service and starts 10 pods of your application.Why use Azure Kubernetes Service (AKS)Azure Kubernetes Service is a managed service for Kubernetes. In a simplified way, Kubernetes consists of two parts, the control plane (master node), and the worker node. The control plane does all the tasks necessary to manage the Kubernetes cluster. The worker nodes are running your applications and everything needed for that, like load balancing. AKS manages the control plane for you, this means that you don’t have to care what’s going on in the background. You can create an AKS cluster and just use it for your applications.Setup AKSMicrosoft did a lot of work on AKS in the last year and greatly improved the deployment of a new Azure Kubernetes Cluster.In the Azure Portal, search for aks and select Kubernetes service. Search for aks Next, select a resource group, provide a name and a region. For the node size, I would recommend using Standard B2s since this is the cheapest VM size at the moment. Also decrease the count to one, which will also save costs. In a production environment, you should use at least three nodes. Create a Kubernetes cluster On the next tabs, leave everything as it is and click Create on the last tab to start the deployment. Start the AKS deployment The deployment should be finished in a couple of minutes.Access the AKS ClusterYou can access your new AKS cluster using PowerShell and the Azure CLI module.After you installed the az module, you can log into your Azure subscription:This opens a browser window where you can enter your username and password. After you are successfully logged in, connect to your AKS cluster. If you are following my example, you can use the following command.Perhaps, you have to change the resource group or name of your aks cluster, depending on what you entered during the deployment.Azure is not deploying the Kubernetes dashboard anymore. As an alternative, I am using Octant which is an open-source tool from VMware. If you are on windows, you can install it using Chocolatey.Once you installed Octant, open it and it will automatically forward your request and open the dashboard. The Ocant Kubernetes dashboard Deploy the first ApplicationTo deploy your first application to Kubernetes, you have to define a Service and a deployment. The Service will act as an ingress controller and does the load balancing and the deployment will deploy the defined container with the desired replica count.You can define both objects inside a single yaml file. First, let’s create the service:This service defines itself as load balancer and redirects to port 80 on pods with the label kubernetesdeploymentdemo. Labels are used as a selector, therefore the Service knows to which pod it should forward a request.Next, create the Deployment object.This object might look a bit complicated in the beginning but it’s quite simple. It defines a Deployment with the name kubernetesdeploymentdemo and sets the label kubernetesdeploymentdemo. Next, it configures three replicas, which means that three pods will be created and in the container section, it defines what container it should download and on what port it should be run.Save this yaml file, for example, as demo.yml and run the following command:If you don’t have kubectl installed, install it with the following Powershell command:After you applied the demo.yml file, you will see a new Service, kubernetesdeploymentdemo, in the Kubernetes dashboard. The Service got created You can also see the external IP of your Service there. Remember this IP for later to test the application. Next up, we can see the Deployment and that 3/3 pods are running. A Deployment got created Lastly, click on the deployment and you can see the three pods running. All pods are running There you can see that all pods are healthy and that they are all running on the same node. In a production environment, they would be running on different nodes to ensure high-availability.Testing the DeploymentOpen the URL from the Service and you will see a Swagger UI. This application is very simple and all it does is to return the name of its host machine. You can also see the name in the headline. Testing the deployed container If you refresh the page a couple of times, you will see different names in the headline due to the load balancing hitting different pods. Different names due to the load balancing CleanupWhen you are finished, don’t forget the delete all created resources. AKS creates two additional resource groups. Make sure to delete them too. Delete all resource groups ConclusionToday, I gave a high-level overview of Kubernetes using Azure Kubernetes Service. Kubernetes helps you to run your container and manages deployments and load balancing. Keep in mind that this was a very simple demo and hasn’t talked about any downsides of Kubernetes and its steep learning curve.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Get xUnit Code Coverage from Docker", "url": "/get-xunit-code-coverage-from-docker/", "categories": "DevOps, Docker", "tags": "Azure DevOps, C#, Docker, xUnit", "date": "2020-11-23 00:00:00 +0100", "snippet": "Getting code coverage in Azure DevOps is not well documented and the first time I configured it, it took me quite some time to figure out how to do it. It gets even more complicated when you run your tests inside a Docker container during the build.In my last post, I showed how to run tests inside a container during the build. Today, I want to show how to get the code coverage of these tests and how to display them in Azure DevOps.Code coverage gives you an indication of how much of your code is covered by at least one test. Usually, the higher the better but you shouldn’t aim for 100%. As always, it depends on the project but I would recommend having around 80%.This post is part of “Microservice Series - From Zero to Hero”.Setting up xUnit for Code CoverageYou can find the code of the demo on GitHub.Install CoverletI use coverlet to collect the coverage. All you have to do is installing the NuGet package. The full NuGet configuration of the test projects looks as following:I am using FakeItEasy to mock objects, FluentAssertions for a more readable assertion and xUnit to run the tests.Collect the Code Coverage ResultsAfter installing coverlet, the next step is to collect the coverage results. To do that, I edit the Dockerfile to enable collecting the coverage results, setting the output format, and the output directory. The code of the tests looks as follows:The output format is json and Cobertura because I want to collect the code coverage of all tests and merge them into the summary file. This is all done behind the scenes, all you have to do is using the MergeWith flag where you provide the path to the json file. You could also build the whole solution if you don&amp;’t want to configure this. The disadvantage is that you will always run all tests. This might be not wanted, especially in bigger projects where you want to separate unit tests from integration or UI tests.This is everything you have to change in your projects to be ready to collect the coverage. The last step is to copy the results out of the container and display them in Azure DevOps.Display the Code Coverage Results in Azure DevOpsIn my last post, I explained how to copy the test results out of the container using the label test=${BuildId}. This means that besides the test results, the coverage results are also copied out of the container already. All I have to do now is to display these coverage results using the PublishCodeCoverageResults tasks from Azure DevOps. The code looks as follows:The whole code to copy everything out of the container, display the test results and the code coverage looks as this:Save the changes and run the CI pipeline. After the build is finished, you will see the Code Coverage tab in the summary overview where you can see the coverage of each of your projects. Summary of the Code Coverage Results ConclusionThe code coverage shows how much of your code is covered by at least one test. This post showed how easy it can be to display these results in Azure DevOps, even when the build runs inside a Docker container.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "C# 9.0 - What's new", "url": "/c-sharp-9-whats-new/", "categories": "Programming", "tags": ".NET 5, C#", "date": "2020-11-16 00:00:00 +0100", "snippet": "Microsoft released with .NET 5 also C# 9.0. This version of C# focuses mainly on productivity improvements and tries to help developers to reduce their time typing.You can find the code of this demo on GitHub.Record TypesMy favorite feature of C# 9.0 is record types. They allow you to define a class with its properties and a constructor in one line. A base class with an inherited class looked as follows so far:With C# 9.0, you can define both classes with its properties, constructors, and even the SaySomething method with the following code:This should reduce the typing required for simple class definitions.Init Only SettersThe next new feature is Init only setters. They allow you to set a value for a property when you create the object but then prohibit you from setting a new value for the property. All you have to do for that is using int instead of set in the property definition:You can set the value for Name when you create the object but you will get a compiler error if you try to set a new value.Improved Pattern MatchingThe pattern matching which was first introduced in C# 7 got new keywords. Now you can concatenate the check of your expression with not, and, and or. In the following code, I check if the provided character either a lowercase or uppercase letter but not a number:Top-Level StatementsTop-Level statements remove all the boilerplate around a class like using statements and a namespace. A typical hello world application would look as follows:With the new top-level statements, you can create the same application with a single line of code:Running this application is merely a gimmick feature to me but I think it has the potential to reduce to time developer using typing repetitive definitions and therefore might help improve productivity.Fit and FinishThe last feature is called fit and finish and allows you to leave out the type definition when you create a new class. The following code creates a new object of the class FitAndFinish and sets the Name property:Since the type of the name is in front of the variable, the compiler knows what type this variable instantiates. I am not too excited about this feature because I always use var and therefore can’t use it but it’s nice to have this feature if someone needs it.ConclusionToday, I gave a quick overview of the new features in C# 9.0. Nothing is too spectacular and the focus was clearly on increasing the developer productivity but I think especially records might be useful in the future.You can find the code of this demo on GitHub." }, { "title": "Upgrade a Microservice from .NET Core 3.1 to .NET 5.0", "url": "/upgrade-microservice-net-core-3-1-net-5-0/", "categories": "ASP.NET, Docker", "tags": "Azure DevOps, CI, Docker, xUnit, .NET 5", "date": "2020-11-11 00:00:00 +0100", "snippet": "Microsoft released the next major release, called .NET 5.0 which succeeds .NET Core 3.1. .NET 5.0 comes with a lot of improvements and also with C# 9. It is also the first step to a unified .NET platform and is the first version of Microsoft’s new release cycle. From now on, Microsoft will release a new version of .NET every November. .NET 6.0 will be released in November 2021, .NET 7.0 in November 2022, and so on.Today, I want to show how to upgrade a microservice and its build pipeline from .NET Core 3.1 to .NET 5.0. You can find the code of this demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”.System Requirements for .NET 5.0To use .NET 5.0 you have to install the .NET 5.0 SDK from the dotnet download page and Visual Studio 16.8 or later.Uprgrade from .NET Core 3.1 to .NET 5.0To upgrade your solution to .NET 5.0, you have to update the TargetFramework in every .csproj file of your solution. ReplacewithInstead of updating all project files and next year updating them again, I created a new file called common.props in the root folder of the solution. This file contains the following code:This file defines the C# version I am using and sets DefaultTargetFramework to net5.0. Additionally, I have a Directory.Build.props file with the following content:This file links the common.props file to the .csproj files. After setting this up, I can use this variable in my project files and can update with it all my projects with one change in a single file. Update the TargetFramework of all your .csproj files with the following code:After updating all project files, update all NuGet packages of your solution. You can do this by right-clicking your solution –&gt; Manage NuGet Packages for Solution… Update your NuGet packages That’s it. Your solution is updated to .NET 5.0. Build the solution to check that you have no errors. Build the solution Additionally, run all your tests to make sure your code still works. Run all unit tests Lastly, I update the path to the XML comments in the CustomerApi.csproj file with the following code:Update CI pipelineThere are no changes required in the CI pipeline because the solution is built in Docker. Therefore, you have to update the Dockerfile. Replace the following two lines:withThis tells Docker to use the new .NET 5.0 images to build and run the application. Additionally, you have to copy the previously created .props files into my Docker image with the following code inside the Dockerfile:Check in your changes and the build pipeline in Azure DevOps should run successfully. The .NET 5.0 build was successful ConclusionToday, I showed how easy it can be to upgrade .NET Core 3.1 to .NET 5.0. The upgrade was so easy because I kept my solution up to date and because microservices are small solutions that are way easier to upgrade than big monolithic applications. The whole upgrade for both my microservices took around 10 minutes. I know that a real-world microservice will have more code than mine but nevertheless, it is quite easy to update it. If you are coming from .NET Core 2.x or even .NET 4.x, the upgrade might be harder.You can find the code of this demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Run xUnit Tests inside Docker during an Azure DevOps CI Build", "url": "/run-xUnit-inside-docker-during-ci-build/", "categories": "DevOps, Docker", "tags": "Azure DevOps, CI, Docker, Unit Test, xUnit", "date": "2020-11-09 00:00:00 +0100", "snippet": "Running your build inside a Docker container has many advantages such as platform independence and better testability for developers. These containers also bring more complexity though.In my last post, I showed how to set up the build inside a Docker container. Unfortunately, I didn’t get any test results or code coverage after the build succeeded. Today, I want to show how to get the test results when you run tests inside Docker and how to display them in Azure DevOps.You can find the code of this demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”.Run Tests inside a Docker ContainerRunning unit tests inside a Docker container is more or less the same as building a project. First, I copy all my test projects inside the container using the COPY command:After copying, I execute dotnet restore on all test projects.Next, I set the label test to the build id. I will need this label later to identify the right layer of the container to copy the test results out of it. Then, I use dotnet test to run the tests in my three test projects. Additionally, I write the test result into the testresults folder and give them different names, e.g. test_results.trx.That’s already everything I have to change to run the tests inside the container and generate test results. You don’t have to split up every command as I did but I would recommend doing so. This helps you finding problems and also is better for the caching which will increase the build time of your container. You could also use a hard-coded value for the label but if you have multiple builds running at the same time, the pipeline may publish the wrong test results since all builds have the same label name.If you run the build, you will see the successful tests in the output of the build step. The tests ran inside the Docker container If you try to look at the Tests tab of the built-in Azure DevOps to see the test results, you won’t see the tab. The build was successful but not Test Results are showing The Tests tab is not displayed because Azure DevOps has no test results to display. Since I ran the tests inside the container, the results are also inside the container. To display them, I have to copy them out of the Docker container and publish them.Copy Test Results after you run Tests inside a Docker ContainerTo copy the test results out of the container, first I have to pass the build id to the dockerfile. To do that, add the following line to the Docker build task:Next, I use the following PowerShell task in the CI pipeline to create an intermediate container that contains the test results.Docker creates a new layer for every command in the Dockerfile. I can access the layer (also called intermediate container) through the label I set during the build. The script selects the first intermediate container with the label test=true and then copies the content of the testresults folder to the testresults folder of the WorkingDirectory of the build agent. Then the container is removed. Next, I can take this testresults folder and publish the test results inside it.Publish the Test ResultsTo publish the test results, I use the PublishTestResult task of Azure DevOps. I only have to provide the format of the results, what files contain results, and the path to the files. The YAML code looks as follows:Run the CI pipeline again and after it is finished, you will see the Tests tab on the summary page. Click on it and you will see that all tests ran successfully. Azure DevOps even gives you a trophy for that. The Tests tab is shown and you get even a trophy I did the same for the OrderApi project, except that I replaced CustomerApi with OrderApi in the Dockerfile.ConclusionDocker containers are awesome. They can be used to run your application anywhere but also to build your application. This enables you to take your build definition and run it in Azure DevOps, as GitHub actions, or in Jenkins. You don’t have to change anything because the logic is encapsulated inside the Dockerfile. This flexibility comes with some challenges, for example, displaying the test results of the unit tests. This post showed that it is pretty simple to get these results out of the container and display them in Azure DevOps.In my next post, I will show how you can also display the code coverage of your tests. You can find the code of this demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Build Docker in an Azure DevOps CI Pipeline", "url": "/build-docker-azure-devops-ci-pipeline/", "categories": "DevOps, Docker", "tags": "Azure DevOps, CI, DevOps, Docker, Docker Hub", "date": "2020-11-02 00:00:00 +0100", "snippet": "In my last post, I showed how to build a .NET Core Microservice with an Azure DevOps CI pipeline. Today, I want to build these microservices in another Azure DevOps CI pipeline and push the images to Docker Hub.This post is part of “Microservice Series - From Zero to Hero”.Set up a Service Connection to Docker HubBefore I create the new CI Pipeline for building the Docker image, I set up a connection to Docker Hub to push my image to its repository. To do that in Azure DevOps, click on Project Settings –&gt; Service connections –&gt; New service connection. Create a new service connection This opens a pop-up where you select Docker Registry. Select Docker Registry for your service connection On the next page, select Docker Hub as your Registry type, enter your Docker ID, and password, and set a name for your connection. Then click Verify and save. Configure the service connection Create a new Azure DevOps CI PipelineAfter setting up the service connection, create a new CI Pipeline. Select the source code location and then any template. After the yml file is created, delete its content. For more details on creating a Pipeline, see my post Run the CI Pipeline during a Pull Request. Create an empty Pipeline Configure the PipelineFirst, you have to set up some basic configuration for the pipeline. I will give it a name, and set a trigger to run the pipeline every time a commit is made to master and use an Ubuntu agent. Additionally, the build should only be triggered if changes to the CustomerApi folder are made. You can do this with the following code:In the next section, I set up the variables for my pipeline. Since this is a very simple example, I only need one for the image name. I define a name and set the tag to the build id using the built-in variable $(Build.BuildId). This increases the tag of my image automatically every time when a build runs.If you want better versioning of the Docker images, use one of the many extensions from the marketplace. In my projects, I use GitTools which you can find here.Build the Docker ImageNow that everything is set up, let’s add a task to build the image. Before you can do that, you have to add a stage and a job. You can use whatever name you want for your stage and job. For now, you only need one. It is good practice to use a meaningful name though.Inside the job, add a task for Docker. Inside this task add your previously created service connection, the location to the dockerfile, an image name, and the build context. As the command use “Build an Image”. Note that I use version 1 because version 2 was not working and resulted in an error I could not resolve.You can either add the YAML code from above or click on the Docker task on the right side. You can also easily edit a task by clicking Settings right above the task. This will open the task on the right side. Edit the Docker task Save the pipeline and run it. This should give you a green build.Push the Image to Docker HubThe last step is to push the image to a registry. For this example, I use Docker Hub because it is publicly available but you can also use a private one like Azure Container Registry (ACR) or even a private Docker Hub repository.Add the following code to your pipeline:Here I set a display name, the container registry “Container Registry” which means Docker Hub, and select my previously created service connection “Docker Hub”. The command indicates that I want to push an image and I set the image name from the previously created variable. This task only runs when the previous task was successful and when the build is not triggered by a pull request.The finished Azure DevOps CI PipelineThe finished pipeline looks as follows:You can also find the code of the CI pipeline on GitHub.Testing the Azure DevOps CI PipelineSave the pipeline and run it. The build should succeed and a new image should be pushed to Docker Hub. The pipeline ran successfully The pipeline ran successfully and if I go to my repository on Docker Hub, I should see a new image with the tag 307 there. The new image got pushed to Docker Hub As a practice, you could set up the CI pipeline for the OrderApi. The pipeline will look exactly the same, except that CustomerApi will be replaced with OrderApi. You can find the finished pipelines on GitHub inside the pipelines folder of each solution.ConclusionAn automated CI pipeline to build and push new images is an integral point of every DevOps process. This post showed that it is quite simple to automate everything and create a new image every time changes are pushed to the master branch.You can find the source code of this demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "We moved to Jekyll", "url": "/we-moved-to-jekyll/", "categories": "Miscellaneous", "tags": "Jekyll, Azure Static Web App, Docker, GitHub Action, WordPress", "date": "2020-10-26 00:00:00 +0100", "snippet": "When I started my blog, I was looking for something simple so I can write my posts and don’t have to manage much behind the scenes. WordPress was the most known blogging platform back then and I also got a cheap hosting package with WordPress included. Taking WordPress back then was a no-brainer but in the last three years since then, I was never really happy with it. Today, I want to talk about the reasons for the migration to Jekyll, what other options I considered, and what disadvantages the migration has.Why migrating away from WordPressWordPress is the most popular blogging framework and offers many plugins which can be easily installed. Additionally, it offers a nice admin backend where you can manage everything from comments, posts, or Google Analytics. I have never used WordPress before but with the marketplace, Google search, and some try and error, I had the first version of my blog running within a couple of hours. A couple of months later, I changed the theme and made some CSS changes. This was when the “problems” started.WordPress is based on PHP which means that you can edit the whole code and create submodules of it so you can safely upgrade themes. I am not a PHP fan, therefore I never wanted to get into it to change the small details that I didn’t like. Over time I had some layout imperfections like a horizontal scrolling bar although there was nothing to scroll to. I couldn’t figure out where it came from (I guess from one of the plugins) but as I said, I didn’t want to get into PHP.The second and way bigger reason why I decided to move away from WordPress is its performance. I am really fortunate that I grew a reader base from all over the world over the last three years. This leads to performance problems for the users though. My WordPress instance is running on a server in central Europ which leads to quite some latency for Asian or American readers. Additionally, due to the load WordPress (maybe also the server) took too long to process the request.Migrating away but where to?Over the last years, I was thinking a couple of times to migrate to a different system. My first idea was to write it myself with ASP.NET Core (inspired by Scott Hanselman) but I discarded the idea quickly because it would be quite some work and I didn’t have any good and affordable hosting options. Most cheap hosting providers only offer support for WordPress or other PHP frameworks but barely support for .NET Core.The next idea was to migrate to Azure App Service to have a better insight into what’s going on and the possibility to replicate it to several locations. This wasn’t an option due to the pricing. Azure App Service is good when you need computing power but with a blog website, you don’t have any computing except loading some text. This would be total overkill and one instance would cost around 70$ a month (most of it was the MySQL DB). 70$ a month is not an option, especially compared to the 35$ a year I am paying with my existing WordPress instance.In May this year, Microsoft announced Azure Static Web Apps which offer free hosting for Javascript frameworks. I took this as an opportunity to look into React (React because I didn’t like Angular 2) but I still don’t like Javascript and writing everything myself in React was not appealing for me.Microsoft improved Azure Static Web Apps and With all the options above ruled out, I decided to go with Jekyll and Azure Static Web App.Why use Jekyll and Azure Static Web App?The biggest advantage of Jekyll is that it supports HTML, Javascript, and CSS and generates static sites. This means that it is blazing fast since there are no computing or database lookups needed anymore. The second big advantage is that it is not PHP and so I can easily change everything to my liking. The next advantage is the simple integration with Azure Static App. The deployment gets created, more or less, automatically and I have full control over the whole process.I am fortunate that I have readers from all over the world. So far, they all had to connect to a server in Europe. With Azure Static Web App, my blog gets replicated on five servers the US, Europe, and Asia which reduces the latency for none Europeans. I only have Google Analytics data for a couple of days but so far the average page load time went down by 52%. The further the user is away, the bigger the performance increase is. For example, the page load time for US-based users decreased by 76%.How to migrate from WordPress to JekyllMigrating from WordPress to Jekyll is easy, in theory. I used the Jekyll Exporter plugin to export my posts and pictures. The problem is, that the plugin only works when there are no other plugins active. I am using around 10 plugins and deactivating them took around 30 min and gave me many 404 and 500 error pages. It was a bit annoying but also made me happy to leave WordPress behind. Once I had my posts and pictures, I was surprised how well the export was. All posts got converted to markdown and could already be used. I edited mainly the images so they don’t use the WordPress CSS classes anymore and look nice on mobile and desktop and removed some boiler-plate text. It took me a couple of hours and some regex to get everything cleaned up.Docker is awesomeThis migration reminded me of how awesome Docker is and how it can improve the efficiency of developers. The theme I am using needs besides Ruby some other Linux programs I have never heard about and also which I didn’t want to install on my Windows 10 WSL. Instead of going through the kinda complicated build and run process, I use Docker to run and build the whole website. The only downside of this approach is that I have to build the website before checking in. In the near future, I want to extend the GitHub Action so it uses Docker to build my website before the deployment.New FeaturesI tried to change as little as possible but the theme I am using has some nice eye candy features. The biggest improvement is the built-in dark mode which enables users to switch between a light and dark theme. Another nice feature is the listing of posts inside a category including their publish dates. Display all posts of a category The last feature I really like is the table of content on the right side which shows you automatically where you are at the moment and also lets you create links to headlines. Table of contents in Jekyll Problems with JekyllTo be honest, I had no major problem. The biggest problems were probably some CSS changes to make something look nice on mobile and desktop. Another inconvenience was that Azure Static Web App doesn’t support root domain (programmingwithwolfgang.com) yet. The documentation suggests to use Cloudflare and configure a redirect from programmingwithwolfgang.com to wwww.programmingwithwolfgang.com. It should work but I didn’t want to add Cloudflare yet.Fortunately, my current hoster has a setting to redirect non-www requests to www.* and since my package is running for another 11 months, I am using this for now. The Azure Static Web App team already said that the support for root domains should be implemented soon.ConslusionMoving from WordPress to Jekyll was surprisingly easy and so far, I don’t regret it at all. Jekyll gives me the ability to change every detail of my website without going into WordPress and PHP and also allows me to run the site locally using Docker which helps a lot with productivity when trying new features. The best part of moving away from WordPress to Jekyll on Azure Static Web App is that the website is way faster and also replicated all over the world which should increase the user experience significantly.You can find the code of my website on GitHub." }, { "title": "Run the CI Pipeline during a Pull Request", "url": "/run-the-ci-pipeline-during-pull-request/", "categories": "DevOps", "tags": "Azure DevOps, CI, Continuous Integration, DevOps Pull request policy", "date": "2020-10-17 16:34:44 +0200", "snippet": "In modern DevOps culture, the goal is to get features as fast as possible into production. Additionally, we have to guarantee that these new features don’t break anything. To do that, I will show in this post how to protect the master branch with a policy that enforces a pull request (PR) and reviews. To further enhance the quality, I will show how to run the CI pipeline from my last post, which builds the solutions and runs all unit tests.This post is part of “Microservice Series - From Zero to Hero”.Protect the Master Branch with a Pull Request PolicyTo create a new policy go to Project settings  –&gt; Repositories –&gt; Policies –&gt; Branch policies and there click the + button. Create a new Branch Policy This opens a fly-out where you can select either of the two options. I select “Protect current and future branches matching a specified pattern” and enter master as the branch name. This means that this policy is only valid for the master branch. Then click Create. Add a Pull Request Policy for the master branch This opens the Branch Policies menu where you can configure your pull request.Configure the Branch PolicyFirst, I require one reviewer, allow the requestor to approve their changes, and reset the vote every time new changes are committed. Usually, I don’t allow the requestor to approve their changes but since I am alone in this demo project I allow it. Microsoft recommends that two reviewers should check the pull request for the highest quality. Configure the minimum number of reviewers Next, I require every pull request to be linked with a work item. There should never be code changes without a PBI or Bug ticket describing the desired changes. Therefore, this is required. Check for linked work items Reviewers provide their feedback with comments, therefore, I require all comments to be resolved before a pull request can be completed. In my projects, always the creator of the comment resolves the comment and not the creator of the PR. Configure comment resolution Companies have different merge strategies. Some use squash merges, some do rebase and some do just basic merges. It is good to have a merge strategy and limit the pull request to your strategy. In my projects, I do squash merges because I don’t care about all the commits during the development. I only care about the finished feature commit, therefore, I only allow squash merges. Limit the merge types Configure automatic BuildsNow we come to the most interesting part of the policy. I add a build policy and select the previously created CustomerApi CI pipeline.  You can find the post here. I set /CustomerApi/* as the path filter. The automatic trigger starts the build every time changes are committed inside the CustomerApi folder and the build expires after 12 hours. This means if the pull request is not completed within 12 hours, the build has to be triggered again. Add a build policy for the CustomerApi to the Pull Request Add another build policy for the OrderApi and enter /OrderApi/* as the path filter. Click on Save and the policy is configured and created.Make changes to the CodeI added a new unit test, commit the changes to the master branch, and push the changes. Due to the policy on the master branch, I am not allowed to push changes directly to the master branch, as seen on the following screenshot. Pushing directly to master is not allowed Since the master branch is protected, I have to create a feature branch. I name this branch addunittest and push the changes to Azure DevOps. Pushing a new branch In Azure DevOps under Repos –&gt; Files, you can see that Azure DevOps registered the changes and already suggest to create a new PR. Click on Create a pull request and you will get into a new window. Create a new Pull Request Create a new Pull RequestAdd a title, and optionally a description, reviewers, and work items. I like to have 1-3 sentences in the description to explain what you did. As for the title, I usually use the PBI number and the PBI title (not in this example though). New Pull Request After the pull request is created, the build will kick off immediately, and also all other required policies will be checked. As you can see on the following screenshot, the build failed due to a failing test, no work item is linked and not all comments are resolved. Therefore, I can’t complete the PR. Required checks failed I fixed my unit test, added a link to the PB, and fixed the suggested changes from the comment. The comment creator resolved the comment and this enabled me to complete the pull request. On the following screenshot, you can see that I also changed the title of the PR to have to PBI number and title in it. All checks passed Complete the Pull RequestWhen you click on Complete, you can select a merge type. Since I restricted the merge strategy to squash commit only, I can’t select any other strategy. Only Squash commit is allowed by the Pull Request policy The definition of done in my projects is that the PBI is set to done when the pull request is finished (because we deploy the feature automatically to prod when the PR is completed). Additionally, I select to delete my branch after merging. Complete the Pull Request The PR creator can also select auto-complete to complete the pull request automatically when all required checks are OK. After the merge to master is completed, the CI pipeline automatically kicks off a build of the master branch. The master branch trigger a CI build ConclusionIn this post, I explained how to protect the master branch from changes in Azure DevOps. I showed how to add a branch policy to the master branch in Azure DevOps and also how to run a build process to check if the solution compiles and if all tests run successfully.You can find the code of this demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Build .NET Core in a CI Pipeline in Azure DevOps", "url": "/build-net-core-in-ci-pipeline-in-azure-devops/", "categories": "DevOps", "tags": ".NET Core, Azure DevOps, CI, Continuous Integration, DevOps, GitHub", "date": "2020-10-13 17:20:53 +0200", "snippet": "A crucial feature of DevOps is to give the developer fast feedback if their code changes work. This can be done by automatically building code and running tests every time changes are checked-in. Today, I will show how to create a CI pipeline (continuous integration) for ASP .NET Core.This post is part of “Microservice Series - From Zero to Hero”.Create a .NET Core CI Pipeline in Azure DevOpsOver the series of my last posts, I created two ASP .NET Core microservices. In this post, I create a CI pipeline to build all projects and run all unit tests in the repository. You can find the code of the demo on GitHub.In your Azure DevOps project, go to Pipelines and click Create Pipeline. Create a new CI Pipeline In the next window, select where you have your code stored. I select GitHub for this Demo. Usually, I have my code directly in Azure DevOps, then I would select Azure Repos Git. On the bottom, you can see “Use the classic editor”. This opens the old task-based editor. You shouldn’t use this anymore since the new standard is to use YML pipelines. This enables you to have your pipeline in your source control. Select where your code is Authorize GitHubSince the code is on GitHub, I have to authorize Azure Pipelines to access my repositories. If the code was on an Azure Repos, this step wouldn’t be necessary. Authorize Azure Pipelines to access GitHub After authorizing Azure Pipelines for GitHub, all your repositories will be displayed. Search and select the repository, you want to make the CI pipeline for. In my case, I select the MicroserviceDemo repository. Find your repository On the next window, I have to approve to install Azure Pipelines in my GitHub repository. This allows Azure DevOps to access the repository and write to the code. This is necessary because the CI pipeline will be added to the source control. Again, this is not necessary when you have your code in Azure DevOps. Approve the access to the GitHub repository Select a TemplateOn the next step, select a template for your CI pipeline. Azure DevOps offers many templates like Docker, Kubernetes, PHP, or Node.js. Since my application is a .NET Core microservice, I select the ASP.NET Core template. Select the ASP.NET Core template for your CI Pipeline That’s it. The template created a simple CI pipeline and you can use it to build your .NET Core solution. In the next section, I will go into more detail about the functionality and add more steps. The CI Pipeline got created from the template Add more steps to the CI PipelineBefore we add more steps to the CI pipeline, let’s have a look at what the template created.Analyze the CI Pipeline from the TemplateAbove the .yml editor, you can see the path to the pipeline yml file in your source control. I recommend having a pipelines folder in the root folder of your solution. In my case, it is WolfgangOfner/MicroserviceDemo/CustomerApi/pipelines/NetCore-CI-azure-pipeline.yml (and also one for the OrderApi but the principle is the same). It is a good practice to name the file to describe what they do. You can find the finished pipeline on GitHub or a bit further down.Lines 1 through 14 configure the pipeline. The trigger section defines that the pipeline is automatically triggered when something changes on the master branch inside the CustomerApi folder. This means that this pipeline does not run when you change something on the OrderApi project. The pool section defines that the pipeline is executed on an ubuntu agent and the variables section lets you define variables for the pipeline. By default, only the buildConfiguration is set to Release.Line 16 starts the first build step that executes a dotnet build in sets the configuration to Release by using the previously defined buildConfiguration variable. Additionally, a display name is set to identify the step easier in the logs.The .yml editor can be a pain and overwhelming, especially when you are starting with pipelines. Once you are used to it, it is way better than the old one though.Add more steps to the CI PipelineI plan to add several new steps to restore NuGet packages, build the solution, run tests, and publish the solution. The publish step should only be run when the pipeline was triggered by the master branch.I remove the build script and select the .NET Core task on the right side. I select restore as command and **/CustomerApi*.csproj as the path to the projects to restore each project starting with CustomerApi. Then click Add, to add the task to your pipeline. Make sure that your cursor is at the beginning of the next line under steps. Add NuGet restore to the CI pipeline I repeat the process of adding new .NET Core tasks but I use build to build all projects, test to run all projects that have Test at the end of the project name, and then publish the CustomerApi project. Since I already built and restored the test projects, you can see that I use the –no-restore and –no-build arguments for them. The whole pipeline looks as follows:Click “Save and Run” and the pipeline will be added to your source control and then executed. The CI Pipeline is running After the build is finished, you see a summary and that all 38 tests passed. All unit tests passed You don’t see anything under Code Coverage. I will cover this in a later post.Run Tasks only when the Master Branch triggered the buildCurrently, the publish task runs always, even if I don’t want to create a release. It would be more efficient to run this task only when the build was triggered by the master branch. To do that, I add a custom condition to the publish task. I want to run the publish only when the previous steps succeeded and when the build was not triggered by a pull request. I do this with the following code:Adding a second pipeline for the OrderApi works exactly the same except that instead of CustomerApi, you use OrderApi.ConclusionCI pipelines help developers to get fast feedback about their code changes. These pipelines can build code and run tests every time something changed. In my next post, I will show how to add code coverage to the results to get even more information about the code changes, and then I will show how to run your code with every pull request. Later, I will extend the pipeline to build and create Docker images and also deploy them to Kubernetes.You can find the code of the demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Azure Static Web App with Blazor", "url": "/azure-static-web-app-with-blazor/", "categories": "Cloud, DevOps", "tags": ".NET Core, Azure Function, Azure Static Web App, Blazor, C#", "date": "2020-10-04 16:51:05 +0200", "snippet": "Last week at Ignite Microsoft announced that the preview of Azure Static Web App now also supports Blazor WebAssembly. In this post, I will show how to create a Blazor client-side (WebAssembly) application and an Azure Function to retrieve some data. Then Azure Static Web App will automatically deploy the Blazor app and the Function.You can find the code of the demo on GitHub.Create a Blazor WebAssembly ApplicationTo create a Blazor WebAssembly application, you need Visual Studio 2019 and the .NET Core SDK 3.1.300 or later.In Visual Studio create a new project and select Blazor App as your template. Select Blazor App as the template Enter a name and then select Blazor WebAssembly App to create a client-side Blazor application. Create a Blazor WebAssembly application Click Create and Visual Studio creates a sample Blazor WebAssembly application.Create an Azure Function in Visual StudioTo create an Azure Function, you need the Azure Functions Core Tools.In your previously created solution, right-click on the solution file, click add a new project, and select Azure Functions as the template. Select Azure Function as the template Enter a name and then select Http trigger to trigger the function via HTTP calls. Select HTTP Trigger for the Azure Function Get Data from the Azure FunctionTo call the Azure Function and get some data about products, I edit the already existing FetchData page. I change the code block to call the Azure Function and then cast the result into a product array. Then, I will loop over the array in the HTML code and display all elements in a table.In the Function, I create a new list with three products and return it. Note that the FunctionName, “Product”, is the same as in the call in the code block.The Product class is in a class library so both projects can reference it.To test that everything is working as expected, start the Blazor app and the Azure Function. Click on the Fetch data and you will see three products displayed. In the console application of the Azure Function, you can see its URL and some log information. To test only the Azure Function, you could also call the displayed URL. This will return the products as JSON. Test the implementation locally Create an Azure Static Web AppGo to the Azure Portal, click on New, search for Static Webb App, and click on Create. Create the Static Web App Enter a name, location, and your GitHub repository. Then select Blazor as Build Presets and enter the location of your Blazor project and of your Azure Function project. Click Review + create and your application will be deployed. Configure the Static Web App The deployment takes only a moment. After it is finished, click on the URL that is displayed in your Static Web App. The Static Web App got deployed This opens your application in a new window and when you click on Fetch data, you will see your products. Testing the deployed Blazor application Below the URL, you see a link to the GitHub Actions which are responsible for the deployment. Alternatively, you could also open your GitHub repository and click on Actions there. When you open the GitHub Actions, you will see the deployment from the Azure Static Web App. GitHub Actions deployed the Blazor application ConclusionI already wrote an article about Azure Static Web Apps when they got announced in May. Back then I had some problems and the functionality was very limited. With these improvements, especially with the support of Blazor, Microsoft is going definitively in the right direction. To have a realistic application, the Azure Function could get some data from a database. Another great benefit is that it is still free and you only pay for the Azure Functions (the first 400,000 seconds are free).You can find the code of the demo on GitHub." }, { "title": "Run Tests inside Docker during CI", "url": "/run-tests-inside-docker-during-ci/", "categories": "DevOps, Docker", "tags": "Azure DevOps, CI, Docker, Unit Test, xUnit", "date": "2020-09-23 09:09:22 +0200", "snippet": "Running your build inside a Docker container has many advantages like platform independence and better testability for developers. These containers also bring more complexity though. In my last post, I showed how to set up the build inside a Docker container. Unfortunately, I didn’t get any test results or code coverage after the build succeeded. Today, I want to show how to get the test results when you run tests inside Docker and how to display them in Azure DevOps.You can find the code of this demo on GitHub.Run Tests inside DockerRunning unit tests inside a Docker container is more or less as building a project. First, I copy all my test projects inside the container using the COPY command:Next, I set the label test to true. I will need this label later to identify the right layer of the container to copy the test results out of it. Then, I use dotnet test to run the tests in my three test projects. Additionally, I write the test result into the testresults folder and give them different names, e.g. test_results.trx.Thats already everything I have to change to run the tests inside the container and generate test results. If you run the build, you will see the successful tests in the output of the build step.The tests ran inside the Docker container. If you try to look at the Tests tab of the built-in Azure DevOps to see the test results, you won’t see the tab. The build was successful but not Test Results are showing The Tests tab is not displayed because Azure DevOps has no test results to display. Since I ran the tests inside the container, the results are also inside the container. To display them, I have to copy them out of the docker container and publish them.Copy Test Results after you run Tests inside DockerTo copy the test results out of the container, I use the following PowerShell task in the CI pipeline.Docker creates a new layer for every command in the Dockerfile. I can access the layer (also called intermediate container) through the label I set during the build. The script selects the first intermediate container with the label test=true and then copies the content of the testresults folder to the testresults folder of the WorkingDirectory of the build agent. Then the container is removed. Next, I can take this testresults folder and publish the test results inside it.Publish the Test ResultsTp publish the test results, I use the PublishTestResult task of Azure DevOps. I only have to provide the format of the results, what files contain results and the path to the files. The YAML code looks as follows:Run the CI pipeline again and after it is finished, you will see the Tests tab on the summary page. Click on it and you will see that all tests ran successfully. Azure DevOps even gives you a trophy for that :D. The Tests tab is shown and you get even a trophy Note if you are following the whole series: I moved the projects from the solution folder to the root folder. In the past couple of months I made the experience that this makes it easier with docker.ConclusionDocker containers are awesome. They can be used to run your application anywhere but also to build your application. This enables you to take your build definition and run it in Azure DevOps, or GitHub actions or in Jenkins. You don’t have to change anything because the logic is encapsulated inside the Dockerfile. This flexibility comes with some challenges, for example, to display the test results of the unit tests. This post showed that it is pretty simple to get these results out of the container and display in Azure DevOps.In my next post, I will show how you can also display the code coverage of your tests. You can find the code of this demo on GitHub." }, { "title": "Create a Docker Image in an Azure DevOps CI Pipeline", "url": "/create-a-docker-image-in-an-azure-devops-ci-pipeline/", "categories": "DevOps, Docker", "tags": "Azure DevOps, CI, DevOps, Docker, Docker Hub", "date": "2020-09-14 00:00:00 +0200", "snippet": "In my last post, I showed how to build a .NET Core Microservice inside a Docker container. Today, I want to build this microservice in an Azure DevOps CI pipeline and push the image to Docker Hub.Set up a Service Connection to Docker HubBefore I create the new CI Pipeline for building the Docker image, I set up a connection to Docker Hub to push my image to its repository. To do that in Azure DevOps, click on Project Settings –&gt; Service connections –&gt; New service connection. Create a new service connection This opens a pop-up where you select Docker Registry. Select Docker Registry for your service connection On the next page, select Docker Hub as your Registry type, enter your Docker ID, password and set a name for your connection. Then click Verify and save. Configure the service connection Create a new Azure DevOps CI PipelineAfter setting up the service connection, create a new CI Pipeline. Select the source code location and then any template. After the yml file is created, delete its content. For more details on creating a Pipeline, see my post “Build .NET Core in a CI Pipeline in Azure DevOps”. Create an empty Pipeline Configure the PipelineFirst, you have to set up some basic configuration for the pipeline. I will give it a name, set a trigger to run the pipeline every time a commit is made to master and use an Ubuntu agent. You can do this with the following code:In the next section, I set up the variables for my pipeline. Since this is a very simple example, I only need one for the image name. I define a name and set the tag to the build id using the built-in variable $(Build.BuildId). This increases the tag of my image automatically every time when a build runs.If you want better versioning of the Docker images, use one of the many extensions from the marketplace. In my projects, we use one of our own plugins which you can find here.Build the Docker ImageNow that everything is set up, let’s add a task to build the image. Before you can do that, you have to add a stage and a job. You can use whatever name you want for your stage and job. For now, you only need one. It is good practice to use a meaningful name though.Inside the job, add a task for Docker. Inside this task add your previously created service connection, the location to the dockerfile, an image name, and the build context. As the command use Build an Image. Note that I use version 1 because version 2 was not working and resulted in an error I could not resolve.You can either add the YAML code from above or click on the Docker task on the right side. You can also easily edit a task by clicking Settings right above the task. This will open the task on the right side. Edit the Docker task Save the pipeline and run it. This should give you a green build.Push the Image to Docker HubThe last step is to push the image to a registry. For this example, I use Docker Hub because it is publicly available but you can also use a private one like Azure Container Registry (ACR) or even a private Docker Hub repository.Add the following code to your pipeline:Here I set a display name, the container registry “Container Registry” which means Docker Hub, and select my previously created service connection “Docker Hub”. The command indicates that I want to push an image and I set the image name from the previously created variable. This task only runs when the previous task was successful and when the build is not triggered by a pull request.The finished Azure DevOps CI PipelineThe finished pipeline looks as follows:You can also find the code of the CI pipeline on GitHub.Testing the Azure DevOps CI PipelineSave the pipeline and run it. The build should succeed and a new image should be pushed to Docker Hub. The pipeline ran successfully The pipeline ran successfully and if I go to my repository on Docker Hub, I should see a new image with the tag 232 there. The new image got pushed to Docker Hub ConclusionAn automated CI pipeline to build and push new images is an integral point of every DevOps process. This post showed that it is quite simple to automate everything and create a new image every time changes are pushed to the master branch.You can find the source code of this demo on GitHub." }, { "title": "Add Docker to an ASP .NET Core Microservice", "url": "/add-docker-to-a-asp-net-core-microservice/", "categories": "Docker", "tags": ".NET Core 3.1, Docker, Microservice", "date": "2020-08-17 18:31:42 +0200", "snippet": "Microservices need to be deployed often and fast. To achieve this, they often run inside a Docker container. In this post, I will show how easy it is to add Docker support to a project using Visual Studio.What is Docker?Docker is the most popular container technology. It is written in Go and open-source. A container can contain a Windows or Linux application and will run the same, no matter where you start it. This means it runs the same way during development, on the testing environment, and on the production environment. This eliminates the famous “It works on my machine”.Another big advantage is that Docker containers share the host system kernel. This makes them way smaller than a virtual machine and enables them to start within seconds or even less. For more information about Docker, check out Docker.com. There you can also download Docker Desktop which you will need to run Docker container on your machine.For more information, see my post “Dockerize an ASP .NET Core Microservice and RabbitMQ”Add Docker Support to the MicroserviceYou can find the code of the demo on GitHub.Open the solution with Visual Studio 2019 and right-click on the CustomerApi project. Then click on Add and select Docker Support… Add Docker Support to the API project This opens a window where you can select the operating system for your project. If you don’t have a requirement to use Windows, I would always use Linux since it is smaller and therefore faster. Select the operating system for the Docker container After you clicked OK, a Dockerfile was added to the project and in Visual Studio you can see that you can start the project with Docker now. Start the project in Docker Click F5 to start the application and your browser will open. To prove that this application runs inside a Docker container, check what containers are running. You can do this with the following command:On the following screenshot, you can see that I have one container running with the name CustomerApi and it runs on port 32770. This is the same port as the browser opened. The microservice is running inside a Docker container The Dockerfile explainedVisual Studio generates a so-called multi-stage Dockerfile. This means that several images are used to keep the output image as small as possible. The first line in the Dockerfile uses the ASP .NET Core 3.1 runtime and names it base. Additionally, the ports 80 and 443 are exposed so we can access the container with HTTP and HTTPs later.The next section uses the .NET Core 3.1 SDK to build the project. This image is only used for the build and won’t be present in the output container. As a result, the container will be smaller and therefore will start faster. Additionally the projects are copied into the container.Next, I restore the NuGet packages of the CustomerApi and then build the CustomerApi project.The last part of the Dockerfile publishes the CustomerApi project. The last line sets the entrypoint as a dotnet application and that the CustomerApi.dll should be run.ConclusionIn today’s DevOps culture it is necessary to change applications fast and often. Additionally, microservices should run inside a container whereas Docker is the defacto standard container. This post showed how easy it is to add Docker to a microservice in Visual Studio.You can find the code of this demo on GitHub." }, { "title": "Create a .NET Core CI Pipeline in Azure DevOps", "url": "/create-net-core-in-ci-pipeline-in-azure-devops/", "categories": "DevOps", "tags": ".NET Core, Azure DevOps, CI, Continuous Integration, DevOps", "date": "2020-08-03 00:00:00 +0200", "snippet": "A crucial feature of DevOps is to give the developer fast feedback if their code changes work. This can be done by automatically building code and running tests every time changes are checked-in. Today, I will show how to create a CI pipeline (continuous integration) for ASP .NET Core.Create a .NET Core CI Pipeline in Azure DevOpsIn my last post, Creating a Microservice with .NET Core 3.1, I created a new ASP .NET Core microservice. I will use the CI pipeline to build all projects and run all unit tests in the repository. You can find the code of the demo on GitHub.In your Azure DevOps project, go to Pipelines and click Create Pipeline. Create a new CI Pipeline In the next window, select where you have your code stored. I select GitHub for this Demo. Usually, I have my code directly in Azure DevOps, then I would select Azure Repos Git. On the bottom, you can see “Use the classic editor”. This opens the old task-based editor. You shouldn’t use this anymore since the new standard is to use YML pipelines. This enables you to have your pipeline in your source control. Select where your code is Authorize GitHubSince the code is on GitHub, I have to authorize Azure Pipelines to access my repositories. If the code was on an Azure Repos, this step wouldn’t be necessary. Authorize Azure Pipelines to access GitHub After authorizing Azure Pipelines for GitHub, all your repositories will be displayed. Search and select for the repository, you want to make the CI pipeline for. In my case, I select the .NETCoreMicroserviceCiCdAks repository. Select your repository On the next window, I have to approve to install Azure Pipelines in my GitHub repository. This allows Azure DevOps to access the repository and write to the code. This is necessary because the CI pipeline will be added to the source control. Again, this is not necessary when you have your code in Azure DevOps.Select a TemplateOn the next step, select a template for your CI pipeline. Azure DevOps offers many templates like Docker, Kubernetes, PHP, or Node.js. Since my application is a .NET Core microservice, I select the ASP.NET Core template. Select the ASP.NET Core template for your CI Pipeline That’s it. The template created a simple CI pipeline and you can use it to build your .NET Core solution. In the next section, I will go into more detail about the functionality and add more steps. The template created a simple CI Pipeline Add more steps to the CI PipelineBefore we add more steps to the CI pipeline, let’s have a look at what the template created.Analyze the CI Pipeline from the TemplateAbove the .yml editor, you can see the path to the pipeline yml file in your source control. In my case it is WolfgangOfner/.NETCoreMicroserviceCiCdAks/dotnetcoreCIPipeline.yml. I renamed the file because I want to add more later.Line 1 through 8 configures the pipeline. The trigger section defines that the pipeline is automatically triggered when something changes on the master branch. The pool section defines that the pipeline is executed on an ubuntu agent and the variables section lets you define variables for the pipeline. By default, only the buildConfiguration is set to Release.On line 10 starts the first build step that executes a dotnet build in sets the configuration to Release by using the previously defined buildConfiguration variable. Additionally, a display name is set to identify the step easier in the logs.The .yml editor can be a pain and overwhelming especially when you are starting with pipelines. Once you are used to it, it is way better than the old editor.Add more steps to the CI PipelineI plan to add several new steps to restore NuGet packages, build the solution, run tests, and publish the solution. The publish step should only be run when the pipeline was triggered by the master branch.I remove the build script and select the .NET Core task on the right side. I select restore as command and **/*.csproj as the path to the projects. This will restore all available projects. Then click Add, to add the task to your pipeline. Make sure that your cursor is at the beginning of the next line under steps.  Add a NuGet restore step to the CI Pipeline I repeat the process of adding new .NET Core tasks but I use build to build all projects, test to run all projects that have Test at the end of the project name, and then publish the CustomerApi project. The whole pipeline looks as follows:Click Save and Run and and the pipeline will be added to your source control and then executed.I created a new branch to test if everything is fine. Running the CI Pipeline After the build is finished, you see a summary and that all 52 tests passed. All unit tests passed You don’t see anything under Code Coverage. I will cover this in a later post.Run Tasks only when the Master Branch triggered the buildCurrently, the publish task runs always, even if I don’t want to create a release. It would be more efficient to run this task only when the build was triggered by the master branch. To do that, I add a custom condition to the publish task. I want to run the publish only when the previous steps succeeded and when the branch name is master. I do this with the following code:Save the pipeline and run it with any branch but the master branch. You will see that the publish task is skipped. The Publish Task got skipped ConclusionCI pipelines help developers to get fast feedback about their code changes. These pipelines can build code and run tests every time something changed. In my next post, I will show how to add code coverage to the results to get even more information about the code changes, and then I will show how to run your code with every pull request. Later, I will extend the pipeline to build and create Docker images and also to deploy them to Kubernetes.You can find the code of the demo on GitHub." }, { "title": "Create a .NET Core Visual Studio Template", "url": "/create-a-net-core-visual-studio-template/", "categories": "Programming, Miscellaneous", "tags": ".NET Core 3.1, C#, Docker, Visual Studio", "date": "2020-07-27 00:00:00 +0200", "snippet": "Over the last couple of weeks, I created several microservices with the same project structure. Setting up a new solution was quite repetitive, time-consuming, and no to be honest no fun. Since my solutions have the same structure, I created a Visual Studio template that sets up everything for me. Today, I want to show you how to create your Visual Studio template with ASP .NET MVC with .NET Core 3.1 and Docker.Creating a Visual Studio TemplateI plan to create a skeleton for my solution and when creating a new project with my template to take the user input as the name for the projects and the entities. For example, if the user names the solution Customer, I will create a Customer.Api, Customer.Service and Customer.Data project and also a Customer entity. You can find the code of the demo on GitHub. Note that there are two branches.Creating the Structure of the Project for the Visual Studio TemplateThe first step is to create the structure of your Visual Studio template. Therefore, I created an empty C# solution and add three projects, Template.Api, Template.Service and Template.Data. For the template, you can install any NuGet you want or add code. I added a controller that calls a service that calls a repository to get some data. Visual Studio Template Project Structure Additionally, I added Swagger to have a UI to test the Api method. When you are satisfied with your project, run it. You should see the Swagger UI now. The Swagger UI for the template As the last step, I test the Get method to verify that everything works correctly. Testing the Template Api The template is set up and as the next step, I am going to export it and install it in Visual Studio.Export the TemplateTo export the Visual Studio Template, click on Project and then Export Template. Export the Visual Studio Template This opens a new window in which you can select what template you want to export and which project. Leave it as Project template, select the Template.Api project and click on Next. Choose which Project to export On the last page of the export, uncheck both checkboxes and click Finish. Finish the Export Repeat this export for all other projects in your solution. After you are finished, you can find the exported files under C:\\Users&lt;YourUserName&gt;\\Documents\\Visual Studio \\My Exported Templates. The exported .zip files Unzip every zip file into a separate folder and delete the zip files. I get quite often a warning that the file header is corrupt during the unzip. You can ignore this message though. Next, create a file with a vstemplate ending, for example, Template.vstemplate in the folder where you unzipped your templates. This file contains links to all projects in the template in the XML format. Copy the following code into the file:Save the file and create a zip of the three folder and the template file. You can easily do this by highlighting everything and the right-click and then select Send to –&gt; Compressed (zipped) folder. Your folder should contain the following files and directories now: Content of the template folder after finishing the creation of the template Install the Visual Studio TemplateTo install the template, all you have to do is to copy the Template.zip file into the following folder: C:\\Users&lt;YourNameUser&gt;\\Documents\\Visual Studio \\Templates\\ProjectTemplates\\Visual C#. Alternatively, you could copy the file into the ProjectTemplates folder.Testing the new TemplateOpen Visual Studio and select Create a new project. Search for My Template and the previously added template will be displayed. Select your template in Visual Studio Create the project and you will see the same structure as in the template. When you start the project, you will see the Swagger UI and can test the Api methods.Make the Template flexible with VariablesHaving a template is nice but it would be even nicer if the projects weren’t named Template and if they would take the name I provide. To achieve this, Visual Studio offers template parameters. You can find all available parameters here. The goal is to name the projects and files accordingly to the user input during the creation of the solution. To do this, I use the variable safeprojectname. This variable gives you the name of the current project. To get the name of the solution prefix it with ext_, therefore I will use ext_safeprojectname. All variables have a Dollar sign at the beginning and the end.Replace Class Names and Namespaces with VariablesI am replacing in all files Template with $ext_safeprojectname$, for example, the TemplateService class:Adding the variable also adds a lot of errors in your solution. You can ignore them though. Errors after adding the template variables Replace File Names with VariablesNot only class names and namespaces should have the provided name, but also the classes should be named accordingly. You can do the same as before and replace Template in all file names with $ext_safeprojectname$. You don’t have to change the project name though. Replace the file names with variables Export the TemplateRepeat the export from before by clicking on Project –&gt; Export Template and export all your projects. Delete your previously created folders and unzipp the exported zip files. In the vstemplate file, replace Template with $safeprojectname$. This will rename the project files. Also make sure that CopyParameters=”true” is set for every project. Otherwise, the user input won’t be copied and the variables will be empty.Zip all files and copy the zip over the previously created zip in the Visual C# folder. Create a new project and select your template and enter Customer as project name. If you did everything right, all files and projects should be named correctly and the project should build without an error. It is very easy to have errors on the first try since you don’t have any help to find errors in the template and every typo will result in a build error. All files got renamed correctly When you don’t have any error, run the project and you should see Customer in the headline, description and Controller. Testing the created solution Add Docker Support to the Visual Studio TemplateAdding Docker support to your Visual Studio Template is very simple. Right-click on your Template.Api project and select Add –&gt; Docker Support. Select an operating system and click OK. This adds a Dockerfile and that’s it already.Use Variables in the DockerfileSince we use variables everywhere, we have to use variables also in the Dockerfile because otherwise the projects wouldn’t be found since their name will change when the solution is created. Replace every Template with $ext_safeprojectname$ in the Dockerfile. When you are done export the project again. This time you only have to export the Api project.After adding the new template to Visual Studio, create a new project and you will see the Dockerfile in your solution. The Dockerfile is in the created project ConclusionUse a Visual Studio Template to set up solutions that have the same structure. Microservices often have the same skeleton and are a great candidate for this. Templates allow you to install all NuGet packages and also to add Test projects. Another advantage is that even junior developers or interns can set up new projects.You can find the code for the template without variables here and the template with variables here." }, { "title": "Creating a Microservice with .NET Core 3.1", "url": "/creating-a-microservice-with-net-core-3-1/", "categories": "ASP.NET", "tags": ".NET Core 3.1, C#, CQRS, MediatR, Microservice, Swagger, xUnit", "date": "2020-07-21 00:00:00 +0200", "snippet": "Using a microservice architecture is a topic that is present at every conference for quite some time already. In my work as a consultant, I often have to train development teams about the basics of microservices, why to use them, do’s and don’ts, and best practices. Therefore, I want to start a new series where I will create a microservice and the following parts deploy it to Kubernetes using AKS, implementing CI/CD pipelines using Azure DevOps, using Helm charts and automated unit tests including code coverage.What is a Microservice?A microservice is an independently deployable service that is modeled around a business domain. An application uses many microservices that work together. A simple example would be an online shop. Potential microservices could be for customers, orders, products, and the search.As the name already suggests, a microservice is very small. The opinions on how small vary. Some say not more than a hundred lines, some say that it should do one thing. My opinion is that a microservice should something in the same context. This can also be several methods. Take a customer service for example. This service could offer methods to do the registration, login, and changing the user’s password.For more details on microservices, I recommend my post “Microservices – Getting Started“.Creating your first MicroserviceYou can find the code of the demo on GitHub on the createMicroservice branch.A microservice is an application that offers operations for a specific context. In my example, the application offers operations to read, create, and update customers. To keep it simple, I use an in-memory database. A bit special is that I am using CQRS and MediatR to read and write data. You can find a detailed description of the application in my post “Programming Microservices with .NET Core 3.1“.When you start the application, you will see the Swagger UI and can also try all the available methods. Swagger UI of the Microservice ConclusionThis post was a short introduction into the microservice that I will use in the following posts to create automatic builds, deployments to Kubernetes, DevOps workflows, and much more. Check out the next post of this series where I create a CI pipeline in Azure DevOps to build the .NET Core solution and run all unit tests.You can find the code of the demo on GitHub on the createMicroservice branch." }, { "title": "Scrum in 2020", "url": "/scrum-in-2020/", "categories": "DevOps", "tags": "Agile, DevOps, Scrum", "date": "2020-07-18 17:52:15 +0200", "snippet": "A couple of years ago, I wrote a series of posts about Scrum. Several years of experience later and due to the rise of containers and microservices I want to revisit my approach to Scrum in 2020 in this post and talk about what should be done differently.What is Scrum?Scrum is a framework that describes an agile approach on how to manage a project. It gives recommendations to teams on how to manage their work to achieve high team performance and support each other in the process. For more details about Scrum read my five-part series.Most sprints are between two to four weeks. That’s already better than waterfall approaches but there are still many handovers between teams or departments. For example, in a project, I worked previously the work items were created by the Product Owner and then given to the developers. After a two week sprint, all finished work items went back to the Product Owner because he was also responsible for testing. After several days to several weeks the testing results came back to the developers and the code got either deployed to production or new work items were created.When deploying to production, the code was handed over to the ops team which wasn’t involved in the development of the new features at all. Suddenly they had to keep code and features running which they didn’t know anything about.If the testing found bugs, these bugs got reported back but since the next sprint already started, they were planed earliest in the sprint after. This means that developers have to touch the same feature they implemented weeks before again. Due to the long time between the implementation and the revisit, it takes some time to get into the code again. This loses a lot of time and therefore causes a lot of costs. In my eyes, this isn’t how you should do Scrum in 2020.How to do Scrum in 2020Scrum in 2020 should be done differently because we have so many great tools that support us to get features done faster. We have microservices, Docker and Kubernetes, CI/CD and, great DevOps tools like Azure DevOps or GitHub Actions. All these tools support developers to get features faster into production. A feature that is implemented but not in production is nothing but costs.To do Scrum in 2020 right, I think that it needs a change of the mindset of organizations and also the adoption of the development process. Changing the development process is straight forward in theory. Have automated tests and use CI/CD. Changing the mindset of an organization or its customers is harder. To get features fast to production the testers need to be included in the development process and if someone else, for example, marketing has to approve the changes, they have to be also included in the development process and give their feedback fast (&lt; 1 day).I also think that the traditional stages of Test (also often called Q&amp;A or Integration) and Production are outdated. With CI/CD you can deploy every pull request and once the pull request is approved, delete the container.The goal should be to develop a feature in the morning and have it running in production in the afternoon. If it takes days or weeks until a finished feature runs in production, you are losing on the value the feature provides and increase the costs.ConclusionScrum in 2020 should be all about getting features into production as fast as possible. If developers implement features and only deploy at the end of a sprint, the costs are higher than necessary an,d also the quality is not as high as possible. To do Scrum right, organizations have to adopt a modern mindset and, developers have to use modern DevOps tools." }, { "title": "Flyweight Pattern in .NET Core 3.1", "url": "/flyweight-pattern-in-net-core-3-1/", "categories": "Design Pattern", "tags": ".NET Core 3.1, C#, Software Architecture", "date": "2020-07-12 00:00:00 +0200", "snippet": "The Flyweight pattern is a structural design pattern that helps you to share objects and therefore reduce the memory usage of your application.When to use the Flyweight PatternYou want to use the flyweight pattern when you have many objects which don’t change. A real-life example would be a restaurant. They serve many dishes but the meals are always the same (maybe the only vary in size). For example, when you go to McDonald’s and five order a Big Mac meals, you get five times the same meal.The flyweight pattern helps to keep the memory usage of your application low and also helps to speed up the processing of your objects. The flyweight pattern works well in combination with the strategy pattern.Flyweight Pattern ImplementationYou can find the code of the demo on GitHub.The flyweight pattern is very simple, therefore I will keep this demo short. For this demo, imagine that I have a fast food place selling different meals. To keep it simple, I serve only burger and pizza meals.First, I create the IMealFlyweight interface which has a definition for the name property and a serve method which takes a string for the size of the meal as the parameter.Next, I implement concrete classes for the pizza and burger meal. Following, you can see the implementation of the pizza meal:The pizza meal sets its name in the constructor and the serve method writes to the console that the meal got served. Already the last step is to create a factory that creates the meal objects for me. As previously mentioned, the main goal of the flyweight pattern is to re-use objects. The factory achieves this by re-using existing objects or creating new ones if they don’t exist. The objects get saved in a dictionary which I use as a cache. In a bigger application, this might be a fast cache like Redis.Note that I added a Thread.Sleep when creating new objects to simulate more real-world behavior.That’s it already. The flyweight pattern is implemented and can be tested now. To test the implementation, I added a print method to the factory which prints the number of items in the cache and their name. In the main method, I create for meal objects and print the cache state before and after the creation.Testing the ImplementationWhen running the application, you will see that there are no items in the cache and then slowly the medium-sized meals are created. The large meals are created way faster because they are read from the cache. After serving all four meals, the cache still has only two items, as expected. Testing the Flyweight Pattern Implementation ConclusionThe flyweight pattern is a very simple design pattern that can help you to reduce memory usage when you have many objects that won’t change. Another benefit of the pattern is that it can help to speed up your application and as seen, it is very easy to implement.You can find the code of the demo on GitHub." }, { "title": "IT Books you should read", "url": "/it-books-you-should-read/", "categories": "Miscellaneous", "tags": "Books, Learning", "date": "2020-07-06 22:51:29 +0200", "snippet": "Nowadays there are a million ways for developers to learn new technologies or principles, like Pluralsight, Youtube, blogs or books. I like to read books when I commute or during cardio in the gym. In this post, I will present all books I have read in the last couple of years and provide some basic information. I will separate them in IT books and non IT books. The books are in no particular order, except The Phoenix Project and The Unicorn Project were the books I enjoyed most.IT BooksThe Phoenix Project: A Novel about IT, DevOps, and Helping Your Business WinBill is an IT manager at Parts Unlimited. It’s Tuesday morning and on his drive into the office, Bill gets a call from the CEO.The company’s new IT initiative, code named Phoenix Project, is critical to the future of Parts Unlimited, but the project is massively over budget and very late. The CEO wants Bill to report directly to him and fix the mess in ninety days or else Bill’s entire department will be outsourced.With the help of a prospective board member and his mysterious philosophy of The Three Ways, Bill starts to see that IT work has more in common with manufacturing plant work than he ever imagined. With the clock ticking, Bill must organize work flow streamline interdepartmental communications, and effectively serve the other business functions at Parts Unlimited.In a fast-paced and entertaining style, three luminaries of the DevOps movement deliver a story that anyone who works in IT will recognize. Readers will not only learn how to improve their own IT organizations, they’ll never view IT the same way again.The Unicorn Project: A Novel about Developers, Digital Disruption, and Thriving in the Age of DataThe Unicorn Project, we follow Maxine, a senior lead developer and architect, as she is exiled to the Phoenix Project, to the horror of her friends and colleagues, as punishment for contributing to a payroll outage. She tries to survive in what feels like a heartless and uncaring bureaucracy and to work within a system where no one can get anything done without endless committees, paperwork, and approvals.The Age of Software is here, and another mass extinction event looms—this is a story about rebel developers and business leaders working together, racing against time to innovate, survive, and thrive in a time of unprecedented uncertainty…and opportunity.Clean Code: A Handbook of Agile Software CraftsmanshipEven bad code can function. But if code isn’t clean, it can bring a development organization to its knees. Every year, countless hours and significant resources are lost because of poorly written code. But it doesn’t have to be that way.Noted software expert Robert C. Martin presents a revolutionary paradigm with Clean Code: A Handbook of Agile Software Craftsmanship . Martin has teamed up with his colleagues from Object Mentor to distill their best agile practice of cleaning code “on the fly” into a book that will instill within you the values of a software craftsman and make you a better programmer—but only if you work at it.What kind of work will you be doing? You’ll be reading code—lots of code. And you will be challenged to think about what’s right about that code, and what’s wrong with it. More importantly, you will be challenged to reassess your professional values and your commitment to your craft._Clean Code _is divided into three parts. The first describes the principles, patterns, and practices of writing clean code. The second part consists of several case studies of increasing complexity. Each case study is an exercise in cleaning up code—of transforming a code base that has some problems into one that is sound and efficient. The third part is the payoff: a single chapter containing a list of heuristics and “smells” gathered while creating the case studies. The result is a knowledge base that describes the way we think when we write, read, and clean code.Readers will come away from this book understanding: How to tell the difference between good and bad code How to write good code and how to transform bad code into good code How to create good names, good functions, good objects, and good classes How to format code for maximum readability How to implement complete error handling without obscuring code logic How to unit test and practice test-driven developmentThis book is a must for any developer, software engineer, project manager, team lead, or systems analyst with an interest in producing better code.The Clean Coder: A Code of Conduct for Professional ProgrammersProgrammers who endure and succeed amidst swirling uncertainty and nonstop pressure share a common attribute: They care deeply about the practice of creating software. They treat it as a craft. They are professionals.In _The Clean Coder: A Code of Conduct for Professional Programmers, _legendary software expert Robert C. Martin introduces the disciplines, techniques, tools, and practices of true software craftsmanship. This book is packed with practical advice–about everything from estimating and coding to refactoring and testing. It covers much more than technique: It is about attitude. Martin shows how to approach software development with honor, self-respect, and pride; work well and work clean; communicate and estimate faithfully; face difficult decisions with clarity and honesty; and understand that deep knowledge comes with a responsibility to act.Readers will learn: What it means to behave as a true software craftsman How to deal with conflict, tight schedules, and unreasonable managers How to get into the flow of coding, and get past writer’s block How to handle unrelenting pressure and avoid burnout How to combine enduring attitudes with new development paradigms How to manage your time, and avoid blind alleys, marshes, bogs, and swamps How to foster environments where programmers and teams can thrive When to say “No”–and how to say it When to say “Yes”–and what yes really meansGreat software is something to marvel at: powerful, elegant, functional, a pleasure to work with as both a developer and as a user. Great software isn’t written by machines. It is written by professionals with an unshakable commitment to craftsmanship. The Clean Coder will help you become one of them–and earn the pride and fulfillment that they alone possess.Clean Architecture: A Craftsman’s Guide to Software Structure and DesignPractical Software Architecture Solutions from the Legendary Robert C. Martin (“Uncle Bob”)By applying universal rules of software architecture, you can dramatically improve developer productivity throughout the life of any software system. Now, building upon the success of his best-selling books Clean Code and The Clean Coder, legendary software craftsman Robert C. Martin (“Uncle Bob”) reveals those rules and helps you apply them.Martin’s _Clean Architecture _doesn’t merely present options. Drawing on over a half-century of experience in software environments of every imaginable type, Martin tells you what choices to make and why they are critical to your success. As you’ve come to expect from Uncle Bob, this book is packed with direct, no-nonsense solutions for the real challenges you’ll face–the ones that will make or break your projects. Learn what software architects need to achieve–and core disciplines and practices for achieving it Master essential software design principles for addressing function, component separation, and data management See how programming paradigms impose discipline by restricting what developers can do Understand what’s critically important and what’s merely a “detail” Implement optimal, high-level structures for web, database, thick-client, console, and embedded applications Define appropriate boundaries and layers, and organize components and services See why designs and architectures go wrong, and how to prevent (or fix) these failures_Clean Architecture _is essential reading for every current or aspiring software architect, systems analyst, system designer, and software manager–and for every programmer who must execute someone else’s designs.Clean Agile: Back to BasicsNearly twenty years after the Agile Manifesto was first presented, the legendary Robert C. Martin (“Uncle Bob”) reintroduces Agile values and principles for a new generation–programmers and non-programmers alike. Martin, author of Clean Code and other highly influential software development guides, was there at Agile’s founding. Now, in _Clean Agile: Back to Basics, _he strips away misunderstandings and distractions that over the years have made it harder to use Agile than was originally intended.Martin describes what Agile is in no uncertain terms: a small discipline that helps small teams manage small projects . . . with huge implications because every big project is comprised of many small projects. Drawing on his fifty years’ experience with projects of every conceivable type, he shows how Agile can help you bring true professionalism to software development. Get back to the basics–what Agile is, was, and should always be Understand the origins, and proper practice, of SCRUM Master essential business-facing Agile practices, from small releases and acceptance tests to whole-team communication Explore Agile team members’ relationships with each other, and with their product Rediscover indispensable Agile technical practices: TDD, refactoring, simple design, and pair programming Understand the central roles values and craftsmanship play in your Agile team’s successIf you want Agile’s true benefits, there are no shortcuts: You need to do Agile right. _Clean Agile: Back to Basics _will show you how, whether you’re a developer, tester, manager, project manager, or customer.97 Things Every Programmer Should Know: Collective Wisdom From The ExpertsTap into the wisdom of experts to learn what every programmer should know, no matter what language you use. With the 97 short and extremely useful tips for programmers in this book, you’ll expand your skills by adopting new approaches to old problems, learning appropriate best practices, and honing your craft through sound advice.With contributions from some of the most experienced and respected practitioners in the industry–including Michael Feathers, Pete Goodliffe, Diomidis Spinellis, Cay Horstmann, Verity Stob, and many more–this book contains practical knowledge and principles that you can apply to all kinds of projects.A few of the 97 things you should know: “Code in the Language of the Domain” by Dan North “Write Tests for People” by Gerard Meszaros “Convenience Is Not an -ility” by Gregor Hohpe “Know Your IDE” by Heinz Kabutz “A Message to the Future” by Linda Rising “The Boy Scout Rule” by Robert C. Martin (Uncle Bob) “Beware the Share” by Udi DahanRelease It!: Design and Deploy Production-Ready SoftwareA single dramatic software failure can cost a company millions of dollars – but can be avoided with simple changes to design and architecture. This new edition of the best-selling industry standard shows you how to create systems that run longer, with fewer failures, and recover better when bad things happen. New coverage includes DevOps, microservices, and cloud-native architecture. Stability antipatterns have grown to include systemic problems in large-scale systems. This is a must-have pragmatic guide to engineering for production systems.If you’re a software developer, and you don’t want to get alerts every night for the rest of your life, help is here. With a combination of case studies about huge losses – lost revenue, lost reputation, lost time, lost opportunity – and practical, down-to-earth advice that was all gained through painful experience, this book helps you avoid the pitfalls that cost companies millions of dollars in downtime and reputation. Eighty percent of project life-cycle cost is in production, yet few books address this topic.This updated edition deals with the production of today’s systems – larger, more complex, and heavily virtualized – and includes information on chaos engineering, the discipline of applying randomness and deliberate stress to reveal systematic problems. Build systems that survive the real world, avoid downtime, implement zero-downtime upgrades and continuous delivery, and make cloud-native applications resilient. Examine ways to architect, design, and build software – particularly distributed systems – that stands up to the typhoon winds of a flash mob, a Slashdotting, or a link on Reddit. Take a hard look at software that failed the test and find ways to make sure your software survives.To skip the pain and get the experience…get this book.Effektive Software Architekturen (German)Software-Architekten müssen komplexe fachliche und technische Anforderungen an IT-Systeme umsetzen und diese Systeme durch nachvollziehbare Strukturen flexibel und erweiterbar gestalten.Dieser Praxisleitfaden zeigt Ihnen, wie Sie Software-Architekturen effektiv und systematisch entwickeln können. Der bekannte Software-Architekt Gernot Starke unterstützt Sie mit praktischen Tipps, Architekturmustern und seinen Erfahrungen.Er gibt Antworten auf zentrale Fragen:– Welche Aufgaben haben Software-Architekten?– Wie gehen Software-Architekten beim Entwurf vor?– Wie kommunizieren und dokumentieren Sie Software-Architekturen?– Wie helfen Architekturmuster und Architekturbausteine?– Wie bewerten Sie Software-Architekturen?– Wie behandeln Sie Persistenz, grafische Benutzeroberflächen, Geschäftsregeln, Integration, Verteilung, Sicherheit, Fehlerbehandlung, Workflow-Management und sonstige technische Konzepte?– Was müssen Software-Architekten über MDA/MDSD, UML 2 und arc42 wissen?– Welche Aufgaben nehmen Enterprise-IT-Architekten wahr?Agile Principles, Patterns, and Practices in C#With the award-winning book Agile Software Development: Principles, Patterns, and Practices, Robert C. Martin helped bring Agile principles to tens of thousands of Java and C++ programmers. Now .NET programmers have a definitive guide to agile methods with this completely updated volume from Robert C. Martin and Micah Martin, Agile Principles, Patterns, and Practices in C#.This book presents a series of case studies illustrating the fundamentals of Agile development and Agile design, and moves quickly from UML models to real C# code. The introductory chapters lay out the basics of the agile movement, while the later chapters show proven techniques in action. The book includes many source code examples that are also available for download from the authors’ Web site.Readers will come away from this book understanding: Agile principles, and the fourteen practices of Extreme Programming Spiking, splitting, velocity, and planning iterations and releases Test-driven development, test-first design, and acceptance testing Refactoring with unit testing Pair programming Agile design and design smells The five types of UML diagrams and how to use them effectively Object-oriented package design and design patterns How to put all of it together for a real-world projectWhether you are a C# programmer or a Visual Basic or Java programmer learning C#, a software development manager, or a business analyst, Agile Principles, Patterns, and Practices in C# is the first book you should read to understand agile software and how it applies to programming in the .NET Framework.C# 8.0 in a NutshellWhen you have questions about C# 8.0 or .NET Core, this best-selling guide has the answers you need. C# is a language of unusual flexibility and breadth, but with its continual growth there’s so much more to learn. In the tradition of the O’Reilly Nutshell guides, this thoroughly updated edition is simply the best one-volume reference to the C# language available today.Organized around concepts and use cases, C# 8.0 in a Nutshell provides intermediate and advanced programmers with a concise map of C# and .NET knowledge that also plumbs significant depths. Get up to speed on C#, from syntax and variables to advanced topics such as pointers, closures, and patterns Dig deep into LINQ with three chapters dedicated to the topic Explore concurrency and asynchrony, advanced threading, and parallel programming Work with .NET features, including regular expressions, networking, serialization, spans, reflection, and cryptography Delve into Roslyn, the modular C# compiler as a servicePro ASP.NET Core MVC 2Now in its 7th edition, the best selling book on MVC is updated for ASP.NET Core MVC 2. It contains detailed explanations of the Core MVC functionality which enables developers to produce leaner, cloud optimized and mobile-ready applications for the .NET platform. This book puts ASP.NET Core MVC into context and dives deep into the tools and techniques required to build modern, cloud optimized extensible web applications. All the new MVC features are described in detail and the author explains how best to apply them to both new and existing projects.The ASP.NET Core MVC Framework is the latest evolution of Microsoft’s ASP.NET web platform, built on a completely new foundation. It represents a fundamental change to how Microsoft constructs and deploys web frameworks and is free of the legacy of earlier technologies such as Web Forms. ASP.NET Core MVC provides a “host agnostic” framework and a high-productivity programming model that promotes cleaner code architecture, test-driven development, and powerful extensibility.Best-selling author Adam Freeman has thoroughly revised this market-leading book and explains how to get the most from ASP.NET Core MVC. He starts with the nuts-and-bolts and shows you everything through to advanced features, going in-depth to give you the knowledge you need. The book includes a fully worked case study of a functioning web application that readers can use as a template for their own projects.What’s New in This Edition Fully updated for Visual Studio 2017, C# 7 and .NET Core 2 Coverage of new features such as view filters Wider platform and tooling coverage than ever before, with more on Visual Studio Code and working with .NET Core on non-Windows platforms Docker-based application deploymentWhat You Will Learn Gain a solid architectural understanding of ASP.NET Core MVC Explore the entire ASP.NET MVC Framework as a cohesive whole See how MVC and test-driven development work in action Learn what’s new in ASP.NET Core MVC 2 and how best to apply these new features to your own work See how to create RESTful web services and Single Page Applications Build on your existing knowledge of previous MVC releases to get up and running with the new programming model quickly and effectivelyWho This Book Is ForThis book is for web developers with a basic knowledge of ASP.NET and C# who want to incorporate the latest improvements and functionality in the ASP.NET Core MVC 2 Framework.Pro ASP.NET MVC 5The ASP.NET MVC 5 Framework is the latest evolution of Microsoft’s ASP.NET web platform. It provides a high-productivity programming model that promotes cleaner code architecture, test-driven development, and powerful extensibility, combined with all the benefits of ASP.NET.ASP.NET MVC 5 contains a number of advances over previous versions, including the ability to define routes using C# attributes and the ability to override filters. The user experience of building MVC applications has also been substantially improved. The new, more tightly integrated, Visual Studio 2013 IDE has been created specifically with MVC application development in mind and provides a full suite of tools to improve development times and assist in reporting, debugging and deploying your code.The popular Bootstrap JavaScript library has also now been included natively within MVC 5 providing you, the developer, with a wider range of multi-platform CSS and HTPro ASP.NET MVC PlatformThe power of ASP.NET MVC 5 stems from the underlying ASP.NET platform. To make your ASP.NET MVC applications the best they can be, you need to fully understand the platform features and know how they can be used to build effective and elegant MVC framework applications.The ASP.NET platform provides ASP.NET MVC applications with a rich suite of services including vital every-day features like extensible request handling, state management, and user authentication. Understanding how these features work is the difference between creating an average web application and the best-in-class.MVC applications that are architected with a thorough knowledge of the underlying platforms are faster to write, faster to run, and more readily adaptable to change. In Pro ASP.NET MVC 5 Platform, best-selling author Adam Freeman explains how to get the most from the entire ASP.NET platform, beginning with a nuts-and-bolts description of the patterns and tools you need through to the most advanced features. He goes in-depth at every stage to give you the practical knowledge that you need to apply these concepts to your own code.Implementing Azure Solutions: Deploy and manage Azure containers and build Azure solutions with easeMicrosoft Azure offers numerous solutions that can shape the future of any business. However, the major challenge that architects and administrators face lies in implementing these solutions.Implementing Azure Solutions helps you overcome this challenge by enabling you to implement Azure Solutions effectively. The book begins by guiding you in choosing the backend structure for your solutions. You will then work with the Azure toolkit and learn how to use Azure Managed Apps to share your solutions with the Azure service catalog. The book then focuses on various implementation techniques and best practices such as implementing Azure Cloud Services by configuring, deploying, and managing cloud services. As you progress through the chapters, you’ll learn how to work with Azure-managed Kubernetes and Azure Container Services.By the end of the book, you will be able to build robust cloud solutions on Azure.What you will learn Create and manage a Kubernetes cluster in Azure Kubernetes Service (AKS) Implement site-to-site VPN and ExpressRoute connections in your environment Explore the best practices in building and deploying app services Use Telemetry to monitor your Azure Solutions Design an Azure IoT solution and learn how to operate in different scenarios Implement a Hybrid Azure Design using Azure StackWho this book is forIf you’re an IT architect, IT professional, or DevOps engineer who plans to implement Azure Solutions for your organization, this book is for you.Extreme Programming Explained: Embrace ChangeAccountability. Transparency. Responsibility. These are not words that are often applied to software development.In this completely revised introduction to Extreme Programming (XP), Kent Beck describes how to improve your software development by integrating these highly desirable concepts into your daily development process.The first edition of Extreme Programming Explained is a classic. It won awards for its then-radical ideas for improving small-team development, such as having developers write automated tests for their own code and having the whole team plan weekly. Much has changed in five years. This completely rewritten second edition expands the scope of XP to teams of any size by suggesting a program of continuous improvement based on: Five core values consistent with excellence in software development Eleven principles for putting those values into action Thirteen primary and eleven corollary practices to help you push development past its current business and technical limitationsWhether you have a small team that is already closely aligned with your customers or a large team in a gigantic or multinational organization, you will find in these pages a wealth of ideas to challenge, inspire, and encourage you and your team members to substantially improve your software development.You will discover how to: Involve the whole team—XP style Increase technical collaboration through pair programming and continuous integration Reduce defects through developer testing Align business and technical decisions through weekly and quarterly planning Improve teamwork by setting up an informative, shared workspaceYou will also find many other concrete ideas for improvement, all based on a philosophy that emphasizes simultaneously increasing the humanity and effectiveness of software development.Every team can improve. Every team can begin improving today. Improvement is possible—beyond what we can currently imagine. Extreme Programming Explained, Second Edition, offers ideas to fuel your improvement for years to come.Cloud Computing from Beginning to EndYour Complete Guide to Cloud Computing and Migrating to the Cloud. This book covers not only the technical details of how public and private cloud technology works but also the strategy, technical design, and in-depth implementation details required to migrate existing applications to the cloud.After reading this book, you will have a much better understanding of cloud technology and the steps required to quickly reap its benefits while at the same time lowering your IT implementation risk. Written by a proven expert in cloud computing, business management, network engineering, and IT security. This is a must-read for IT management, CIOs, senior IT engineers, and program managers in the government, DoD, and commercial sectors.Azure for Architects: Implementing cloud design, DevOps, containers, IoT, and serverless solutions on your public cloudOver the years, Azure cloud services have grown quickly, and the number of organizations adopting Azure for their cloud services is also gradually increasing. Leading industry giants are finding that Azure fulfills their extensive cloud requirements.Azure for Architects – Second Edition starts with an extensive introduction to major designing and architectural aspects available with Azure. These design patterns focus on different aspects of the cloud, such as high availability, security, and scalability. Gradually, we move on to other aspects, such as ARM template modular design and deployments.This is the age of microservices and serverless is the preferred implementation mechanism for them. This book covers the entire serverless stack available in Azure including Azure Event Grid, Azure Functions, and Azure Logic Apps. New and advance features like durable functions are discussed at length. A complete integration solution using these serverless technologies is also part of the book. A complete chapter discusses all possible options related to containers in Azure including Azure Kubernetes services, Azure Container Instances and Registry, and Web App for Containers.Data management and integration is an integral part of this book that discusses options for implementing OLTP solutions using Azure SQL, Big Data solutions using Azure Data factory and Data Lake Storage, eventing solutions using stream analytics, and Event Hubs. This book will provide insights into Azure governance features such as tagging, RBAC, cost management, and policies.By the end of this book, you will be able to develop a full-?edged Azure cloud solution that is Enterprise class and future-ready.What you will learn Create an end-to-end integration solution using Azure Serverless Stack Learn Big Data solutions and OLTP–based applications on Azure Understand DevOps implementations using Azure DevOps Architect solutions comprised of multiple resources in Azure Develop modular ARM templates Develop Governance on Azure using locks, RBAC, policies, tags and cost Learn ways to build data solutions on Azure Understand the various options related to containers including Azure Kubernetes ServicesWho this book is forIf you are Cloud Architects, DevOps Engineers, or developers who want to learn key architectural aspects of the Azure Cloud platform, then this book is for you.Prior basic knowledge of the Azure Cloud platform is good to have.Architecting the Cloud: Design Decisions for Cloud Computing Service Models (SaaS, PaaS, and IaaS)An expert guide to selecting the right cloud service model for your businessCloud computing is all the rage, allowing for the delivery of computing and storage capacity to a diverse community of end-recipients. However, before you can decide on a cloud model, you need to determine what the ideal cloud service model is for your business. Helping you cut through all the haze, Architecting the Cloud is vendor neutral and guides you in making one of the most critical technology decisions that you will face: selecting the right cloud service model(s) based on a combination of both business and technology requirements. Guides corporations through key cloud design considerations Discusses the pros and cons of each cloud service model Highlights major design considerations in areas such as security, data privacy, logging, data storage, SLA monitoring, and more Clearly defines the services cloud providers offer for each service model and the cloud services IT must provideArming you with the information you need to choose the right cloud service provider, Architecting the Cloud is a comprehensive guide covering everything you need to be aware of in selecting the right cloud service model for you.Microservices: Flexible Software ArchitectureThe Most Complete, Practical, and Actionable Guide to MicroservicesGoing beyond mere theory and marketing hype, Eberhard Wolff presents all the knowledge you need to capture the full benefits of this emerging paradigm. He illuminates microservice concepts, architectures, and scenarios from a technology-neutral standpoint, and demonstrates how to implement them with today’s leading technologies such as Docker, Java, Spring Boot, the Netflix stack, and Spring Cloud.The author fully explains the benefits and tradeoffs associated with microservices, and guides you through the entire project lifecycle: development, testing, deployment, operations, and more. You’ll find best practices for architecting microservice-based systems, individual microservices, and nanoservices, each illuminated with pragmatic examples. The author supplements opinions based on his experience with concise essays from other experts, enriching your understanding and illuminating areas where experts disagree. Readers are challenged toexperiment on their own the concepts explained in the book to gain hands-on experience. Discover what microservices are, and how they differ from other forms of modularization Modernize legacy applications and efficiently build new systems Drive more value from continuous delivery with microservices Learn how microservices differ from SOA Optimize the microservices project lifecycle Plan, visualize, manage, and evolve architecture Integrate and communicate among microservices Apply advanced architectural techniques, including CQRS and Event Sourcing Maximize resilience and stability Operate and monitor microservices in production Build a full implementation with Docker, Java, Spring Boot, the Netflix stack, and Spring Cloud Explore nanoservices with Amazon Lambda, OSGi, Java EE, Vert.x, Erlang, and Seneca Understand microservices’ impact on teams, technical leaders, product owners, and stakeholdersManagers will discover better ways to support microservices, and learn how adopting the method affects the entire organization. Developers will master the technical skills and concepts they need to be effective. Architects will gain a deep understanding of key issues in creating or migrating toward microservices, and exactly what it will take to transform their plans into reality.Practical Monitoring: Effective Strategies for the Real WorldDo you have a nagging feeling that your monitoring needs improvement, but you just aren’t sure where to start or how to do it? Are you plagued by constant, meaningless alerts? Does your monitoring system routinely miss real problems? This is the book for you.Mike lays out a practical approach to designing and implementing an effective monitoring–from the application down to hardware in a datacenter, and everything between. Practical Monitoring will provide you with straight-forward strategies and tactics for designing and implementing a strong monitoring foundations for your company.Practical Monitoring has a unique vendor-neutral approach to monitoring. Rather than discuss how to implement specific tools, Mike teaches the principles and underlying mechanics behind monitoring so you can implement the lessons in any tool. Practical Monitoring covers such topics as: Monitoring anti-patterns Principles of monitoring design How to build an effective on-call rotation Getting metrics and logs out of your application Cloud Computing – Concepts, Technology &amp; ArchitectureClouds are distributed technology platforms that leverage sophisticated technology innovations to provide highly scalable and resilient environments that can be remotely utilized by organizations in a multitude of powerful ways. To successfully build upon, integrate with, or even create a cloud environment requires an understanding of its common inner mechanics, architectural layers, and models, as well as an understanding of the business and economic factors that result from the adoption and real-world use of cloud-based services.In Cloud Computing: Concepts, Technology &amp; Architecture, Thomas Erl, one of the world’s top-selling IT authors, teams up with cloud computing experts and researchers to break down proven and mature cloud computing technologies and practices into a series of well-defined concepts, models, technology mechanisms, and technology architectures, all from an industry-centric and vendor-neutral point of view. In doing so, the book establishes concrete, academic coverage with a focus on structure, clarity, and well-defined building blocks for mainstream cloud computing platforms and solutions.Subsequent to technology-centric coverage, the book proceeds to establish business-centric models and metrics that allow for the financial assessment of cloud-based IT resources and their comparison to those hosted on traditional IT enterprise premises. Also provided are templates and formulas for calculating SLA-related quality-of-service values and numerous explorations of the SaaS, PaaS, and IaaS delivery models.With more than 260 figures, 29 architectural models, and 20 mechanisms, this indispensable guide provides a comprehensive education of cloud computing essentials that will never leave your side.Starting and Scaling DevOps in the EnterpriseDevOps is a fundamental shift in how leading edge companies are starting to manage their software and IT work. Businesses need to move more quickly than ever before, and large software organizations are applying these DevOps principles to develop new software faster than anyone previously thought possible. DevOps started in small organizations and in large organizations that had or created architectures that enabled small teams to independently develop, qualify, and deploy code. The impact on productivity is so dramatic that larger organizations with tightly coupled architectures are realizing they either need to embrace DevOps or be left behind.The biggest challenge is that they can’t just empower small teams to work independently because their legacy architectures require coordinating the development, qualification, and deployment of code across hundreds of people. They need a DevOps approach that not only addresses their unique challenges, but also helps them reach an organization-wide agreement on where to start and how to scale DevOps. That is where Starting and Scaling DevOps in the Enterprise comes in. Starting and Scaling DevOps in the Enterprise is a quick, easy-to-read guide that helps structure those improvements by providing a framework that large organizations can use to understand DevOps principles in the context of their current development processes and gain alignment across the organization for successful implementations.The book illustrates how to analyze your current development and delivery processes to ensure you gain positive momentum by implementing the DevOps practices that will have the greatest immediate impact on the productivity of your organization, with the goal of achieving continuous improvement over time.Monolith to Microservice: Evolutionary Patterns to Transform Your MonolithHow do you detangle a monolithic system and migrate it to a microservice architecture? How do you do it while maintaining business-as-usual? As a companion to Sam Newman’s extremely popular Building Microservices, this new book details a proven method for transitioning an existing monolithic system to a microservice architecture.With many illustrative examples, insightful migration patterns, and a bevy of practical advice to transition your monolith enterprise into a microservice operation, this practical guide covers multiple scenarios and strategies for a successful migration, from initial planning all the way through application and database decomposition. You’ll learn several tried and tested patterns and techniques that you can use as you migrate your existing architecture. Ideal for organizations looking to transition to microservices, rather than rebuild Helps companies determine whether to migrate, when to migrate, and where to begin Addresses communication, integration, and the migration of legacy systems Discusses multiple migration patterns and where they apply Provides database migration examples, along with synchronization strategies Explores application decomposition, including several architectural refactoring patterns Delves into details of database decomposition, including the impact of breaking referential and transactional integrity, new failure modes, and moreKubernetes: Up &amp; RunningKubernetes radically changes the way applications are built and deployed in the cloud. Since its introduction in 2014, this container orchestrator has become one of the largest and most popular open source projects in the world. The updated edition of this practical book shows developers and ops personnel how Kubernetes and container technology can help you achieve new levels of velocity, agility, reliability, and efficiency.Kelsey Hightower, Brendan Burns, and Joe Beda—who’ve worked on Kubernetes at Google and beyond—explain how this system fits into the lifecycle of a distributed application. You’ll learn how to use tools and APIs to automate scalable distributed systems, whether it’s for online services, machine learning applications, or a cluster of Raspberry Pi computers. Create a simple cluster to learn how Kubernetes works Dive into the details of deploying an application using Kubernetes Learn specialized objects in Kubernetes, such as DaemonSets, jobs, ConfigMaps, and secrets Explore deployments that tie together the lifecycle of a complete application Get practical examples of how to develop and deploy real-world applications in KubernetesThe DevOps HandbookIncrease profitability, elevate work culture, and exceed productivity goals through DevOps practices.More than ever, the effective management of technology is critical for business competitiveness. For decades, technology leaders have struggled to balance agility, reliability, and security. The consequences of failure have never been greater―whether it’s the healthcare.gov debacle, cardholder data breaches, or missing the boat with Big Data in the cloud.And yet, high performers using DevOps principles, such as Google, Amazon, Facebook, Etsy, and Netflix, are routinely and reliably deploying code into production hundreds, or even thousands, of times per day.Following in the footsteps of The Phoenix Project, The DevOps Handbook shows leaders how to replicate these incredible outcomes, by showing how to integrate Product Management, Development, QA, IT Operations, and Information Security to elevate your company and win in the marketplace.Lean Enterprise: How High Performance Organizations Innovate at Scale (vlt runter?)How well does your organization respond to changing market conditions, customer needs, and emerging technologies when building software-based products? This practical guide presents Lean and Agile principles and patterns to help you move fast at scale—and demonstrates why and how to apply these methodologies throughout your organization, rather than with just one department or team.Through case studies, you’ll learn how successful enterprises have rethought everything from governance and financial management to systems architecture and organizational culture in the pursuit of radically improved performance. Adopting Lean will take time and commitment, but it’s vital for harnessing the cultural and technical forces that are accelerating the rate of innovation. Discover how Lean focuses on people and teamwork at every level, in contrast to traditional management practices Approach problem-solving experimentally, by exploring solutions, testing assumptions, and getting feedback from real users Lead and manage large-scale programs in a way that empowers employees, increases the speed and quality of delivery, and lowers costs Learn how to implement ideas from the DevOps and Lean Startup movements even in complex, regulated environmentsDocker on Windows: From 101 to production with Docker on WindowsLearn how to run new and old applications in Docker containers on Windows – modernizing the architecture, improving security and maximizing efficiency.Key Features Run .NET Framework and .NET Core apps in Docker containers for efficiency, security and portability Design distributed containerized apps, using enterprise-grade open source software from Docker Hub Build a CI/CD pipeline with Docker, going from source to a production Docker Swarm in the cloudBook DescriptionDocker on Windows, Second Edition teaches you all you need to know about Docker on Windows, from the 101 to running highly-available workloads in production. You’ll be guided through a Docker journey, starting with the key concepts and simple examples of .NET Framework and .NET Core apps in Docker containers on Windows. Then you’ll learn how to use Docker to modernize the architecture and development of traditional ASP.NET and SQL Server apps.The examples show you how to break up legacy monolithic applications into distributed apps and deploy them to a clustered environment in the cloud, using the exact same artifacts you use to run them locally. You’ll see how to build a CI/CD pipeline which uses Docker to compile, package, test and deploy your applications. To help you move confidently to production, you’ll learn about Docker security, and the management and support options.The book finishes with guidance on getting started with Docker in your own projects. You’ll walk through some real-world case studies for Docker implementations, from small-scale on-premises apps to very large-scale apps running on Azure.What you will learn Understand key Docker concepts: images, containers, registries and swarms Run Docker on Windows 10, Windows Server 2019, and in the cloud Deploy and monitor distributed solutions across multiple Docker containers Run containers with high availability and failover with Docker Swarm Master security in-depth with the Docker platform, making your apps more secure Build a Continuous Deployment pipeline, running Jenkins and Git in Docker Debug applications running in Docker containers using Visual Studio Plan the adoption of Docker in your organizationWho this book is forIf you want to modernize an old monolithic application without rewriting it, smooth the deployment to production, or move to DevOps or the cloud, then Docker is the enabler for you. This book gives you a solid grounding in Docker so you can confidently approach all of these scenarios.Modern API Design with ASP.NET Core 2Use ASP.NET Core 2 to create durable and cross-platform web APIs through a series of applied, practical scenarios. Examples in this book help you build APIs that are fast and scalable. You’ll progress from the basics of the framework through to solving the complex problems encountered in implementing secure RESTful services. The book is packed full of examples showing how Microsoft’s ground-up rewrite of ASP.NET Core 2 enables native cross-platform applications that are fast and modular, allowing your cloud-ready server applications to scale as your business grows.Major topics covered in the book include the fundamentals and core concepts of ASP.NET Core 2. You’ll learn about building RESTful APIs with the MVC pattern using proven best practices and following the six principles of REST. Examples in the book help in learning to develop world-class web APIs and applications that can run on any platform, including Windows, Linux, and MacOS. You can even deploy to Microsoft Azure and automate your delivery by implementing Continuous Integration and Continuous Deployment pipelines.What You Will Learn Incorporate automated API tooling such as Swagger from the OpenAPI specification Standardize query and response formats using Facebook’s GraphQL query language Implement security by applying authentication and authorization using ASP.NET Identity Ensure the safe storage of sensitive data using the data protection stack Create unit and integration tests to guarantee code qualityWho This Book Is ForDevelopers who build server applications such as web sites and web APIs that need to run fast and cross platform; programmers who want to implement practical solutions for real-world problems; those who want in-depth knowledge of the latest bits of ASP.NET Core 2.0Microservices with Docker on Microsoft AzureMicroservice-based applications enable unprecedented agility and ease of management, and Docker containers are ideal for building them. Microsoft Azure offers all the foundational technology and higher-level services you need to develop and run any microservices application. Microservices with Docker on Microsoft Azure brings together essential knowledge for creating these applications from the ground up, or incrementally deconstructing monolithic applications over time.The authors draw on their pioneering experience helping to develop Azure’s microservices features and collaborating with Microsoft product teams who’ve relied on microservices architectures for years. They illuminate the benefits and challenges of microservices development and share best practices all developers and architects should know.You’ll gain hands-on expertise through a detailed sample application, downloadable at github.com/flakio/flakio.github.io. Step by step, you’ll walk through working with services written in Node.js, Go, and ASP.NET 5, using diverse data stores (mysql, elasticsearch, block storage). The authors guide you through using Docker Hub as a service registry, and Microsoft Azure Container service for cluster management and service orchestration.Coverage includes: Recognizing how microservices architectures are different, and when they make sense Understanding Docker containers in the context of microservices architectures Building, pulling, and layering Docker images Working with Docker volumes, containers, images, tags, and logs Using Docker Swarm, Docker Compose, and Docker Networks Creating Docker hosts using the Azure portal, Azure Resource Manager, the command line, docker-machine, or locally via Docker toolbox Establishing development and DevOps environments to support microservices applications Making the most of Docker’s continuous delivery options Using Azure’s cluster and container orchestration capabilities to operate and scale containerized microservices applications with maximum resilience Monitoring microservices applications with Azure Diagnostics, Visual Studio Application Insights, and Microsoft Operations Management Suite Developing microservices applications faster and more effectively with Azure Service Fabric An extensive sample application demonstrating the microservices concepts discussed throughout the book is available onlineDomain Driven Design: Tackling Complexity in the Heart of SoftwareThe software development community widely acknowledges that domain modeling is central to software design. Through domain models, software developers are able to express rich functionality and translate it into a software implementation that truly serves the needs of its users. But despite its obvious importance, there are few practical resources that explain how to incorporate effective domain modeling into the software development process.Domain-Driven Design fills that need. This is not a book about specific technologies. It offers readers a systematic approach to domain-driven design, presenting an extensive set of design best practices, experience-based techniques, and fundamental principles that facilitate the development of software projects facing complex domains. Intertwining design and development practice, this book incorporates numerous examples based on actual projects to illustrate the application of domain-driven design to real-world software development.Readers learn how to use a domain model to make a complex development effort more focused and dynamic. A core of best practices and standard patterns provides a common language for the development team. A shift in emphasis–refactoring not just the code but the model underlying the code–in combination with the frequent iterations of Agile development leads to deeper insight into domains and enhanced communication between domain expert and programmer. Domain-Driven Design then builds on this foundation, and addresses modeling and design for complex systems and larger organizations.Specific topics covered include: Getting all team members to speak the same language Connecting model and implementation more deeply Sharpening key distinctions in a model Managing the lifecycle of a domain object Writing domain code that is safe to combine in elaborate ways Making complex code obvious and predictable Formulating a domain vision statement Distilling the core of a complex domain Digging out implicit concepts needed in the model Applying analysis patterns Relating design patterns to the model Maintaining model integrity in a large system Dealing with coexisting models on the same project Organizing systems with large-scale structures Recognizing and responding to modeling breakthroughsWith this book in hand, object-oriented developers, system analysts, and designers will have the guidance they need to organize and focus their work, create rich and useful domain models, and leverage those models into quality, long-lasting software implementations.The Art of Unit TestingThe Art of Unit Testing, Second Edition guides you step by step from writing your first simple tests to developing robust test sets that are maintainable, readable, and trustworthy. You’ll master the foundational ideas and quickly move to high-value subjects like mocks, stubs, and isolation, including frameworks such as Moq, FakeItEasy, and Typemock Isolator. You’ll explore test patterns and organization, working with legacy code, and even “untestable” code. Along the way, you’ll learn about integration testing and techniques and tools for testing databases and other technologies.About this BookYou know you should be unit testing, so why aren’t you doing it? If you’re new to unit testing, if you find unit testing tedious, or if you’re just not getting enough payoff for the effort you put into it, keep reading.The Art of Unit Testing, Second Edition guides you step by step from writing your first simple unit tests to building complete test sets that are maintainable, readable, and trustworthy. You’ll move quickly to more complicated subjects like mocks and stubs, while learning to use isolation (mocking) frameworks like Moq, FakeItEasy, and Typemock Isolator. You’ll explore test patterns and organization, refactor code applications, and learn how to test “untestable” code. Along the way, you’ll learn about integration testing and techniques for testing with databases.The examples in the book use C#, but will benefit anyone using a statically typed language such as Java or C++.What’s Inside Create readable, maintainable, trustworthy tests Fakes, stubs, mock objects, and isolation (mocking) frameworks Simple dependency injection techniques Refactoring legacy code.NET DevOps for Azure: A Developer’s Guide to DevOps Architecture the Right WayUse this book as your one-stop shop for architecting a world-class DevOps environment with Microsoft technologies..NET DevOps for Azure is a synthesis of practices, tools, and process that, together, can equip a software organization to move fast and deliver the highest quality software. The book begins by discussing the most common challenges faced by developers in DevOps today and offers options and proven solutions on how to implement DevOps for your team.Daily, millions of developers use .NET to build and operate mission-critical software systems for organizations around the world. While the marketplace has scores of information about the technology, it is completely up to you to put together all the blocks in the right way for your environment.This book provides you with a model to build on. The relevant principles are covered first along with how to implement that part of the environment. And while variances in tools, language, or requirements will change the needed implementation, the DevOps model is the architecture for the working environment for your team. You can modify parts of the model to customize it to your enterprise, but the architecture will enable all of your teams and applications to accelerate in performance.What You Will Learn: Get your .NET applications into a DevOps environment in Azure Analyze and address the part of your DevOps process that causes delays or bottlenecks Track code using Azure Repos and conduct acceptance tests Apply the rules for segmenting applications into Git repositories Understand the different types of builds and when to use each Know how to think about code validation in your DevOps environment Provision and configure environments; deploy release candidates across the environments in Azure Monitor and support software that has been deployed to a production environmentThis book is for .NET Developers who are using or want to use DevOps in Azure but don’t know where to begin.Cloud Native DevOps with KubernetesKubernetes is the operating system of the cloud native world, providing a reliable and scalable platform for running containerized workloads. In this friendly, pragmatic book, cloud experts John Arundel and Justin Domingus show you what Kubernetes can do—and what you can do with it.You’ll learn all about the Kubernetes ecosystem, and use battle-tested solutions to everyday problems. You’ll build, step by step, an example cloud native application and its supporting infrastructure, along with a development environment and continuous deployment pipeline that you can use for your own applications. Understand containers and Kubernetes from first principles; no experience necessary Run your own clusters or choose a managed Kubernetes service from Amazon, Google, and others Use Kubernetes to manage resource usage and the container lifecycle Optimize clusters for cost, performance, resilience, capacity, and scalability Learn the best tools for developing, testing, and deploying your applications Apply the latest industry practices for security, observability, and monitoring Adopt DevOps principles to help make your development teams lean, fast, and effectiveProgramming KubernetesIf you’re looking to develop native applications in Kubernetes, this is your guide. Developers and AppOps administrators will learn how to build Kubernetes-native applications that interact directly with the API server to query or update the state of resources. AWS developer advocate Michael Hausenblas and Red Hat principal software engineer Stefan Schimanski explain the characteristics of these apps and show you how to program Kubernetes to build them.You’ll explore the basic building blocks of Kubernetes, including the client-go API library and custom resources. All you need to get started is a rudimentary understanding of development and system administration tools and practices, such as package management, the Go programming language, and Git. Walk through Kubernetes API basics and dive into the server’s inner structure Explore Kubernetes’s programming interface in Go, including Kubernetes API objects Learn about custom resources—the central extension tools used in the Kubernetes ecosystem Use tags to control Kubernetes code generators for custom resources Write custom controllers and operators and make them production ready Extend the Kubernetes API surface by implementing a custom API serverLearning Chaos EngineeringMost companies work hard to avoid costly failures, but in complex systems a better approach is to embrace and learn from them. Through chaos engineering, you can proactively hunt for evidence of system weaknesses before they trigger a crisis. This practical book shows software developers and system administrators how to plan and run successful chaos engineering experiments.System weaknesses go beyond your infrastructure, platforms, and applications to include policies, practices, playbooks, and people. Author Russ Miles explains why, when, and how to test systems, processes, and team responses using simulated failures on Game Days. You’ll also learn how to work toward continuous chaos through automation with features you can share across your team and organization. Learn to think like a chaos engineer Build a hypothesis backlog to determine what could go wrong in your system Develop your hypotheses into chaos engineering experiment Game Days Write, run, and learn from automated chaos experiments using the open source Chaos Toolkit Turn chaos experiments into tests to confirm that you’ve overcome the weaknesses you discovered Observe and control your automated chaos experiments while they are runningAccelerate: The Science of Lean Software and DevOps: Building and Scaling High Performing Technology OrganizationsWinner of the Shingo Publication AwardDoes technology actually matter? And how can we apply technology to drive business value?For years, we’ve been told that the performance of software delivery teams doesn’t matter—that it can’t provide a competitive advantage to our companies. Through four years of groundbreaking research, Dr. Nicole Forsgren, Jez Humble, and Gene Kim set out to find a way to measure software delivery performance—and what drives it—using rigorous statistical methods. This book presents both the findings and the science behind that research.Readers will discover how to measure the performance of their teams, and what capabilities they should invest in to drive higher performance.Cloud Architecture PatternIf your team is investigating ways to design applications for the cloud, this concise book introduces 11 architecture patterns that can help you take advantage of cloud-platform services. You’ll learn how each of these platform-agnostic patterns work, when they might be useful in the cloud, and what impact they’ll have on your application architecture. You’ll also see an example of each pattern applied to an application built with Windows Azure.The patterns are organized into four major topics, such as scalability and handling failure, and primer chapters provide background on each topic. With the information in this book, you’ll be able to make informed decisions for designing effective cloud-native applications that maximize the value of cloud services, while also paying attention to user experience and operational efficiency.Learn about architectural patterns for: Scalability. Discover the advantages of horizontal scaling. Patterns covered include Horizontally Scaling Compute, Queue-Centric Workflow, and Auto-Scaling. Big data. Learn how to handle large amounts of data across a distributed system. Eventual consistency is explained, along with the MapReduce and Database Sharding patterns. Handling failure. Understand how multi-tenant cloud services and commodity hardware influence your applications. Patterns covered include Busy Signal and Node Failure. Distributed users. Learn how to overcome delays due to network latency when building applications for a geographically distributed user base. Patterns covered include Colocation, Valet Key, CDN, and Multi-Site Deployment.Managing KubernetesWhile Kubernetes has greatly simplified the task of deploying containerized applications, managing this orchestration framework on a daily basis can still be a complex undertaking. With this practical book, site reliability and DevOps engineers will learn how to build, operate, manage, and upgrade a Kubernetes cluster—whether it resides on cloud infrastructure or on-premises.Brendan Burns, cofounder of Kubernetes, and Craig Tracey, staff field engineer at Heptio, dissect how Kubernetes works internally and demonstrate ways to maintain, adjust, and improve the cluster to suit your particular use case. You’ll learn how to make architectural choices for designing a cluster, managing access control, monitoring and alerting, and upgrading Kubernetes. Dive in and discover how to take full advantage of this orchestration framework’s capabilities. Learn how your cluster operates, how developers use it to deploy applications, and how Kubernetes can facilitate a developer’s job Adjust, secure, and tune your cluster by understanding Kubernetes APIs and configuration options Detect cluster-level problems early and learn the steps necessary to respond and recover quickly Determine how and when to add libraries, tools, and platforms that build on, extend, or otherwise improve a Kubernetes clusterComplete Guide to Test AutomationRely on this robust and thorough guide to build and maintain successful test automation. As the software industry shifts from traditional waterfall paradigms into more agile ones, test automation becomes a highly important tool that allows your development teams to deliver software at an ever-increasing pace without compromising quality.Even though it may seem trivial to automate the repetitive tester’s work, using test automation efficiently and properly is not trivial. Many test automation endeavors end up in the “graveyard” of software projects. There are many things that affect the value of test automation, and also its costs. This book aims to cover all of these aspects in great detail so you can make decisions to create the best test automation solution that will not only help your test automation project to succeed, but also allow the entire software project to thrive.One of the most important details that affects the success of the test automation is how easy it is to maintain the automated tests. Complete Guide to Test Automation provides a detailed hands-on guide for writing highly maintainable test code.What You’ll Learn Know the real value to be expected from test automation Discover the key traits that will make your test automation project succeed Be aware of the different considerations to take into account when planning automated tests vs. manual tests Determine who should implement the tests and the implications of this decision Architect the test project and fit it to the architecture of the tested application Design and implement highly reliable automated tests Begin gaining value from test automation earlier Integrate test automation into the business processes of the development team Leverage test automation to improve your organization’s performance and quality, even without formal authority Understand how different types of automated tests will fit into your testing strategy, including unit testing, load and performance testing, visual testing, and moreWho This Book Is ForThose involved with software development such as test automation leads, QA managers, test automation developers, and development managers. Some parts of the book assume hands-on experience in writing code in an object-oriented language (mainly C# or Java), although most of the content is also relevant for non-programmers.Test-Driven DevelopmentQuite simply, test-driven development is meant to eliminate fear in application development. While some fear is healthy (often viewed as a conscience that tells programmers to “be careful!”), the author believes that byproducts of fear include tentative, grumpy, and uncommunicative programmers who are unable to absorb constructive criticism. When programming teams buy into TDD, they immediately see positive results. They eliminate the fear involved in their jobs, and are better equipped to tackle the difficult challenges that face them.TDD eliminates tentative traits, it teaches programmers to communicate, and it encourages team members to seek out criticism However, even the author admits that grumpiness must be worked out individually! In short, the premise behind TDD is that code should be continually tested and refactored. Kent Beck teaches programmers by example, so they can painlessly and dramatically increase the quality of their work.Prometheus: Up &amp; Running: Infrastructure and Application Performance MonitoringGet up to speed with Prometheus, the metrics-based monitoring system used by tens of thousands of organizations in production. This practical guide provides application developers, sysadmins, and DevOps practitioners with a hands-on introduction to the most important aspects of Prometheus, including dashboarding and alerting, direct code instrumentation, and metric collection from third-party systems with exporters.This open source system has gained popularity over the past few years for good reason. With its simple yet powerful data model and query language, Prometheus does one thing, and it does it well. Author and Prometheus developer Brian Brazil guides you through Prometheus setup, the Node exporter, and the Alertmanager, then demonstrates how to use them for application and infrastructure monitoring. Know where and how much to apply instrumentation to your application code Identify metrics with labels using unique key-value pairs Get an introduction to Grafana, a popular tool for building dashboards Learn how to use the Node Exporter to monitor your infrastructure Use service discovery to provide different views of your machines and services Use Prometheus with Kubernetes and examine exporters you can use with containers Convert data from other monitoring systems into the Prometheus formatEffective DevOps: Building a Culture of Collaboration, Affinity, and Tooling at Scale&gt;Some companies think that adopting devops means bringing in specialists or a host of new tools. With this practical guide, you’ll learn why devops is a professional and cultural movement that calls for change from inside your organization. Authors Ryn Daniels and Jennifer Davis provide several approaches for improving collaboration within teams, creating affinity among teams, promoting efficient tool usage in your company, and scaling up what works throughout your organization’s inflection points.Devops stresses iterative efforts to break down information silos, monitor relationships, and repair misunderstandings that arise between and within teams in your organization. By applying the actionable strategies in this book, you can make sustainable changes in your environment regardless of your level within your organization. Explore the foundations of devops and learn the four pillars of effective devops Encourage collaboration to help individuals work together and build durable and long-lasting relationships Create affinity among teams while balancing differing goals or metrics Accelerate cultural direction by selecting tools and workflows that complement your organization Troubleshoot common problems and misunderstandings that can arise throughout the organizational lifecycle Learn from case studies from organizations and individuals to help inform your own devops journeyThe Software Architect Elevator: Redefining the Architect’s Role in the Digital Enterprise&gt;As the digital economy changes the rules of the game for enterprises, the role of software and IT architects is also transforming. Rather than focus on technical decisions alone, architects and senior technologists need to combine organizational and technical knowledge to effect change in their company’s structure and processes. To accomplish that, they need to connect the IT engine room to the penthouse, where the business strategy is defined.In this guide, author Gregor Hohpe shares real-world advice and hard-learned lessons from actual IT transformations. His anecdotes help architects, senior developers, and other IT professionals prepare for a more complex but rewarding role in the enterprise.This book is ideal for: Software architects and senior developers looking to shape the company’s technology direction or assist in an organizational transformation Enterprise architects and senior technologists searching for practical advice on how to navigate technical and organizational topics CTOs and senior technical architects who are devising an IT strategy that impacts the way the organization works IT managers who want to learn what’s worked and what hasn’t in large-scale transformationLearning Dapr: Building Distributed Cloud Native Applications&gt;Get the authoritative guide to Dapr, the distributed application runtime that works with new and existing programming languages alike. Written by the model&amp;;s creators, this introduction shows you how Dapr not only unifies stateless, stateful, and actor programming models but also runs everywhere&amp;;in the cloud or on the edge.Authors Haishi Bai and Yaron Schneider with Microsoft&amp;;s Azure CTO team explain that, with Dapr, you don&amp;;t need to include any SDKs or libraries in your user code. Instead, you automatically get flexible binding, state management, the actor pattern, pub-sub, reliable messaging, and many more features. This book shows developers, architects, CIOs, students, and computing enthusiasts how to get started with Dapr. Learn the new programming model for cloud native applications Write high-performance distributed applications without drilling into technical details Use Dapr with any language or framework to write microservices easily Learn how Dapr provides consistency and portability through open APIs and extensible, community-driven components Explore how Dapr handles state, resource bindings, and pub-sub messaging to enable resilient event-driven architectures that scale Integrate cloud applications with various SaaS offerings, such as machine learningPractical Microservices: Build Event-Driven Architectures with Event Sourcing and CQRS&gt;MVC and CRUD make software easier to write, but harder to change. Microservice-based architectures can help even the smallest of projects remain agile in the long term, but most tutorials meander in theory or completely miss the point of what it means to be microservice-based. Roll up your sleeves with real projects and learn the most important concepts of evented architectures. You’ll have your own deployable, testable project and a direction for where to go next.Much ink has been spilled on the topic of microservices, but all of this writing fails to accurately identity what makes a system a monolith, define what microservices are, or give complete, practical examples, so you’re probably left thinking they have nothing to offer you. You don’t have to be at Google or Facebook scale to benefit from a microservice-based architecture. Microservices will keep even small and medium teams productive by keeping the pieces of your system focused and decoupled.Discover the basics of message-based architectures, render the same state in different shapes to fit the task at hand, and learn what it is that makes something a monolith (it has nothing to do with how many machines you deploy to). Conserve resources by performing background jobs with microservices. Deploy specialized microservices for registration, authentication, payment processing, e-mail, and more. Tune your services by defining appropriate service boundaries. Deploy your services effectively for continuous integration. Master debugging techniques that work across different services. You’ll finish with a deployable system and skills you can apply to your current project.Add the responsiveness and flexibility of microservices to your project, no matter what the size or complexity.Fundamentals of Software Architecture: An Engineering Approach. A Comprehensive Guide to Patterns, Characteristics, and Best Practices&gt;Salary surveys worldwide regularly place software architect in the top 10 best jobs, yet no real guide exists to help developers become architects. Until now. This book provides the first comprehensive overview of software architecture&amp;;s many aspects. Aspiring and existing architects alike will examine architectural characteristics, architectural patterns, component determination, diagramming and presenting architecture, evolutionary architecture, and many other topics.Mark Richards and Neal Ford&amp;;hands-on practitioners who have taught software architecture classes professionally for years&amp;;focus on architecture principles that apply across all technology stacks. You&amp;;ll explore software architecture in a modern light, taking into account all the innovations of the past decade.This book examines: Architecture patterns: The technical basis for many architectural decisions Components: Identification, coupling, cohesion, partitioning, and granularity Soft skills: Effective team management, meetings, negotiation, presentations, and more Modernity: Engineering practices and operational approaches that have changed radically in the past few years Architecture as an engineering discipline: Repeatable results, metrics, and concrete valuations that add rigor to software architecture  Non It BooksTeam Topologies: Organizing Business and Technology Teams for Fast Flow Effective software teams are essential for any organization to deliver value continuously and sustainably. But how do you build the best team organization for your specific goals, culture, and needs? Team Topologies is a practical, step-by-step, adaptive model for organizational design and team interaction based on four fundamental team types and three team interaction patterns. It is a model that treats teams as the fundamental means of delivery, where team structures and communication pathways are able to evolve with technological and organizational maturity. In Team Topologies, IT consultants Matthew Skelton and Manuel Pais share secrets of successful team patterns and interactions to help readers choose and evolve the right team patterns for their organization, making sure to keep the software healthy and optimize value streams. Team Topologies is a major step forward in organizational design for software, presenting a well-defined way for teams to interact and interrelate that helps make the resulting software architecture clearer and more sustainable, turning inter-team problems into valuable signals for the self-steering organization. Team of Teams: New Rules of Engagement for a Complex WorldWhen General Stanley McChrystal took command of the Joint Special Operations Task Force in 2004, he quickly realized that conventional military tactics were failing. Al Qaeda in Iraq was a decentralized network that could move quickly, strike ruthlessly, then seemingly vanish into the local population. The allied forces had a huge advantage in numbers, equipment, and training—but none of that seemed to matter. To defeat Al Qaeda, they would have to combine the power of the world’s mightiest military with the agility of the world’s most fearsome terrorist network. They would have to become a “team of teams”—faster, flatter, and more flexible than ever.In Team of Teams, McChrystal and his colleagues show how the challenges they faced in Iraq can be rel­evant to countless businesses, nonprofits, and or­ganizations today. In periods of unprecedented crisis, leaders need practical management practices that can scale to thousands of people—and fast. By giving small groups the freedom to experiment and share what they learn across the entire organiza­tion, teams can respond more quickly, communicate more freely, and make better and faster decisions.Drawing on compelling examples—from NASA to hospital emergency rooms—Team of Teams makes the case for merging the power of a large corporation with the agility of a small team to transform any organization.Creativity Inc.: Overcoming the Unseen Forces That Stand in the Way of True InspirationCreativity, Inc. is a manual for anyone who strives for originality and the first-ever, all-access trip into the nerve center of Pixar Animation—into the meetings, postmortems, and “Braintrust” sessions where some of the most successful films in history are made. It is, at heart, a book about creativity—but it is also, as Pixar co-founder and president Ed Catmull writes, “an expression of the ideas that I believe make the best in us possible.”For nearly twenty years, Pixar has dominated the world of animation, producing such beloved films as the Toy Story trilogy, Monsters, Inc., Finding Nemo, The Incredibles, Up, WALL-E, _and _Inside Out, _which have gone on to set box-office records and garner thirty Academy Awards. The joyousness of the storytelling, the inventive plots, the emotional authenticity: In some ways, Pixar movies are an object lesson in what creativity really _is. Here, in this book, Catmull reveals the ideals and techniques that have made Pixar so widely admired—and so profitable.As a young man, Ed Catmull had a dream: to make the first computer-animated movie. He nurtured that dream as a Ph.D. student at the University of Utah, where many computer science pioneers got their start, and then forged a partnership with George Lucas that led, indirectly, to his co-founding Pixar in 1986. Nine years later, Toy Story was released, changing animation forever. The essential ingredient in that movie’s success—and in the thirteen movies that followed—was the unique environment that Catmull and his colleagues built at Pixar, based on leadership and management philosophies that protect the creative process and defy convention, such as: Give a good idea to a mediocre team, and they will screw it up. But give a mediocre idea to a great team, and they will either fix it or come up with something better. If you don’t strive to uncover what is unseen and understand its nature, you will be ill prepared to lead. It’s not the manager’s job to prevent risks. It’s the manager’s job to make it safe for others to take them. The cost of preventing errors is often far greater than the cost of fixing them. A company’s communication structure should not mirror its organizational structure. Everybody should be able to talk to anybody.Never Split the Difference: Negotiating As If Your Life Depended On ItA former international hostage negotiator for the FBI offers a new, field-tested approach to high-stakes negotiations―whether in the boardroom or at home.After a stint policing the rough streets of Kansas City, Missouri, Chris Voss joined the FBI, where his career as a hostage negotiator brought him face-to-face with a range of criminals, including bank robbers and terrorists. Reaching the pinnacle of his profession, he became the FBI’s lead international kidnapping negotiator. Never Split __the Difference takes you inside the world of high-stakes negotiations and into Voss’s head, revealing the skills that helped him and his colleagues succeed where it mattered most: saving lives. In this practical guide, he shares the nine effective principles―counterintuitive tactics and strategies―you too can use to become more persuasive in both your professional and personal life.Life is a series of negotiations you should be prepared for: buying a car, negotiating a salary, buying a home, renegotiating rent, deliberating with your partner. Taking emotional intelligence and intuition to the next level, Never Split the Difference gives you the competitive edge in any discussion.Getting to Yes: Negotiating Agreement Without Giving InThe key text on problem-solving negotiation-updated and revisedSince its original publication nearly thirty years ago, Getting to Yes has helped millions of people learn a better way to negotiate. One of the primary business texts of the modern era, it is based on the work of the Harvard Negotiation Project, a group that deals with all levels of negotiation and conflict resolution.Getting to Yes offers a proven, step-by-step strategy for coming to mutually acceptable agreements in every sort of conflict. Thoroughly updated and revised, it offers readers a straight- forward, universally applicable method for negotiating personal and professional disputes without getting angry-or getting taken.The Millionaire Fastlane: Crack the Code to Wealth and Live Rich for a Lifetime!Has the “settle-for-less” financial plan become your plan for wealth? That sounds something like this:Graduate from college, get a good job, save 10% of your paycheck, buy a used car, cancel the movie channels, quit drinking expensive Starbucks mocha lattes, save and penny-pinch your life away, trust your life-savings to Wall Street, and one day, when you are oh, say, 65 years old, you can retire rich.Since you were old enough to hold a job, you’ve been hoodwinked to believe that wealth can be created by blindly trusting in the uncontrollable and unpredictable markets: the housing market, the stock market, and the job market. This soul-sucking, dream-stealing dogma is “The Slowlane” – an impotent FINANCIAL GAMBLE that dubiously promises wealth in a wheelchair.Accept the Slowlane as your financial roadmap and your financial future will blow carelessly asunder on a sailboat of HOPE: HOPE you can get a job and keep it, HOPE the stock market doesn’t tank, HOPE for a robust economy, HOPE, HOPE, and HOPE. Is HOPE really the centerpiece of your family’s financial plan?Drive the Slowlane and you will find your life deteriorate into a miserable exhibition about what you cannot do, versus what you can. For those who don’t want a lifetime subscription to mediocrity, there is an alternative; an expressway to extraordinary wealth capable of burning a trail to financial freedom faster than any road out there. And shockingly, this road has nothing to do with jobs, 401(k), mutual funds, or a lifestyle of miserly living and 190 square foot tiny houses. Just some of what you will learn: Why jobs, 401(k)s, mutual funds, and 40-years of mindless frugality will never make you rich young. Why most entrepreneurs fail and how to immediately put the odds in your favor. The real law of wealth: Leverage this and wealth has no choice but to be magnetized to you. The leading cause of poorness: Change this and you change everything. How the rich really get rich – and no, it has nothing to do with a paycheck or a 401K match. The mathematics of wealth and how any “Joe Schmo” can tap into real wealth real fast. Why the guru’s sacred deities – compound interest and indexed funds – are impotent wealth accelerators. Why popular guru platitudes like “do what you love” and “follow your passion” will most likely keep you poor, not rich. And 250+ more poverty busting distinctions…Demand more. Change lanes and find your explosive wealth accelerator. Hit the Fastlane, crack the code to wealth, and find out how to live rich for a lifetime.Nonviolent Communication: A Language of Life: Life-Changing Tools for Healthy Relationships2,000,000 COPIES SOLD WORLDWIDE • TRANSLATED IN MORE THAN 35 LANGUAGESWhat is _Violent _Communication?If “violent” means acting in ways that result in hurt or harm, then much of how we communicate—judging others, bullying, having racial bias, blaming, finger pointing, discriminating, speaking without listening, criticizing others or ourselves, name-calling, reacting when angry, using political rhetoric, being defensive or judging who’s “good/bad” or what’s “right/wrong” with people—could indeed be called “violent communication.”What is _Nonviolent _Communication?Nonviolent Communication is the integration of four things:** Consciousness: a set of principles that support living a life of compassion, collaboration, courage, and authenticity&lt;7li&gt; Language: understanding how words contribute to connection or distance Communication: knowing how to ask for what we want, how to hear others even in disagreement, and how to move toward solutions that work for all Means of influence: sharing “power with others” rather than using “power over others” Nonviolent Communication serves our desire to do three things: Increase our ability to live with choice, meaning, and connection Connect empathically with self and others to have more satisfying relationships Sharing of resources so everyone is able to benefit IT Books " }, { "title": "Decorator Pattern in .NET Core 3.1", "url": "/decorator-pattern-in-net-core-3-1/", "categories": "Design Pattern", "tags": ".NET Core 3.1, C#, Software Architecture", "date": "2020-07-01 00:00:00 +0200", "snippet": "The decorator pattern is a structural design pattern used for dynamically adding behavior to a class without changing the class. You can use multiple decorators to extend the functionality of a class whereas each decorator focuses on a single-tasking, promoting separations of concerns. Decorator classes allow functionality to be added dynamically without changing the class thus respecting the open-closed principle.You can see this behavior on the UML diagram where the decorator implements the interface to extend the functionality. Decorator Pattern UML (Source) When to use the Decorator PatternThe decorator pattern should be used to extend classes without changing them. Mostly this pattern is used for cross-cutting concerns like logging or caching. Another use case is to modify data that is sent to or from a component.Decorator Pattern Implementation in ASP .NET Core 3.1You can find the code of the demo on GitHub.I created a DataService class with a GetData method which returns a list of ints. Inside the loop, I added a Thread.Sleep to slow down the data collection a bit to make it more real-world like.This method is called in the GetData action and then printed to the website. The first feature I want to add with a decorator is logging. To achieve that, I created the DataServiceLoggingDecorator class and implement the IDataService interface. In the GetData method, I add a stopwatch to measure how long collecting data takes and then log the time it took.Additionally, I want to add caching also using a decorator. To do that, I created the DataServiceCachingDecorator class and also implemented the IDataService interface. To cache the data, I use IMemoryCache and check the cache if it contains my data. If not, I load it and then add it to the cache. If the cache already has the data, I simply return it. The cache item is valid for 2 hours.All that is left to do is to register the service and decorator in the ConfigureServices method of the startup class with the following code:With everything in place, I can call the GetData method from the service which gets logged and the data placed in the cache. When I call the method again, the data will be loaded from the cache.Start the application and click on Get Data. After a couple of seconds, you will see the data displayed. Displaying the loaded data When you load the site again, the data will be displayed immediately due to the cache.ConclusionThe decorator pattern can be used to extend classes without changing the code. This helps you to achieve the open-closed principle and separation of concerns. Mostly the decorator pattern is used for cross-cutting concerns like caching and logging.You can find the code of the demo on GitHub." }, { "title": "Blazor Server vs. Blazor WebAssembly", "url": "/blazor-server-vs-blazor-webassembly/", "categories": "ASP.NET, Frontend", "tags": "Blazor Server, Blazor WebAssembly, C#, WASM, Blazor", "date": "2020-06-29 00:00:00 +0200", "snippet": "Blazor Server was release with .NET Core 3.0 in September 2019 and Blazor WebAssembly (WASM) was released in May 2020. Today, I will talk about the differences, when to use what version, and everything else you need to know about Blazor Server vs. Blazor WebAssembly.Blazor Server vs. Blazor WebAssembly FeaturesBefore, I go into the details of each version of Blazor, let’s have a look at their features.WebAssembly Hosting Model WASM runs in the browser on the client. The first request to the WASM application downloads the CLR, Assemblies, JavaScript, CSS (React and Angular work similar). It runs in the secure WASM sandbox. The Blazor Javascript handler accesses the DOM (Document Object Model).Server Hosting Model The C# code runs on the server. Javascript hooks are used to access the DOM. Binary messages are used to pass information between the browser and the server using SignalR. If something is changed the server sends back DOM update messages.SignalRSignalR is an integral part of Blazor and offers these features: It is free, open-source and a first-class citizen of .NET Core Sends async messages over persistent connections. Connections are two-way. Every client has its own connection. Azure SignalR Services offers a free tier. Example applications are chat applications or the car tracking in the Uber app.Blazor Server vs. Blazor WebAssembly Project TemplatesTo create both versions of Blazor you should have an up to date version of Visual Studio 2019. Blazor WASM was added in May 2020, whereas Blazor Server was included in the launch of VS 2019.You can find the code of the demo on GitHub.Blazor ServerIn Visual Studio create a new project and select Blazor App and then Blazor Server App. Create a new Blazor Server App The template creates a sample application where you can increase a counter and load data about the weather forecast. Start the application, open the Counter feature and click a couple of times on the button. The blazor.server.js Javascript intercepts the button click action and uses SignalR to send messages to the server where the code is executed. The blazor.server.js file is loaded in the _Host.cshtml file.You can see the messages sent to the server when you open the developer tools, go to the Network tab and click the button several times. SignalR messages between server and browser Blazor WebAssemblyIn Visual Studio create a new project and select Blazor App and then Blazor WebAssembly App. Create a new Blazor WebAssembly App Creating a WASM App creates three projects: client, server, and shared whereas shared is a .NET Standard project. Start the application, open the Network tab of the developer tools and you will see that 6.6 MB got downloaded. If you start the application in Release mode, only 2.3 MB are downloaded. Switching to Release turns on the linker and therefore only necessary files are sent. All the files which are sent to the client The Dotnet.wasm file is a webassembly file which contains a mono-based .net runtime.ScalabilityWASM doesn’t need scaling for the client part because it runs in the browser. You have to scale the server part in the same way you scale a classic web server. For the server part, Microsoft estimates that you need around 85 KB of RAM for every client. Microsoft tested how much a Blazor server can handle. The test is successful when the latency is below 200 milliseconds. To keep it close to a real-world application, the test was a button click per second. A server with 1 CPU and 3.5 GB ram could support 5k concurrent connections. A server with 4 CPU and 14 GB ram could support 20k concurrent connections.SignalR web sockets use port 80, which can be offloaded from the server to Azure SignalR Service which allows 100k  concurrent users and 100 million messages per day. The free offering allows 20 concurrent users and 20k messages per day.Pros and Cons Blazor ServerAdvantages Faster loading than WASM Access to secure resources like databases or cloud-based services Supports browsers which don’t support WASM like IE 11 C# code is not sent to the browserDisadvantages Extra latency due to sending data back and forth No offline support Scalability can be challenging Server required (serverless possible)Pros and Cons Blazor WebAssemblyAdvantages Faster UI Code When performance matters use WASM Offline support Can be distributed via CDN, no server required (except for API) Any .NET standard 2.0 C# can be runDisadvantages An API layer is required if you want to access secure resources Debugging still a bit limitedConclusionBlazor Server and WebAssembly application both have their advantages and disadvantages. If you want to serve a large number of users and don’t have secret code running, use WASM. It also offers offline support. Use Blazor Server if you need fast loading of the first page or if you have sensitive code that you don’t want to send to the browser.You can find the code of the demo on GitHub." }, { "title": "Proxy Pattern in .NET Core", "url": "/proxy-pattern-in-net-core/", "categories": "Design Pattern", "tags": "ASP.NET Core MVC, C#, GRPC, Software Architecture", "date": "2020-06-27 00:00:00 +0200", "snippet": "The proxy pattern belongs to the group of structural patterns and is often used for cross cutting-concerns. Proxies can be used to control access, logging, and security features. Another advantage is separation of concern (SoC) and loose coupling of your components.The proxy often acts as a substitute of a concrete object, as shown on the following UML diagram. Proxy Pattern UML (Source) Real-World ExamplesWhen you purchase something and pay with a check, the check is a proxy. It allows the seller to access your bank account and withdraw a certain amount. Another example is a network proxy. All requests go through this proxy and it blocks suspicious or malicious requests. Safe requests can pass through.In software developmen,t a proxy can be used to enable endless scrolling. The proxy pattern can be used to load content in the background while the user interface is rendered.When to use itProxies help to achieve separation of concern and the single responsible principle (SRP). They are often for cross-cutting concerns like caching, logging, or access control. The details are kept inside the proxy and therefore your classes are free of these details. Another advantage of the proxy pattern is loose coupling and DRY (don’t repeat yourself).Related PatternsThere are four patterns similar to the proxy pattern: Prototype Decorator Flyweight AdapterImplementation of the Proxy PatternYou can find the code of the demo on GitHub.There are four types of proxies: Remote Smart Protective VirtualRemote Proxy PatternThe remote proxy pattern is the most commonly used proxy and you might have already used it without knowing. Using a remote proxy allows you to hide all the details about the network and the communication from the client. The best example of a remote proxy in .NET Core is GRPC. The GRPC client works as a proxy and makes it look like you are calling a local method. Behind the scenes, the GRPC client is calling the GRPC server over the network. Another use case is to centralize the knowledge of the network details. That’s what Proto does in GRPC or also Swagger.Implementation of the Remote Proxy PatternTo demonstrate the remote proxy, I created a GRPC server using the default Visual Studio template. In my proxy class, I am calling the server with my name and get a greeting message back. The code looks like I am calling a local method because the proxy hides all the network implementation and details.Start both projects in my demo and navigate to /RemoteProxy/HelloMessage and you will see the message in your browser. Getting a message from the Remote Proxy Make sure that the server is running, otherwise, you will get an exception. I will write another post about GRPC soon and won’t go into any details about it here.Smart Proxy PatternThe smart proxy pattern helps to perform additional logic around resource access like caching, or locking shared resources, for example for file access. In the following example, the smart proxy will detect that there is already a lock around a file and reuse it.Implementation of the Smart ProxyIn this example, I am opening two file streams to the same file. Usually, this would throw an exception because the file is already in use. In my proxy, I will check if a FileStream is already in use and if so, return the existing one.The implementation is pretty simple:The OpenWrite method is implemented in my proxy where I check if a stream already exists. If so, I return the existing stream, if not, I return a new stream.Protective ProxyThe protective proxy manages access to resources and acts as a gatekeeper. This version eliminates repetitive security checks from client code and helps to achieve SOC, SRP, and DRY.Implementation of the Protective ProxyIn this example, I want to set up some access roles for a document. Only the author of the document is allowed to update its name and only a user with the editor role is allowed to review the document. The protective proxy checks the user roles and blocks any unauthorized action. The logic of the review and update method stay unchanged and they don’t even know anything about these security checks.To test the implementation, I have some test cases in the ProtectiveProxyTests class.Virtual Proxy PatternThe virtual proxy pattern is often used with expensive UI operations, for example when you have infinite scrolling like on the Facebook wall. The proxy is used as a placeholder and is also responsible for getting the data. Most of the time this proxy is used for lazy loading entities.ConclusionThis post described the proxy pattern and how to use its different versions. Before writing this post, I haven’t heard about the proxy and I don’t think that it is used too often. But I also believe that it is good to have heard about it and so you can use it whenever you need it.The code examples where take and inspired from this Pluralsight course.You can find the code of the demo on GitHub." }, { "title": "Blazor - Getting Started", "url": "/blazor-getting-started/", "categories": "ASP.NET, Frontend", "tags": ".NET Core, Blazor, C#, SignalR", "date": "2020-06-22 00:00:00 +0200", "snippet": "Blazor was introduced with .NET Core 3.0 and is a web UI single page application (SPA) framework. Developers write the code in HTML, CSS, C#, and optionally Javascript. Blazor was developed by Steve Sanderson and presented  in July 2018. The main advantage over Javascript frameworks is that you can run C# directly in the browser which should be appealing for enterprise developers who don’t want to deal with JavascriptBlazor FeaturesBlazor comes with the following features: Run C# code inside your browser as WebAssembly (WASM) Blazor is a mix of browser and Razor WASM is more locked down than Javascript and therefore more secure Messages between the browser and the backend are sent via SignalR There is no direct DOM access but you can interact with the DOM using Javascript Modern browser support WASM: Firefox, Chrome, Edge, Safari (IE 11 does not) Components can be shared via NugetThere are two different versions: Blazor Server and Blazor Webassembly. I will talk about the difference in more detail in my next post. For now, you only have to remember that the Server version runs on the server and sends code to the browser and the Webassembly version runs directly in the browser and calls APIs, like Angular or React apps.Create your First Blazor Server AppTo follow the demo you need Visual Studio 2019 and at least .NET Core 3.0. You can find the source of the demo on GitHub.To create a new Blazor App, select the Blazor template in Visual Studio. Select the Blazor App template After entering a name and location, select Blazor Server App, and click on Create. Create a Blazor Server App This creates the solution and also a demo application. Start the application and you will see a welcome screen and a menu on the left side. Click on Fetch data to see some information about the weather forecast or on Counter to see a button which increases the amount when you click it. The counter function of the demo app All these features are implemented with HTML, CSS, and C# only. No need for Javascript.Create a new ComponentOpen the Counter.razor file in the Pages folder and you can see the whole code for the implementation of the Counter feature.On the top, you have the page attribute which indicates the route. In this example, when you enter /counter, this page will be rendered. The logic of the functionality is in the code block and the button has an onclick event. This event calls the IncrementCount method which then increases the count of the currentCode variable.Create a new PageIn this section, I will add a new Blazor component with a dropdown menu, event handling, and binding of a variable.Right-click on the Pages folder and add a Blazor Component with the name DisplayProducts. Then add the following code:This code sets the route to products and displays a headline. In the code block, I override the OnInitialized method and create a class Product. The OnInitialized method is executed during the load of the page. You might know this from Webforms. Next, I add a dropdown menu and add each element of my Products list.As the last step, I add an onchange event to the dropdown menu and the code of the executed method in the code block. The selected value will be displayed below the dropdown menu. If you haven’t selected anything, the whole div tag won’t be displayed. My whole component looks like the following:Run the application, navigate to /products and you will see the dropdown menu. Select a product and it will be displayed below the dropdown. Testing my first Blazor Component ConclusionBlazor is a new frontend framework that enables developers to run C# in the browser. This might be a great asset for enterprises that already use .NET but don’t want to deal with Javascript frameworks.You can find the source of the demo on GitHub." }, { "title": "Azure Pipelines Task Library Encoding Problem", "url": "/azure-pipelines-task-library-encoding-problem/", "categories": "DevOps", "tags": "Azure DevOps, Azure Key Vault, Azure Pipelines Task Library", "date": "2020-06-19 00:00:00 +0200", "snippet": "For one of our customers, we use an Azure DevOps pipeline to read secrets from the Azure Key Vault and then write them into pipeline variables. During the operation, we encountered an encoding problem with the azure pipeline task library. The library encoded the percentage symbol during the read. Instead of Secret%Value, we got Secret%25Value. After writing the value into our pipeline variable, the value Secret%25Value was saved.The CD PipelineOur Azure DevOps CD pipeline reads all the secrets from an Azure Key Vault using the Azure Key Vault Task. Then we pass these secrets into a Powershell script and copy the values into new variables. You can see the tasks in question in the following code sample.We do this because our customer has several configurations for each environment. For example, there are secrets for test1WebServer, test1ImageServer, test1DbServer, test2Webserver and, so on in the key vault. We take these variables and write them into generic variables like WebServer, DbServer and, so on. This enables us to have one pipeline and the customer can use as many configurations as they want.Azure Pipelines Task Library Encoding ProblemThe problem we encountered is that the Azure Key Vault Task read the secrets but encoded the % sign. Instead of Secret%Value, we got Secret%25Value and therefore wrote Secret%25Value into our variable inside the Powershell. As a result, the deployment failed since our secret is not correct. After too many hours of searching, we found an issue on GitHub with the same Azure pipelines task library encoding problem.  You can find a GitHub issue here. The problem occurs in version 2.9.3. The Azure Key Vault task uses version 2.8.0. You can find the code on GitHub.Fixing the Azure Pipelines Task Library Encoding ProblemIn our Powershell script, we were using “azure-pipelines-task-lib”: “^2.8.0”. The ^ updates to all future versions, which is as of this writing 2.9.3. We use this to stay up to date without changing the version of all packages by hand. Unfortunately, the azure-pipelines-task-lib 2.8 is not compatible with 2.9.3 and therefore led to the encoding problem. Fixing the problem was as easy as removing the ^ and redeploying the task to the Azure marketplace.ConsequencesRight now the pipeline is working again but once Microsoft updates the Azure Key Vault task to use a new version of the azure-pipelines-task-lib, the pipeline will break again. When this happens we have to update the azure-pipelines-task-lib inside our Powershell script and it should work again.If you want to read more about CI/CD pipelines, read my article Create Automatic Builds for .NET and .NET Core Applications with Azure DevOps." }, { "title": "Cross Site Scripting (XSS) in ASP .NET Core", "url": "/cross-site-scripting-in-asp-net-core/", "categories": "ASP.NET", "tags": ".NET Core 3.1, ASP.NET Core MVC, C#, Javascript, OWASP Top 10, Security", "date": "2020-06-17 00:00:00 +0200", "snippet": "Cross Site Scripting (XSS) is an attack where attackers inject code into a website which is then executed. XSS is on place seven of the OWASP Top 10 list of 2017 but could be easily avoided. In this post, I will talk about the concepts of cross site scripting and how you can protect your application against these attacks.What is Cross Site ScriptingCross site scripting is the injection of malicious code in a web application, usually, Javascript but could also be CSS or HTML. When attackers manage to inject code into your web application, this code often gets also saved in a database. This means every user could be affected by this. For example, if an attacker manages to inject Javascript into the product name on Amazon. Every user who opens the infected product would load the malicious code.Consequences of XSS AttacksThere are many possible consequences for your users if your website got attacked by cross site scripting: Attackers could read your cookies and therefore gain access to your private accounts like social media or bank Users may be redirected to malicious sites Attackers could modify the layout of the website to lure users into unintentional actions Users could be annoyed which will lead to damage to your reputation and probably a loss of revenue Often used in combination with other attacks like cross site request forgery (CSRF)Best Practices against Cross Site Scripting AttacksPreventing XSS attacks is pretty simple if you follow these best practices: Validate every user input, either reject or sanitize unknown character, for example, &lt; or &gt; which can be used to create Test every input from an external source Use HttpOnly for cookies so it is not readable by Javascript (therefore an attacker can’t use Javascript to read your cookies) Use markdown instead of HTML editorsCross Site Scripting in ASP .NET CoreASP .NET Core Is already pretty safe out of the box due to automatically encoding HTML, for example &lt; gets encoded into &amp;lt. Let’s have a look at two examples where XSS attacks can happen and how to prevent them. You can find the code for the demo on GitHub.ASP .NET Core 3.1 DemoXSS can occur when you display text which a user entered. ASP .NET Core automatically encodes text when you use @Model, but displays the code as it if if you use @Html.Raw.Preventing XSS Attacks in formsThe following code creates a form where the user can enter his user name. The input is displayed once in a safe way and once in an unsafe way.When a user enters his user name everything is fine. But when an attacker enters Javascript, the Javascript will be executed when the text is rendered inside the unsafe outputtag. When you enter the following code as your name:and click submit, an alert windows will be displayed. The injected code got executed When you click on OK, the text will be rendered into the safe output line and nothing will be displayed in the unsafe output line because the browser interprets the Javascript. The Javascript is displayed as text in the safe output line Preventing XSS Attacks in Query ParametersAnother way to inject code is through query parameters. If your application ready query parameters but doesn’t sanitize them, Javascript in it will be executed. The following code contains two forms. When you click on the button a query parameter will be read and printed to an alert box.The first submit button will execute Javascript whereas the second one uses the JavaScriptEncode to encode the text first. To simulate an attack replace the value of the UserId with the following code and click enter:Click the submit button of the unsafe form and you will see two Javascript alerts. The first one saying “Saving user name for account with id: ” and then a second one saying “You got attacked”. The Javascript got executed When you click the submit button of the safe form, you will see the Javascript as text. The Javascript is displayed as text and not executed In reality, an attacker wouldn’t display an alert box but try to access your cookies or redirect you to a malicious website.ConclusionThis post showed what cross site scripting attacks are and how they can be executed. ASP .NET Core makes it very easy to prevent these attacks and to offer a great user experience to your users.You can find the code for the demo on GitHub." }, { "title": "Cross Site Request Forgery (CSRF) in ASP .NET Core", "url": "/cross-site-request-forgery-csrf-in-asp-net-core/", "categories": "ASP.NET", "tags": ".NET Core 3.1, ASP.NET Core MVC, C#, CSRF, OWASP Top 10, Security", "date": "2020-06-15 00:00:00 +0200", "snippet": "Cross Site Request Forgery, also known as session riding is an exploit where attackers trick users to send requests that they don’t know about and don’t want to do. It was on the OWASP Top 10 every year, except in 2017. Although it is not on the current list, it is still important that developers take care of it and don’t leave any vulnerabilities in their application. Today I will describe what Cross Site Request Forgery is and how it can be prevented in ASP .NET Core MVC using .NET Core 3.1What is Cross Site Request Forgery (CSRF)Attackers using cross site request forgery try to trick users to send malicious requests to a website that trusts the user. This is possible because websites trust the browser of a user using cookies. The goal of the attack is to use your identity cookie to execute requests. For example, you log in to your Facebook account. Facebook then sets a cookie in your browser to identify you. An attacker could now trick you to send a request to Facebook. This request could be sending a message to thousands of users promoting a malicious website.How can Attackers perform a Cross Site Request Forgery Attack?The most common way to perform a cross site request forgery attack is by luring users to a malicious website using phishing emails. Attacks send millions of emails claiming that you won something and that you have to click on a link to collect your price. On the malicious site, you are asked to click on a button to accept your prize. This button click then sends the malicious request.Requirements for an AttackTo successfully perform a cross site request forgery attack, the following requirements have to be met: The user must have visited the attacked site (Facebook in the example above) The user must visit the site of the attacker (usually via phishing emails)The attackers try to make money with these attacks, for example, promoting something or sending links to malicious websites.Preventing Cross Site Request Forgery AttacksThere are some simple tricks you can use to prevent CSRF attacks: Use SameSite cookies, at least lax but preferably strict Use antiforgery tokens Always use HTTP Post with formsHttpOnly is good practice if you don’t need to access the cookie using Javascript. Be aware that encrypting the cookie does not help against CSRF attacks.Using the SameSite SettingIn ASP .NET Core 2.1 and higher, you can use CookieOptions to set the SameSite attribute. Use at least lax but use strict wherever possible. To set the setting use the following code:Using Antiforgery TokensThe ASP .NET Core server uses two randomly generated antiforgery tokens. The first one is sent as a cookie and the second one is places as a hidden form field. When the form is submitted both tokens are sent back to the server. If a request does not send both tokens, the request is not accepted.The ASP .NET Core tag helper automatically includes the antiforgery token into a form field. You can create a form for a name using this code:When you look at the HTML code of the form, you can see the generated field for the token. The auto-generated antiforgery token in the form You can also see two cookies in your browser. In Chrome you can see the cookies when you open the Developer Tools (F12) and then click on Application and then on Cookies. The User Id and Anti Forgery Cookie The last step is to tell the server to check the antiforgery token. You can do this by using the ValidateAntiForgeryToken attribute on an action.ConclusionToday, I explained what cross site request forgery (CSRF) attacks are and how easy it is to protect your application against it using built-in ASP .NET Core functionality. If you want to read more about security risks, I recommend you to take a look at the OWAS Top 10." }, { "title": "Free Website Hosting with Azure", "url": "/free-website-hosting-with-azure/", "categories": "Cloud", "tags": "Azure, Azure Functions, Azure Static Web App, C#, Cosmos DB, React", "date": "2020-06-07 00:00:00 +0200", "snippet": "Last week, I talked about hosting your static website with Azure Static Web Apps. Today, I will extend this example using a free Cosmos DB for the website data and Azure Functions to retrieve them. This approach will give you free website hosting and global distribution of your website.You can find the demo code of the Azure Static Web App here and the code for the Azure Functions here.Azure Cosmos DBCosmos DB is a high-end NoSQL database that offers incredible speed and global distribution. Cosmos DB is way too comprehensive to talk about all the features here. I am using it because it offers a free tier which should give you enough compute resources for a static website.Create a Free Tier Cosmos DBIn the Azure Portal search for Azure Cosmos DB, select it and click on Create or select Azure Cosmos DB from the left panel and then click on Create Azure Cosmos DB account. Create a new Cosmos DB On the next page, select a resource group and make sure that the Free Tier Discount is applied. After filling out all information click on Review + create. Set up the free tier of the Cosmos DB The deployment will take around ten minutes.Add Data to the Cosmos DatabaseAfter the deployment is finished, navigate to the Data Explorer tab in your Cosmos DB account. Click on New Container and a new tab is opened on the right side. There enter a Database id, Container id, and Partition key and click OK. Create a new database and container in the Azure Cosmos DB Open the newly created database and the Products container and click on New Item. This opens an editor where you can add your products as JSON. Add data to the container Again, Azure Cosmos DB is too big to go into any details in this post. For the free hosting of your website, it is only important to know that I added the data for the website into the database. The next step is to edit the Azure Function so it doesn’t return a static list but uses the Azure Cosmos DB instead.Using an Azure Function to read Data from Cosmos DBI am re-using the Azure Function from my last post. If you don’t have any yet, create a new Azure Function with an HTTP trigger. To connect to the Cosmos DB, I am installing the Microsoft.Azure.Cosmos NuGet package and create a private variable with which I will access the data.Next, I create a method that will create a connection to the container in the database.To connect to the Azure Cosmos DB container, you have to enter your URI and primary key. You can find them in the Keys tab of your Cosmos DB account. Get the Uri and Primary Key of the Cosmos DB In the next method, I am creating an iterator that will return all my products. I add these products to a list and return the list. You can filter the query by providing a filter statement in the GetItemQueryIterator method.In the Run method of the Azure Function, I am calling both methods and convert the list to a JSON object before returning it.I keep the Product class as it is.Start the Azure Function, enter the URL displayed in the command line and you will see your previously entered data. Test the Azure Function locally The last step is to deploy the Azure Function. In my last post, I already imported the publish profile. Since nothing has changed, I can right-click on my project, select Publish and then Publish again. Publish the Azure Function Testing the Free Website Hosting ImplementationOpen the URL of your Azure Static Web App and the data from the Cosmos DB will be displayed. The data from the database is displayed in the React app ConclusionToday, I showed how to use Azure Cosmos DB, Azure Functions and Azure Static Web Apps to achieve free website hosting and also a global distribution of the website. You can find the demo code of the Azure Static Web App here and the code for the Azure Functions here.During Ignite in September 2020, Microsoft announced new features for Static Web Apps. From now on it is also possible to host Blazor apps and the connection with the Azure Function got improved a lot. You can find my post about it here." }, { "title": "Repository Pattern in .NET Core", "url": "/repository-pattern-net-core/", "categories": "Design Pattern", "tags": ".NET Core, C#, Entity Framework Core", "date": "2020-06-04 00:00:00 +0200", "snippet": "A couple of years ago, I wrote about the Repository and Unit of Work pattern. Although this post is quite old and not even .NET Core, I get many questions about it. Since the writing of the post, .NET core matured and I learned a lot about software development. Therefore, I wouldn’t implement the code as I did back then. Today, I will write about implementing .the repository pattern in .NET coreWhy I am changing the Repository Pattern in .NET CoreEntity Framework Core already serves as unit of work. Therefore you don’t have to implement it yourself. This makes your code a lot simpler and easier to understand.The Repository Pattern in .NET CoreFor the demo, I am creating a simple 3-tier application consisting of controller, services, and repositories. The repositories will be injected into the services using the built-in dependency injection. You can find the code for the demo on GitHub.In the data project, I have my models and repositories. I create a generic repository that takes a class and offers methods like get, add, or update.Implementing the RepositoriesThis repository can be used for most entities. In case one of your models needs more functionality, you can create a concrete repository that inherits from Repository. I created a ProductRepository which offers product-specific methods:The ProductRepository also offers all generic methods because its interface IProductRepository inherits from IRepository:The last step is to register the generic and concrete repositories in the Startup class.The first line registers the generic attributes. This means if you want to use it in the future with a new model, you don’t have to register anything else. The second and third line register the concrete implementation of the ProductRepository and CustomerRepository.Implementing Services which use the RepositoriesI implement two services, the CustomerService and the ProductService. Each service gets injected a repository. The ProductServices uses the IProductRepository and the CustomerService uses the ICustomerRepository;. Inside the services, you can implement whatever business logic your application needs. I implemented only simple calls to the repository but you could also have complex calculations and several repository calls in a single method.Implementing the Controller to test the ApplicationTo test the application, I implemented a really simple controller. The controllers offer for each service method a parameter-less get method and return whatever the service returned. Each controller gets the respective service injected.When you call the create customer action, a customer object in JSON should be returned. Test the creation of a customer Use the databaseIf you want to use the a database, you have to add your connection string in the appsettings.json file. My connection string looks like this:By default, I am using an in-memory database. This means that you don’t have to configure anything to test the applicationI also added an SQL script to create the database, tables and test data. You can find the script here.ConclusionIn today’s post, I gave my updated opinion on the repository pattern and simplified the solution compared to my post a couple of years ago. This solution uses entity framework core as unit of work and implements a generic repository that can be used for most of the operations. I also showed how to implement a specific repository, in case the generic repository can’t full fill your requirements. Implement your own unit of work object only if you need to control over your objects.You can find the code for the demo on GitHub." }, { "title": "Azure Static Web Apps", "url": "/azure-static-web-apps/", "categories": "Cloud, Programming", "tags": "Azure Function, C#, GitHub Action, React, Static Web Apps", "date": "2020-05-30 00:00:00 +0200", "snippet": "Azure Static Web Apps were announced at the Build conference this year and they allow you to host your static websites for free. You can use HTML, plain Javascript of front-end frameworks like Angular or React. The website retrieves data, if needed, from Azure Functions. A great feature of Azure Static Web Apps is that you don’t have to configure your CI/CD pipeline. This is done for you by Azure and GitHub using Git Actions.Today, I want to show how to create a React front-end and how to retrieve data from an Azure Function. You can find the code of the React demo here and the code of the Azure Function demo here.Azure FunctionsAzure Function is an event-driven serverless feature which can be used to automate workloads or connect different parts of an application. They can be flexible scaled and they offer different triggers, for example, run some code when a message is written to a queue when a web API call was made or many more. You can use different programming languages like Javascript, Node.js, or .NET Core.Azure Functions offer a consumption plan, where you pay what you used, a premium plan, or an App Service plan. I like the consumption plan because it gives you 400,000 GB/s for free every month. This should be more than enough for small projects.Creating an Azure Function in the Azure PortalTo create an Azure Function in the Azure portal, search for Function App, and click on Create. In the Basics tab of the creation process, enter your basic information like the resource group, the region, and the runtime. Create an Azure Function In the next screenshot, you can see a summary of all my provided information. Note that I selected Windows as my operating system. First I wanted to use Linux but for whatever reason, I couldn’t deploy to the Linux Azure Function. Reviewing the Azure Function before creating it Click on Create and the Azure Function deployment starts.Creating an Azure Function using Visual StudioTo create my Azure Function, I am using Visual Studio 2019. You can also use Visual Studio Code. If you want to use Javascript for your Azure Function, you even have to use VS Code since Visual Studio only supports C#. Create a new project and select the Azure Functions template. Select the Azure Function template In the next window select HTTP trigger and set the Authorization level to Anonymous. This configuration starts the code execution every time an HTTP request is received. Select HTTP trigger and set the Authorization level to Anonymous After the Azure Function is created, I change the code so it returns a JSON list of products:The Product class has the following properties:Start the application and a console window will appear telling you the URL of your function. Enter this URL into your browser and you should see the JSON list displayed. Testing the Azure Function locally Deployment of the Azure FunctionYou can deploy the Azure Function directly from within Visual Studio. You can deploy to an existing Azure Function or even create a new one. Since I already created one, I will deploy it to this one. To make things even easier, I will download the publish profile from the Function in the Azure portal by clicking on Get publish profile on the Overview tab. Download the publish profile of the Azure Function After you downloaded the publish profile, in Visual Studio right-click on your project and select Publish. This opens a new window, select Import Profile and then select your previously downloaded publish profile. Import the downloaded publish profile After the publish profile is imported, click on Publish and the Azure Function will be published to Azure.Configuring and Testing of the Azure FunctionIn the Azure portal, click on the Functions tab of your Azure Function. There you will see your previously deployed function. The published function appears in the Azure Portal under Functions Click on the Function (Function1 in my case) and then click on Test. This opens a new panel where you can send a request to your function to test its functionality. Call the Azure Function to test its functionality Click on Run and the JSON list of your products should be displayed in the Output tab. The function returns the JSON with products Next call the function from your browser. The URL is .azurewebsites.net/api/YourFunction. Enter the URL in your browser and you will see the JSON displayed. Test the function call in the browser If we create the React app now and try to call the Azure Function, it won’t work. The reason why it won’t work is that CORS is not configured and therefore the request will be blocked. The configure CORS, open then the CORS tab, and enter http://localhost:3000. This will be the URL of the React app during the development. Configure CORS I also tried * to allow all requests but it didn’t work for me.Create the React App which will be deployed using Static Web AppsYou should have basic knowledge of React. If you are new to React, take a look at the documentation to install node.js and npm.Open a new Powershell window and create a new react app with the following command:This will create a react project, named react-static-web-app. Go inside the project folder in Powershell and open Visual Studio Code with the following code:I will change the application to call my Azure Function and then display the returned list with Bootstrap cards. First, I create a new folder, called components, and create a new file inside this folder called products.js. Then I add the following code to the new file:This method takes a list of products and displays every item. The next step is to implement the Azure Function call in the App.js file.Lastly, I add the Bootstrap css file in the index.html file which is located in the Public folder.Open a new terminal in VS Code and start the application with:This automatically opens your browser and should display your product list. Testing the React app with the data from the Azure Function Check in the React app into GitHub and let’s deploy it with Static Web Apps.Create Static Web AppsIn the Azure portal, search for Static Web App and click on Create to start the deployment process. Create a Static Web App On the Basics tab enter a name, select a region, and select your GitHub repository and branch. Note the region is only the initial region for the deployment. After the deployment, Static Web Apps deploys your application globally to give your users the best possible user experience. Configure the Static Web App On the Build tab, I removed the Api location because I have my Azure Function already deployed. The Static Web Apps are still in preview and I couldn’t deploy a C# Azure Function because the build failed. The build demanded that the direction of the function must be set in the function.json file. With C#, you can’t edit the function.json file because it is created during the build. I was able to deploy a Javascript Azure Function using Static Web Apps though. Configure the location of the app and api On the following screenshot, you can see all my entered information. Click on Create and the deployment process starts. Overview of the Static Web App before creation After the deployment is finished, you can see the URL of the Static Web App and also a link to the created GitHub Action. Overview of the published Static Web App Click on GitHub Action runs and you will be redirected to the Action inside your GitHub repository. The Git Action is building your application Click on the CI/CD pipeline and you can see more detailed information. The Git Action run finished Before you can test your deployed React app, you have to enter its URL in the CORS settings of your Azure Function. You can find the URL on the overview tab of your Static Web Apps. Add the URL of the Static Web App to the CORS Setting of the Azure Function After you entered the URL in the CORS settings, call it in your browser and you should see your React app displaying the list of products. Testing the Static Web App with the Azure Function ConclusionStatic Web Apps are a great new feature to quickly deploy your static website and host it globally. The feature is still new and in preview therefore it is no surprise that everything is not working perfectly yet. Once all problems are solved, I think it will be a great tool for a simple website, especially since it is free.In my next post, I will show you how to host your website for free and also extend the Azure Function to be able to read data from a database.You can find the code of the React demo here and the code of the Azure Function demo here." }, { "title": "Set up Docker-Compose for ASP .NET Core 3.1 Microservices", "url": "/set-up-docker-compose-for-asp-net-core-3-1-microservices/", "categories": "Docker", "tags": ".NET Core 3.1, C#, CQRS, Docker, Docker-Compose, MediatR, Microservice, RabbitMQ, Swagger", "date": "2020-04-24 00:00:00 +0200", "snippet": "In my last post, I described how to dockerize my microservices. To start the whole application, you have to start both microservices and RabbitMQ. Today, I will add a docker-compose file which is a tool to easily run multi-container applications.This post is part of “Microservice Series - From Zero to Hero”.What is the Docker-Compose File?Docker-compose is a yml file in which you can set up all the containers your application needs to run. Simplified, it executes several docker run commands after each other. If you are used to docker run commands, the content of the compose file will look familiar. Let’s have a look at the content of the file:This file describes two images, rabbitmq, and customerapi. Let’s have a closer look at the customerapi definition: Ports: The container is listening to the ports 8000 and 8001 and redirects the request to the ports 80 and 443 inside the container. Environment: This section provides environment variables and their value to enable Kestrel to process SSL requests. Image: This specifies which image should be used. If it is not available locally, it will be downloaded from Dockerhub. Restart: Here you can configure the restart policy. This container is always restarting on failure. Other options are always and until-stopped. Depends on: This section specifies dependencies. It only specifies that the rabbitmq container has to be started before the customerapi container. It doesn’t guarantee that the container is already finished starting upTo get started with docker-compose, I highly recommend the website composerize.com. On this website, you can paste a docker run command and it will give you a compose file with your parameters.Start multiple Container with Docker-ComposeYou can find the code of  the finished demo on GitHub.To execute your compose file, open Powershell, and navigate to the location of your file. In my demo code, I am providing the compose file in the root folder. Once you navigated to your file, use the following command:The -d parameter executes the command detached. This means that the containers run in the background and don’t block your Powershell window. Start multiple container with docker-compose To prove that all three containers are running, use docker ps. Check all running container Another great feature of docker-compose is, that you can stop all your applications with a single command:Build and run ContainersYou don’t have to use images from Dockerhub in your compose file, you can also build images and then run them. To build an image, use the build section and set the context to the location of the Dockerfile. I have created a new Dockerfile, called Dockerfile.Build which looks like the original one except that it doesn’t contain any tests or anything that might slow down the build.I named this file docker-compose.Build. You can use the -f parameter to specify the file in your docker-compose command: Build images with docker-compose How can Containers talk to each other?When starting multiple containers with a compose file, a default network is created in which all containers are placed. Containers can reach each other with the container name. For example, the CustomerApi can send data to the queue using the name Rabbitmq.ConclusionIn today’s post, I talked about docker-compose and how it can be used to easily set up applications with multiple containers.With this post, most of the features of the two microservices are implemented. With my next post, I will start to focus more on the DevOps process. This means that I will create CI pipelines, run tests automatically during pull requests, and later on will automatically deploy the microservices.You can find the code of  the finished demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "ASP .NET Core with HTTPS in Docker", "url": "/asp-net-core-with-https-in-docker/", "categories": "Docker, ASP.NET", "tags": ".NET Core 3.1, C#, CQRS, Docker, Docker-Compose, MediatR, Microservice, RabbitMQ, SSL, Swagger", "date": "2020-04-22 00:00:00 +0200", "snippet": "In my last post, I dockerized my ASP .NET Core 3.1 microservices but the HTTPS connection didn’t work. Kestrel needs a certificate to process HTTPS requests. Today, I will show you how to create a development certificate and how to provide it to your Docker container so you can use ASP .NET Core with HTTPS in Docker.Start a Docker Container without a CertificateBefore I start, I want to show you what happens when you try to start a .NET Core application without a valid certificate. I use the following command to start the container:This command sets a port mapping, adds an environment variable and starts the image customerapi from my Dockerhub repository. Executing this command will result in the following exception: Start a .NET Core application without a certificate As you can see, Kestrel can’t start because no certificate was specified and no developer certificate could be found. When you start your application inside a Docker container within Visual Studio, Visual Studio manages the certificate for you. But without Visual Studio, you have to create the certificate yourself.Creating a Certificate to use ASP .NET Core with HTTPS in DockerYou can create a certificate with the following command: dotnet dev-certs https -ep [Path of the certificate]-p [Password]. I create the certificate under D:\\temp and set Password as its password. Creating the certificate Note that you must set a password. Otherwise, Kestrel won’t be able to use the certificate.Provide the Certificate to the Docker ImageAfter creating the certificate, you only have to share it with your container and the .NET Core application should start. I use the following command: Start a .NET Core application and provide a certificate When you open https://localhost:32788, you should see the Swagger UI. Testing the application with HTTPS Explaining the Docker ParameterIn this section, I will shortly explain the used parameter from the example above. -p maps the container from the host inside the container. -p 32789:80 means that the container is listening on port 32789 and redirects it to port 80 inside the application. -e is used to provide an environment variable. Kestrel__Certificates__Default__Path tells the application where the certificate is located and Kestrel__Certificates__Default__Password tells the application the password of the certificate. “ASPNETCORE_URLS=https://+;http://+” tells Kestrel to listen for HTTP and HTTPS requests. -v creates a volume that allows you to share files from your computer with the container.Using the Certificate in the ContainerI created a certificate and copied it into the container during the build. To do that you have to remove .pfx from the .gitignore file. Note that you should never share your certificate or put it inside a container. I only did it to simplify this demo. To use the certificate inside the container, use the following command:ConclusionThis post showed how to create a certificate and how to provide it to your application inside a Docker container. This enables you to use ASP .NET Coree with HTTPS in Docker.In my next post, I will create a docker-compose file which will help you to start both microservices and RabbitMQ with a single command.You can find the code of  the finished demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Dockerize an ASP .NET Core Microservice and RabbitMQ", "url": "/dockerize-an-asp-net-core-microservice-and-rabbitmq/", "categories": "Docker, ASP.NET", "tags": ".NET Core 3.1, C#, CQRS, Docker, Docker-Compose, MediatR, Microservice, RabbitMQ, Swagger", "date": "2020-04-21 00:00:00 +0200", "snippet": "In my last post, I added RabbitMQ to my two microservices which finished all the functional requirements. Microservices became so popular because they can be easily deployed using Docker. Today I will dockerize my microservices and create a Docker container that can be run anywhere as long as Docker is installed. I will explain most of the Docker commands but basic knowledge about starting and stopping containers is recommended.This post is part of “Microservice Series - From Zero to Hero”.What is Docker?Docker is the most popular container technology. It is written in Go and open-source. A container can contain a Windows or Linux application and will always be identical bit by bit, no matter where you start it. This means it runs the same way during development, on the testing environment, and on the production environment. This eliminates the famous “It works on my machine”.Another big advantage is that Docker containers share the host system kernel. This makes them way smaller than a virtual machine and enables them to start within seconds or even less. For more information about Docker, check out Docker.com. There you can also download Docker Desktop which you will need to run a Docker container on your machine.What is Dockerhub?Dockerhub is like GitHub for Docker containers. You can sign up for free and get unlimited public repos and one private repo. There are also enterprise plans which give you more private repos, build pipelines for your containers, and security scanning.To dockerize an application means that you create a Docker container or at least a Dockerfile which describes how to create the container. You can upload the so-called container image to container registries like Dockerhub so other developers can easily download and run them.Dockerhub is the go-to place if you want to download official container images. The RabbitMQ from the last post was downloaded from there or you can download Redis, SQL Server from Microsoft, or thousands of other popular applications.Dockerize the MicroservicesYou can find the code of  the finished demo on GitHub.Visual Studio makes it super easy to dockerize your application. All you have to do is to right-click on the API project and then select Add –&gt; Docker Support. Dockerize your application This opens a new window where you can select Linux or Windows as OS for the container. I am always going for Linux as my default choice because the image is way smaller than Windows ones and therefore starts faster. Also, all my other containers run on Linux and on Docker Desktop, you can only run containers with the same OS at a time. After clicking OK, the Dockerfile and .dockerignore files are added. That’s all you have to do to dockerize the application.Dockerfile and .dockerignore FilesThe .dockerignore file is like the .gitignore file and contains extensions and paths which should not be copied into the container. Default extensions in the .dockerignore file are .vs, /bin or /obj. The .dockerignore file is not required to dockerize your application but is highly recommended. Content of the .dockerignore file The Dockerfile is a set of instructions to build and run an image. Visual Studio creates a multi-stage Dockerfile which means that it builds the application but only adds necessary files and images to the container image. The Dockerfile uses the .NET Core SDK to build the image but uses the way smaller .NET Core runtime image inside of the container. Let’s take a look at the different stages of the Dockerfile.Understanding the multi-stage DockerfileThe first part downloads the .NET Core 3.1 runtime image from Dockerhub and gives it the name base which will be used later on. Then it sets the working directory to /app which will also be later used. Lastly, the ports 80 and 443 are exposed which tells Docker to listen to these two ports when the container is running.The next section downloads the .NET Core 3.1 SDK from Dockerhub and names it build. Then the working directory is set to /src and all project files (except the test projects) of the solution are copied inside the container. Then dotnet restore is executed to restore all NuGet packages and the working directory is changed to the directory of the API project. Note that the path starts with /src, the working directory path I set before I copied the files inside the container. Lastly, dotnet build is executed which builds the project with the Release configuration into the path /app/build.The build image in the first line of the next section is the SDK image that we downloaded before and named build. We use it to run dotnet publish which publishes the CustomerApi project.The last section uses the runtime image and sets the working directory to /app. Then the published files from the last step are copied into the working directory. The dot means that it is copied to your current location, therefore /app. The Entrypoint command tells Docker that the container contains a .NET application and that it should run the CustomerApi.dll when the container starts.For more details on the Dockerfile and .dockerignore file check out the official documentation.Test the dockerized ApplicationAfter adding the Docker support to your application, you should be able to select Docker as a startup option in Visual Studio. When you select Docker for the first time, Visual Studio will run the Dockerfile, therefore build and create the container. This might take a bit because the images for the .NET Core runtime and SDK need to be downloaded. After the first download, they are cached and can be quickly reused. Select Docker as startup option Click F5 or on the Docker button and your application should start as you are used to. If you don’t believe me that it is running inside a Docker container, you can check the running containers in PowerShell with the command docker ps. Check the running containers The screenshot above shows that the customerapi image was started two minutes ago, that it is running for two minutes, and that it maps the port 32789 to port 80 and 32788 to 433. To stop a running container, you can use docker stop [id]. In my case. this would be docker stop f25727f43d6b. You don’t have to use the full id, like in git. Docker only needs to clearly identify the image you want to stop. So you could use docker stop f25.Build the Dockerfile without Visual StudioYou don’t need Visual Studio to create a Docker image. This is useful when you want to create the image and then push it to a container registry like Dockerhub. You should always do this in a build pipeline but it’s useful to know how to do it by hand and sometimes you need it to quickly test something.Open Powershell or bash and navigate to the folder containing the CustomerApi.sln file. To build an image, you can use docker build [build context] [location of Dockerfile]. Optionally, you can add a tag by using -t tagname. Useto build the Dockerfile which is in your current folder with the tag name customerapi. This will download the needed images (or use them from the cache) and start to build your image. Build the docker image To confirm that your image was really created, use docker images. Confirm that the image was created Start the newly built ImageTo start an image use docker run [-p “port outside of the container”:”port inside the container”] name of the image to start. In my example: Run the previously created image After the container is started, open localhost:32789 and you should see the Swagger UI of the API. If you use the HTTP port, you will get a connection closed error. HTTPS is currently not working because we have to provide a certificate so kestrel can process HTTPS requests. I will explain in my next post how to add a certificate to the container. For now, I will only use the HTTP port.Push the Image to DockerhubWe confirmed that the image is running, and now it is time to share it and therefore upload it to Dockerhub. Dockerhub is the default registry in Docker Desktop. Use “docker login” to login into your Dockerhub account. Log in into Dockerhub Next, I have to tag the image I want to upload with the name of my Dockerhub account and the name of the repository I want to use. I do this with docker tag Image DockerhubAccount/repository.The last step is to push the image to Dockerhub using “docker push tagname”. Push the image to Dockerhub To confirm that the image was pushed to Dockerhub, I open my repositories and see the newly create customerapi there. Confirm that the image was pushed to Dockerhub Testing the uploaded ImageTo confirm that everything worked fine, I will download the image and run it on any machine. The only requirement is that Docker is installed. When you click on the repository, you can see the command to download the image. In my example, this is docker pull wolfgangofner/customerapi. I will use docker run because this runs the image and if it is not available, automatically pull it too. Run the previously uploaded image Open localhost:32789 and the Swagger UI will appear.For practice purposes, dockerize the OrderApi. The steps are identical to the steps for the CustomerApi.ConclusionToday, I showed how to dockerize the microservices to create immutable Docker images which I can easily share using Dockerhub and run everywhere the same way. Currently, only the HTTP port of the application works because we haven’t provided an SSL certificate to process HTTPS requests. In my next post, I will create a development certificate and start the image with it.You can find the code of  the finished demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "RabbitMQ in an ASP .NET Core 3.1 Microservice", "url": "/rabbitmq-in-an-asp-net-core-3-1-microservice/", "categories": "Docker, ASP.NET", "tags": ".NET Core 3.1, C#, CQRS, Docker, Docker-Compose, MediatR, Microservice, RabbitMQ, Swagger", "date": "2020-04-18 00:00:00 +0200", "snippet": "In my last posts, I created two microservices using ASP .NET Core 3.1. Today, I will implement RabbitMQ, so the microservices can exchange data while staying independent. RabbitMQ can also be used to publish data even without knowing the subscribers. This means that you can publish an update and whoever is interested can get the new information.This post is part of “Microservice Series - From Zero to Hero”.What is RabbitMQ and why use it?RabbitMQ describes itself as the most widely deployed open-source message broker. It is easy to implement and supports a wide variety of technologies like Docker, .NET, or Go. It also offers plugins for monitoring, managing, or authentication. I chose RabbitMQ because it is well known, quickly implemented, and especially can be easily run using Docker.Why use a Queue to send Data?Now that you know what RabbitMQ is, the next question is: why should you use a queue instead of directly sending data from one microservice to the other one. There are a couple of reasons why using a queue instead of directly sending data is better: Higher availability and better error handling Better scalability Share data with whoever wants/needs it Better user experience due to asynchronous processingHigher Availability and better Error HandlingErrors happen all the time. In the past, we designed our systems to avoid errors. Nowadays we design our systems to catch errors and handle them in a user-friendly way.Let’s say we have an online shop and the order service sends data to the process order service after the customer placed his order. If these services are connected directly and the process order service is offline, the customer will get an error message, for example, “An error occurred. Please try it again later”. This user probably won’t return and you lost the revenue of the order and a potential future customer.If the order service places the order in a queue and the order processing service is offline, the customer will get a message that the order got placed and he or she might come back in the future. When the order processing service is back online, it will process all entries in the queue. You might know this behavior when booking a flight. After booking the flight it takes a couple of minutes until you get your confirmation per email.Better ScalabilityWhen you place messages in a queue, you can start new instances of your processor depending on the queue size. For example, you have one processor running, when there are ten items in the queue, you start another one, and so on. Nowadays with cloud technologies and serverless features, you can easily scale up to thousands of instances of your processor.Share Data with whoever wants/needs itMost of the time, you don’t know who wants to process the information you have.Let’s say our order service publishes the order to the queue. Then the order processing service, reporting services, and logistics services can process the data. Your service as a publisher doesn’t care who takes the information. This is especially useful when you have a new service in the future which wants your order data too. This service only has to read the data from the queue. If your publisher service sends the data directly to the other services, you would have to implement each call and change your service every time a new service wants the data.Better User Experience due to asynchronous processingBetter user experience is the result of the three other advantages. The user is way less likely to see an error message and even when there is a lot of traffic like on Black Friday, your system can perform well due to the scalability and the asynchronous processing.Implement RabbitMQ with .NET CoreYou can find the code of  the finished demo on GitHub.RabbitMQ has a lot of features. I will only implement a simple version of it to publish data to a queue in the Customer service and to process the data in the Order service.Implementation of publishing dataI am a big fan of Separation of Concerns (SoC) and therefore I am creating a new project in the CustomerApi solution called CustomerApi.Message.Send. Next, I install the RabbitMQ.Client NuGet package and create the class CustomerUpdateSender in the project. I want to publish my Customer object to the queue every time the customer is updated. Therefore, I create the SendCustomer method which takes a Customer object as a parameter.Publish Data to RabbitMQPublishing data to the queue is pretty simple. First, you have to create a connection to RabbitMQ using its hostname, a username, and a password using the ConnectionFactory. With this connection, you can use QueueDeclare to create a new queue if it doesn’t exist yet. The QueueDeclare method takes a couple of parameters like a name and whether the queue is durable.After creating the connection to the queue, I convert my customer object to a JSON object using JsonConvert and then encode this JSON to UTF8.The last step is to publish the previously generated byte array using BasicPublish. BasicPublish has like QueueDeclare a couple of useful parameters but to keep it simple, I only provide the queue name and my byte array.That’s all the logic you need to publish data to your queue. Before I can use it, I have to do two more things though. First, I have to register my CustomerUpdateSender class in the Startup class. I am also providing the settings for the queue such as the name or user from the appsettings. Therefore, I have to read this section in the Startup class.The last step is to call the SendCustomer method when a customer is updated. This call is in the Handle method of the UpdateCustomerCommandHandler.Note: I updated the implementation on November 27, 2020. Instead of creating a connection every time the method is called, I reuse the connection and only create a new channel. This follows the RabbitMQ best practices and helps to improve the performance significantly. I am not reusing the channel since I don’t need the highest performance and I want to keep the implementation simple. The new code looks as follows:Implementation of reading Data from RabbitMQImplementing the read functionality is a bit more complex because we have to constantly check the queue if there are new messages and if so, process them. I love .NET Core and it comes really handy here. .NET Core provides the abstract class BackgroundService which provides the method ExecuteAsync. This method can be overridden and is executed regularly in the background.In the OrderApi solution, I create a new project called OrderApi.Messaging.Receive, install the RabbitMQ.Client NuGet and create a class called CustomerFullNameUpdateReceiver. This class inherits from the BackgroundService class and overrides the ExecuteAsync method.In the constructor of the class, I initialize my queue the same way as in the CustomerApi using QueueDeclere. Additionally, I register events that I won’t implement now but might be useful in the future.Reading Data from the QueueIn the ExecuteAsync method, I am subscribing to the receive event and whenever this event is fired, I am reading the message and encode its body which is my Customer object. Then I am using this Customer object to call another service that will do the update in the database.That’s all you have to do to read data from the queue. The last thing I have to do is to register my CustomerFullNameUpdateReceiver class as a background service in the Startup class.The code above checks whether the setting RabbitMq:Enables is true. If it is set to true, the background service gets registered. This check is useful because so you can either use the appsettings.json file or provide an environment variable to override it.Run RabbitMQ in DockerThe publish and receive functionalities are implemented. The last step before testing them is to start an instance of RabbitMQ. The easiest way is to run it in a Docker container. If you don’t know what Docker is or haven’t installed it, download Docker Desktop for Windows from here or for Mac from here. After you installed Docker, copy the two following lines in Powershell or bash:Don’t worry if you don’t understand them. Simplified these two lines download the RabbitMQ Docker image, start it as a container and configure the ports, the name, and the credentials. Run RabbitMQ in a Docker container After RabbitMQ is started, you can navigate to localhost:15672 and login with guest as user and guest as password. Login into the RabbitMQ management portal Navigate to the Queues tab and you will see that there is no queue yet. A message was published and consumed Now you can start the OrderApi and the CustomerApi project. The order of how you start them doesn’t matter. After you started the OrderApi project, the CustomerQueue will be created and you can see it in the management portal. No queues are created yet Click on CustomerQueue and you will see that there is no message in the queue yet and that there is one consumer (the OrderApi). Overview of the CustomerQueue and its Consumers Go to the Put action of the CustomerApi and update a customer. If you use my in-memory database you can use the Guid “9f35b48d-cb87-4783-bfdb-21e36012930a”. The other values don’t matter for this test. Update a customer After you sent the update request, go back to the RabbitMQ management portal and you will see that a message was published to the queue and also a message was consumed. A message was published and consumed Shortcomings of my ImplementationIn the CustomerApi, there is no real exception handling right now. This means that if there is an error while processing a message, the message will be deleted from the queue and therefore be lost. Also if there is no connection to RabbitMQ, the message will be discarded and therefore lost. In a production environment, you should save this message somewhere and process it once all systems are back up and running.Another problem is that after the message is read, it is removed from the queue. This means only one receiver is possible at the moment. There are also no unit tests for the implementation of the RabbitMQ client.ConclusionThis post explained why you should use queues to decouple your microservices and how to implement RabbitMQ using Docker and ASP .NET Core 3.1. Keep in mind that this is a quick implementation and needs some work to be production-ready.In my next post, I will dockerize the application which will make it way easier to run and distribute the whole application.You can find the code of  the finished demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”.Update: On November 27 2020, I refactored the registration of the service to make it more resilient and also change the implementation of the sender to reuse the connection instead of creating a new one for every message." }, { "title": "Mediator Pattern in ASP .NET Core 3.1", "url": "/mediator-pattern-in-asp-net-core-3-1/", "categories": "Design Pattern, ASP.NET", "tags": ".NET Core 3.1, C#, CQRS, Docker, Docker-Compose, Mediator, MediatR, Microservice, RabbitMQ, Swagger", "date": "2020-04-17 00:00:00 +0200", "snippet": "The mediator pattern is a behavioral design pattern that helps to reduce chaotic dependencies between objects. The main goal is to disallow direct communication between the objects and instead force them to communicate only via the mediator.This post is part of “Microservice Series - From Zero to Hero”.ProblemServices or classes often have several dependencies on other classes and you quickly end up with a big chaos of dependencies. The mediator pattern serves as an organizer and calls all needed services. No service has a dependency on another one, only on the mediator.You can see the mediator pattern also in real life. Think about a big airport like JFK with many arriving and departing planes. They all need to be coordinated to avoid crashes. It would be impossible for a plan to talk to all other planes. Instead, they call the tower, their mediator, and the tower talks to all planes and organizes who goes where.Advantages and Disadvantages of the Mediator PatternThe mediator pattern brings a couple of advantages: Less coupling: Since the classes don’t have dependencies on each other, they are less coupled. Easier reuse: Fewer dependencies also help to reuse classes. Single Responsibility Principle: The services don’t have any logic to call other services, therefore they only do one thing. Open/closed principle: Adding new mediators can be done without changing the existing code.There is also one big disadvantage of the mediator pattern: The mediator can become such a crucial factor in your application that it is called a “god class”.Implementation of the Mediator PatternYou can find the code of  the finished demo on GitHub.To use the mediator pattern in .NET, I am using the MediatR NuGet package, which helps to call the services.Installing MediatRI am installing the MediatR and the MediatR.Extensions.Microsoft.DependencyInjection in my Api project. In the Startup class, I registered my mediators using:I can do this because the controllers are in the same project. In the OrderApi, I am also using the ICustomerNameUpdateService interface as a mediator. Therefore, I also have to register it.Now, I can use the IMediator object with dependency injection in my controllers.Using the Mediator patternEvery call consists of a request and a handler. The request is sent to the handler which processes this request. A request could be a new object which should be saved in the database or an id of an object which should be retrieved. I am using CQRS, therefore my requests are either a query for read operations or a command for a write operation.In the OrderController, I have the Order method which will create a new Order object. To create the Order, I create a CreateOrderCommand and map the Order from the post request to the Order of the CreateOrderCommandObject. Then I use the Send method of the mediator.The request (or query and command in my case) inherit from the IRequest&lt;T&gt; interface where T indicates the return value. If you don’t have a return value, then inherit from IRequest.The send method sends the object to the CreateOrderCommmandHandler. The handler inherits from IRequestHandler&lt;TRequest, TResponse&gt; and implements a Handle method. This Handle method processes the CreateOrderCommand. In this case, it calls the AddAsync method of the repository and passes the Order.If you don’t have a return value, the handler inherits from IRequestHandler&lt;TRequest&gt;.ConclusionThe mediator pattern is a great pattern to reduce the dependencies within your application which helps you to reuse your components and also to keep the Single Responsible Principle. I showed I implemented it in my ASP .NET Core 3.1 microservices using the MediatR NuGet package.In my next post, I will implement RabbitMQ which enables my microservices to exchange data in a decoupled asynchronous way.You can find the code of the finished demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "CQRS in ASP .NET Core 3.1", "url": "/cqrs-in-asp-net-core-3-1/", "categories": "Design Pattern, ASP.NET", "tags": ".NET Core 3.1, C#, CQRS, Docker, Docker-Compose, Mediator, MediatR, Microservice, RabbitMQ, Swagger", "date": "2020-04-16 00:00:00 +0200", "snippet": "CQRS stands for Command Query Responsibility Segregation and is used to use different models for read and for write operations. In this post, I will explain how I implemented CQRS in my microservices and how to use the mediator pattern with it to get even more abstraction.You can find the code of  the finished demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”.What is CQRS?CQRS or Command Query Responsibility Segregation is a design pattern to separate the read and write processes of your application. Read operations are called Queries and write operations are called Commands. Open one of the two microservices and you will see in the service project two folders, Command and Query. Inside the folder, you can see a handler and a command or query. They are used for the mediator, which I will describe in my next post. Operations are split in Commands and Queries In the following examples, you will see that CQRS is simpler than it sounds. Simplified it is only a split of the read and write operations in different classes.Taking a look at a QueryIn the CustomerApi solution, you can find the GetCustomerByIdQueryHandler inside the service project. Since this class is a query, it is used to read data. Inside the class is a Handle method, which calls the repository to get the first customer where the id matches the passed id.Taking a look at a CommandIn the CustomerApi solution, you can find the CreateCustomerCommandHandler inside the service project. This class also has a Handle method but this time it executes a write operation.Advantages of CQRSCQRS offers the following advantages: Separation of Concern, therefore simpler classes and models Better scalability since you can have a microservice only for queries and one only for commands. Reads occur often way more often than writes. Better performance as you can use a database for reading and a database for writing. You could also use a fast cache like Redis for the reading. Event sourcing: it is not part of CQRS but they are often used together. Event sourcing is a collection of events that enables you to have the exact state of an object at any time.Disadvantages of CQRSAs always, CQRS also comes with some downside: More complexity especially in bigger systems because often you have reads which also update some data, for example, a user logs in (read) and you want to store its IP and time of login (write). Eventual consistency when using a database for writing and one for reading. The read database needs to be synchronized to hold the new data. This could take a while. Not applicable in all projects: CQRS brings some complexity to your system and especially simple applications that do only basic CRUD operations shouldn’t use CQRS.ConclusionThis post gave a short overview of CQRS and how it can be used to separate the read and write operations in your application. In my demo code, I only use it to separate these operations but you could put the queries and commands in different solutions that allow you to independently scale them.In my next post, I will describe the mediator pattern and how I use it to remove dependencies between commands and queries.You can find the code of  the finished demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Document your Microservice with Swagger", "url": "/document-your-microservice-with-swagger/", "categories": "ASP.NET", "tags": ".NET Core 3.1, C#, CQRS, Docker, Docker-Compose, MediatR, Microservice, RabbitMQ, Swagger", "date": "2020-04-15 00:00:00 +0200", "snippet": "Swagger is an open-source toolset that can be easily integrated into your solution and helps you to document and test your APIs. It is so simple that even non-technical people can use it.In my last post, I created two microservices and today I will explain how to integrate Swagger.This post is part of “Microservice Series - From Zero to Hero”.What is Swagger?Swagger is an open-source toolset from Smartbear and is based on the OpenAPI specificiation. For .NET Core, you can install Swagger easily as a NuGet package and configure it in the Startup class of your solution. In simple words, Swagger wraps the XML comments of your API in a nice-looking functional GUI which shows you the available actions and models and also lets you send requests to the actions. You can even attach authentication objects like JWT.Install SwaggerYou can find the code of  the finished demo on GitHub.To implement Swagger, I installed the Swashbuckle.AspNetCore NuGet package in the API project. Next, I added the path to the XML file which contains all the XML comments of the actions and models in the ConfigureServices method in the Startup class.The next step is to tell ASP .NET Core to use Swagger and its UI. You can add both in the Configure method of the Startup class. Additionally, I configured Swagger to load the UI when starting your solution.That’s all you have to configure in Swagger. Now I only have to make two adjustments to the starting project. First, I tell the project to create the XML file by opening the properties of the project. Go to the Build tab and check “XML documentation file”. It is important that you use All Configurations as Configuration in the dropdown on top. Create the XML documentation file After you configured your project to create the XML documentation file, you will get warnings that an XML comment is missing on top of your actions. I find these warnings pretty annoying, so I suppress them by adding 1591 in the text box for “Suppress warnings” in the Build tab (see screenshot above).The last step is to remove the launch URL from the launchSettings.json file. You can just remove the line, otherwise, the Swagger UI won’t be loaded when the application is started and you have to call its URL manually.That’s all you have to do to set up Swagger. Before I test it, I will add some XML comments to the actions, attributes to my model, and some more configurations.Adding XML comments to API ActionsThe XML comment on an action describes what the action does, what the parameters are, what it returns and what return codes it can produce. Usually, I have the opinion that the name of the method and parameter should describe themselves but in this case, we need a comment for the Swagger UI. To create an XML comment write three / on top of an action. This will give you the template of the comment.Additionally, I add the response codes and the ProducesResponseType attribute which will help users of the UI to understand what return codes can be expected from the API.Adding Attributes to the ModelThe Swagger UI for .NET Core also includes the models of your application. The UI shows which models are available, what properties they have including their data type and their attributes, e.g. if the property is required. To use this feature, you only have to add the attribute to the property of your models. Swagger creates everything out of the box by itself.Personalize the Swagger UISwagger is also easily extensible. You can load your own CSS, or change the headline or information displayed on top of the page. For now, I will add my contact information so developers or customers can contact me if they have a problem. To add your contact information use the SwaggerDoc extension and pass an OpenApiInfo object inside the AddSwaggerGen extension in the Startup class.Testing SwaggerEverything is set up and when you start the application, you should see the Swagger UI. Swagger UI At top of the page, you can see my headline and my contact information. Then you can see my two actions, even in different colors for the different HTTP verbs they use, and then you can see my models. Next to the post-action, you can see the XML comment I added to describe the method. The put action doesn’t have an XML comment yet, therefore no text is displayed.When you click on an action, it opens and shows you information about the parameters, and also shows the responses which I added previously in the XML comment. Swagger UI information about an action When you click on the “Try it out” button in the top right corner, Swagger will already create a request for you with all parameters. You can edit them and then send the request to the server and the UI will show you the reply. Testing an action of the API After clicking on “Try it out”, you can define the format of your request on the top right. I leave the default application/json and also leave the created model as it is. When you click on “Execute”, the response and also the sent request, and the request URL will be shown below. In my case, the response is a code 200 and a Customer JSON object. On the bottom right is a button where you can even download the result. This might be useful if you have a test document and want to attach the result to it.ConclusionToday, I talked about Swagger, one of my favorite NuGet packages. Swagger can be used to document and test your application and make this information easily accessible even for non-technical people. Swagger is also very easy to set up and can be extended and modified to fit your needs.In my next post, I will explain the implementation of CQRS and how it can be used to split up your read and write operations.You can find the code of  the finished demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Programming a Microservice with .NET Core 3.1", "url": "/programming-microservices-net-core-3-1/", "categories": "ASP.NET", "tags": ".NET Core 3.1, C#, CQRS, Docker, Docker-Compose, MediatR, Microservice, RabbitMQ, Swagger", "date": "2020-04-13 00:00:00 +0200", "snippet": "In my last post, I talked about the theory of a microservice. Today it is going to be more practical. I will create two microservices using ASP .NET Core 3.1. Over the next posts., I will extend the microservices using CQRS, Docker and docker-compose, RabbitMQ, and automatic builds and deployments.This post is part of “Microservice Series - From Zero to Hero”.Create a Microservice using ASP .NET Core 3.1You can find the code of  the finished demo on GitHub. I will talk about the key aspects of the microservice but not about every detail. You will need at least a basic understanding of C# and ASP.NET Core.To-do list for the MicroserviceOur two microservice should satisfy the following requirements: Implement a Customer API with the following methods: create customer, update customer Implement an Order API with the following methods: create order, pay order, get all orders which had already been paid When a customer name is updated, it should also be updated in the Order API The APIs should not directly call each other to update data ASP .NET Core 3.1 Web API with DI/IoC Communication between microservices should be implemented through some kind of queue Use DDD and CQRS approaches with the Mediator and Repository PatternTo keep it simple, I will use an in-memory database. During the implementation, I will point out what you have to change if you want to use a normal database. I will split up the full implementation of these microservices over multiple posts.In this post, I will create the microservices with the needed features. In the following posts, I will implement Swagger, create a Docker container, set up RabbitMQ and explain CQRS and the Mediator pattern.Structure of the MicroserviceI created a solution for each microservice. You can see the structure of the microservices on the following screenshot. Structure of the Order Microservice Both microservices have the same structure, except that the order solution has a Messaging.Receive project whereas the customer solution has a Messaging.Send project. I will use these projects later to send and receive data using RabbitMQ.An important aspect of an API is that you don’t know who your consumers are and you should never break existing features. To implement versioning, I place everything like controllers or models in a v1 folder. If I want to extend my feature and it is not breaking the current behavior, I will extend it in the already existing classes. If my changes were to break the functionality, I will create a v2 folder and place the changes there. With this approach, no consumer is affected and they can implement the new features whenever they want or need them.The API ProjectThe API project is the heart of the application and contains the controllers, validators, and models as well as the startup class in which all dependencies are registered.Controllers in the API ProjectI try to keep the controller methods as simple as possible. They only call different services and return a model or status to the client. They don’t contain any business logic.The _mediator.Send method is used to call a service using CQRS and the Mediator pattern. I will explain that in a later post. For now, it is important to understand that a service is called and that a Customer is returned. In case of an exception, a bad request and an error message are returned.My naming convention is that I use the name of the object, in that case, Customer. The HTTP verb will tell you what this action does. In this case, the post will create an object, whereas put would update an existing customer.ValidatorsTo validate the user input, I use the NuGet FluentValidations and a validator per model. Your validator inherits from AbstractValidator where T is the class of the model you want to validate. Then you can add rules in the constructor of your validator. The validator is not really important for me right now and so I try to keep it simple and only validate that the first and last name has at least two characters and that the age and birthday are between zero and 150 years. I don't validate if the birthday and the age match. This should be changed in the future.StartupIn the Startup.cs, I register my services, validators and configure other parts of the application like AutoMapper, the database context or Swagger. This part should be self-explanatory and I will talk about Swagger or RabbitMQ later. Register the classes and configure the services DataThe Data project contains everything needed to access the database. I use Entity Framework Core, an in-memory database, and the repository pattern.Database ContextIn the database context, I add a list of customers that I will use to update an existing customer. The database context is created for every request, therefore updated or created customers will be lost after the request. This behavior is fine for the sake of this demo.If you want to use a normal database, all you have to do is delete the adding of customers in the constructor and change the following line in the Startup class to use your connection string instead of using an in-memory database.You can either hard-code your connection string in the Startup class or better, read it from the appsettings.json file.RepositoryI have a generic repository for CRUD operations that can be used for every entity. This repository has methods like AddAsync and UpdateAsync.Additionally to the generic repository, I have a CustomerRepository that implements a Customer-specific method, GetCustomerByIdAsync.The OrderRepository has more Order specific methods. The CustomerRepository inherits from the generic repository and its interface inherits from the repository interface. Since the CustomerContext has the protected access modifier in the generic repository, I can reuse it in my CustomerRepository.DomainThe Domain project contains all entities and no business logic. In my microservices, this is only the Customer or Order entity.Messaging.SendThe Messaging.Send project contains everything I need to send Customer objects to a RabbitMQ queue. I will talk about the specifics of the implementation in a later post.ServiceThe Service project is split into Commands and Queries. This is how CQRS separates the concerns of reading and writing data. I will go into the details in a later post. For now, all you have to know is that commands write data and queries read data. A query consists of a query and a handler. The query indicates what action should be executed and the handler implements this action. The commands follow the same principle.The handler often calls the repository to retrieve or change data.TestsFor my tests, I like to create a test project for each normal project whereas the name is the same except that I add .Test at the end. I use xUnit, FakeItEasy, and FluentAssertions. Currently, there are no tests for the RabbitMQ logic.Run the MicroserviceIn the previous section I only talked about the Customer service but the Order service has the same structure and should be easy to understand.Now that the base functionality is set up, it is time to test both microservices. Before you can start them, you have to make sure that RabbitMQ is disabled in the OrderApi project. Go to the OrderApi and open the appsettings. There you have to make sure that Enabled is set to false:Test the MicroserviceAfter you made the changes to both APIs, you can start them. This should display the Swagger UI which gives you information about all actions and models and also lets you send requests. The UI should be self-explanatory but I will talk more about it in my next post. The Swagger UI with the available actions and models ConclusionToday, I talked about the structure and the features of my microservices. This is just the beginning but both applications are working and could be already deployed. It is important to keep in mind that each microservice has its own data storage and is kept as simple as possible.In my next post, I will talk about Swagger and how you can use it to easily and quickly document your microservice while providing the opportunity to test requests.Note: On October 11, I removed the Solution folder and moved the projects to the root level. Over the last months, I made the experience that this makes it quite simpler to work with Dockerfiles and have automated builds and deployments.You can find the code of  the finished demo on GitHub.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": "Microservices - Getting Started", "url": "/microservices-getting-started/", "categories": "Design Pattern", "tags": "Docker, High Availability, Kubernetes, Microservice", "date": "2020-04-08 00:00:00 +0200", "snippet": "Many developers have heard about microservices and how it is the next big thing. However, for many developers I have talked to, microservices is just another buzzword like DevOps. I have been working on different projects using microservices for a little more than a year now and in this post, I would like to talk about the theory and the ideas behind the concept. In my next posts, I will show you how to implement a microservice using ASP .NET Core 3.1.This post is part of “Microservice Series - From Zero to Hero”.What is a Microservice?As the name already suggests, a microservice is very small. The opinions on how small vary. Some say that it should not be more than a hundred lines, while others say that it should only do one thing. My opinion is that a microservice should offer one or more methods in the same context. For instance, a customer service microservice could offer methods to carry out the registration, login, and changing the user’s password.For your application, take the microservices that you require so that you may compose the desired functionality. For example, if you have an online shop, you could have microservices for products, search, wishlist, customers, and orders.Why are Microservices so popular?The most important aspect of a microservice is that it works completely independently. This means that a microservice has its own database (or another storage). This is very important as this guarantees that changes in other services won’t break the microservice. At first, it may be strange to find out that a microservice is a small application in itself, especially with its own database, but this makes it way easier to change or deploy new features. This is the same principle as KISS (Keep it simple stupid) and SRP (Single Responsibility Principle) in programming. Both principles strive to keep things small and simple.A major reason why microservices became so popular is due to the fact that it helps to achieve high availability for your application. To achieve this high availability you must not couple services together and should instead keep the connections loose. This loose connection can be achieved by using message systems like RabbitMQ, Azure Queue, or Azure Service Bus. A service sends a message to the queue and the other services can then process this message. If a service is offline, the message will stay in the queue until the service is back online and can process all these messages. The downside of this approach is that it leads to a lot more complexity and problems like latency, consistency, and debugging. I will talk more about that in the Disadvantages of Microservices section further down.Advantages of MicroservicesUsing a microservice architecture can bring the following advantages: Easy to build and maintain Easy deployment New features can be implemented fast Usage of different technologies Teams can focus and specialize on one domain Better scalability and resource usageMicroservices are easy to build and maintainMicroservices are supposed to be small. Therefore they can be built, maintained, and understood easily. Tests are less complicated and new team members can learn the functionality quicker.Microservices can be easily deployedSince microservices are small and are independent of other projects, they are easy to deploy. A microservice can often be deployed with a couple of lines in a Dockerfile.New features can be implemented fastAs already mentioned, microservices are small and can be understood quicker than monoliths. Therefore, it is also easier and faster to implement and deploy new features.Usage of different technologiesAnother major benefit, especially in bigger projects is that you can use different technologies for each service. The microservices often talk with each other over HTTP or gRPC. Therefore, each service can be written in a different language. An example of this is that one service can be .NET Core, one can be Java and the third one can be Go.Teams can focus and specialize on one domainMicroservices also help teams to focus on their domain since they don’t have to care about unrelated services. For example, the team which provides the search service for an online shop only has to focus on features for the search. This means that they don’t have to care how the shop shows products or processes orders. This also helps the team to specialize in advanced search techniques which can lead to an even better product.Better scalability and resource usageMicroservices are small and often run in a Docker container in a Kubernetes cluster. If you have a monolithic online shop and it’s Black Friday, you have to scale your whole application, no matter if a feature is highly used or not at all. With a microservice architecture, you can scale the services which are in high demand. For example, if a lot of people are placing orders, you can scale the order service but you don’t have to scale the customer service. This helps to save resources and therefore decreases the costs of running your application. Another advantage of the small services is that you can place them better on a server which increases the utilization which also helps to reduce the costs.Disadvantages of MicroservicesHowever, it’s not all great when utilizing a microservice architecture. The downsides include: Increased complexity Latency Data consistency Debugging is harder Cascading of problems to other services Handling of the messages in the queueIncreased complexityEarlier I said that microservices are small and easy to understand. But when your application consists of hundreds or thousands of microservices, the application becomes considerably complex. This especially occurs when the services talk to each other. Dave Hahn gave a great talk about the architecture of Netflix in 2015 where he admitted that no one in the company understands the whole architecture. Netflix Microservices Architecture in 2015 (Source Youtube) LatencyEvery call to another service adds some latency and the user has to wait longer until they get a result. Microservices should only call other services which are really needed, and these should be placed as close together as possible.Data consistencyMicroservices often exchange data asynchronously and in addition have their own data storage. This can lead to data inconsistency For instance there may be a situation where the customer service updates a customer but the order service hasn’t done so yet. The data will eventually be synchronized but there is no way of knowing when that may occur.Debugging is harderDebugging can be harder, especially when you have problems that only occur in production. It is essential to have a good monitoring and logging strategy.Cascading of problems to other servicesA failing service can bring down a lot of other services. For example, if the product service has a problem and times out, every service calling it has to wait until they get an error message back. There may quickly be a lot of affected services when the service calling the product service is also called by another service. To prevent such a system failure, you should consider implementing a circuit breaker.Handling of the messages in the queueMessages between services are often sent via a queue. This occurs so that messages can be handled even if a service is offline at the time of publishing. The messages in the queue can get problematic if they can’t be processed. A service that is unable to process it, puts it back in the queue. This costs resources and time. To avoid this issue, you could set a time to live and remove the message from the queue when the time is reached.ConclusionIn this article, I have tried to give a short overview of the theory of microservices. Although microservices have advantages, they also have drawbacks especially in comparison to monolithic applications. In the end, there is no perfect solution and you should think about the requirements of your project and then decide if microservices are the way to go. I would also recommend starting with a small project and gaining some experience in running them.This post was all about microservices in theory, in my next posts, I will implement two microservices using ASP .NET Core, CQRS, mediator, RabbitMQ and Docker.This post is part of “Microservice Series - From Zero to Hero”." }, { "title": ".NET Core 3.0 - What's new", "url": "/net-core-3-0-whats-new/", "categories": "Programming", "tags": ".NET Core, .NET Core 3.0, C#", "date": "2019-12-07 00:00:00 +0100", "snippet": "Microsoft released with .NET Core 3.0 the next major version of .NET Core which brings improvements to the deployment, .NET Core WPF and WinForms applications and much more. Today, I want to showcase some of these new features.Build and Deployment Improvements.NET Core used to create a dll as an output, even for a console application. Starting with .NET Core 3.0, an exe file is created by default. The downside of the default behavior is that it copies all dependencies into the output folder which can be hundreds of files in a bigger application.Single File ExecutablesA pretty neat feature of .NET Core is that you can create a single file executable that contains all dependencies. This makes the output way clearer. To create a single file executable, you only have to add the RuntimeIdentifier and PublishSingleFile in your csproj file. Configure your application to publish as a single file The RuntimeIdentifier tells the compiler for what operating system it should create the executable. This could be, for example, win10-x64 or win10-x86. Creating a single file will make the executable way bigger since all the dependencies are packed into the file. Everything got packed into the executable Trim the publish outputTo reduce the size of your published output, you can trim it. Trimming removes not needed dependencies.To configure ready to run, you only have to add the PublishedTrim tag to your csproj and set it to true. Remove not needed dependencies before the publish Be careful though because if you work with reflections, dependencies might get removed although you need them. You have to test your application before using it. The executable is much smaller after the trim Ready to run publishWith .NET Core 3.0, you can create ready to run executables. This uses ahead of time compilation, to have better startup time because it contains the native code and .net intermediate code. The additional .net intermediate code might make the executable bigger.To configure ready to run, you only have to add the PublishReadyToRun tag to your csproj and set it to true. Publish ready to run .NET Core 3.0 removes JSON.NET.NET Core 3.0 introduces the new Namespace System.Text.Json. Until now JSON.NET was the standard JSON library for .net applications and also .NET Core depends on it. With .NET Core 3.0, the namespace System.Text.Json was introduced to remove this dependency. The classes in this namespace are super fast and need only a low amount of ram. The downside is that they are not feature-rich yet. This should change in future versions. You don’t have to use them but you can..NET Core 3.0 goes DesktopWith .NET Core 3.0, you can now create WPF and WinForms applications. Currently, they are Windows only and the tooling needs a bit more work but new desktop applications can and should use .NET Core. Visual Studio provides a template for the .NET Core WPF application. Create a WPF .NET Core Application After the application is created, you can see in the csproj file that it is a WPF application and used .NET Core 3.0 csproj file of the .NET Core WPF project The tooling is not perfect yet but if you need additional UI controls you can use the Microsoft.Toolkit.Wpf.UI.Controls namespace for WinUI features. With this namespace you can, for example, draw in a canvas in your application.The .NET Core application can be built as an MSIX package which will contain most of the needed references and can be distributed via the Microsoft Store. You can find the demo .NET Core WPF application on GitHub.Additional Features in .NET Core 3.0There are many more new features in .NET Core 3.0 like: Platform-dependent Intrinsics: provide access to hardware-specific instructions and can improve performance Cryptography improvements: The new ciphers AES-GCM and AES CCM were added. HTTP/2 support: Note HTTP/2 is not enabled by default. Serial Port support for Linux Improved Garbage CollectorConclusion.NET Core is great and with the new version, it got even better. My favorite new features are the features around the improved deployment. Also, the new namespace for JSON is nice, although not production-ready yet if you have complex requirements. I am not a desktop developer but I guess that .NET Core 3.0 is a major game-changer for desktop developers.You can find the code of today’s demo on GitHub." }, { "title": "C# 8.0 - What's New", "url": "/c-8-0-whats-new/", "categories": "Programming", "tags": ".NET Core, .NET Core 3.0, C#", "date": "2019-12-05 11:23:42 +0100", "snippet": "Together with .NET Core 3.0, Microsoft released C# 8.0. Today, I want to take a look at the new features of C# 8.0 using .NET Core 3.0.To follow along with the demo you need Visual Studio 2019 and .NET Core 3.0. You can find the code of the demo on GitHub.Nullable Reference TypesAvoiding NullReferenceExceptiony can be hard. Therefore C# 8.0 introduces nullable reference types which help us to avoid null reference mistakes at compile-time. This new feature allows you to mark properties as nullable and non-nullable. For example, you can have nullable and non-nullable strings now.  Let’s see some code:I have created a new class, Car, with two string properties. Implementation of the Car class I expected the compiler to give me a warning that Brand and Make could be null. But everything seems fine. Nullable Reference Types are an opt-in feature. This means that you have to enable the feature to be able to use it. It should be enabled for new C# 8.0 projects, but it didn’t work for me. To enable it add netcoreapp3.0 to the PropertyGroup of your .csproj file. Enable Nullable Reference Types for your C# 8.0 project After adding this tag, you will see two compiler warnings and visual effects under the properties. Warning of uninitialized properties If you want them to be null, you can use the Elvis operator to mark them as nullable. This removes the compiler warning. Mark the properties as nullable Another useful feature is that the compiler warns you of possible null reference exceptions when you use a nullable property. In the following screenshot, I try to access the Length of the Make property. Since I haven’t initialized the Make property, this would lead to a NullReferenceException. NullReference warning from the compiler in the code The compiler is great but not perfect. Sometimes a property can’t be null but the compiler still gives you the warning. Then you can tell the compiler that this property is never null by using the ! operator. Telling the compiler that a property is never null In this case, you shouldn’t use it though because the compiler is right and the property is null.Pattern MatchingPattern matching was first introduced in C# 7. C# 8.0 extends the pattern matching by allowing us to use more types of patterns in more places.Property PatternsProperty patterns allow you to check one or multiple properties of an object and return a value, depending on the properties. In my example, I have a house class with a name and floors. If the house has exactly 10 floors and is named Skyscraper, I return true. Otherwise, I return false. This is not the best example but I am not really good at coming up with examples 😉 Property Pattern Matching Switch ExpressionsThis feature allows you to switch over objects and return different values, depending on the type of the object and additionally on the properties of the example. For this demo, I created three classes, Circle, Rectangle and Triangle. I pass the object to a method and the method returns information about the object, depending on its type. Switch Expressions On the screenshot above, you can see the switch statement for my three different objects. You can also have a switch in a switch. I use this to return a different text if the object is a normal rectangle or a square. The switch expression doesn’t allow a fall-through, the, therefore no break is needed after the case. The last line handles the default case, using the _ to catch all cases which weren’t handled before.Nesting switch statements in the switch can be handy but don’t overdo since your code can become hard to read very fast.Tuple PatternsTuple patterns are similar to switch expressions. Here you can pass two values (a tuple) which are evaluated in the case. In my demo, I pass two colors and return the result if you mix these two colors. For example, red and blue will lead to purple. Tuple Patterns The last two lines are interesting. The second last line, (_, _) when color1 == color2 doesn’t care which colors are passed, as long as both have the same value. The last one works as the default case and returns unknown when no case was hit before.Indices and RangesWorking with indices and ranges can be confusing. I am gonna try to explain them first before I show a code demo. C#8.0 provides the new range operator .. which allows you to work with ranges. For example, you can have an array with ascending numbers from 1 to 10. With the range operator, you could select all items from index 2 to 8. It is important to note that the beginning of a range is inclusive and the end is exclusive. This means the range [1..8] returns the numbers 2 – 8. This is due to the previously mentioned inclusion of the beginning and exclusion of the end. Additionally, C#, like many other languages is zero index-based. This means that index 1 will give you the second number, therefore 2.You can select an index based on the beginning of the array but also based on the end of the array. To use an offset of the end of the array, use the ^ operator. For example [^1] will give you the last number of the array. Note that [^0] will lead to an index out of range exception because it works the same way as array.Length. This would also give you an out of range exception due to the zero-based indexing.Let’s code some examples which should make this feature clearer.Additional C# 8.0 FeaturesC# 8.0 brings too many new features to highlight here. Additional features are: Default Interface Members Using Declarations Async Streams Static Local Functions Disposable ref Structs Many moreFor more information about the new features of C# 8.0 see Microsoft’s documentation.ConclusionIn this post, I presented some of the new features of C# 8.0 like nullable reference types, pattern matching, and async streams. There are many more new features that you can look into in detail in the official documentation.You can find the code of today’s demo on GitHub." }, { "title": "Create Policies for your Branches in Azure DevOps", "url": "/create-policies-for-your-branches-in-azure-devops/", "categories": "DevOps", "tags": "Azure DevOps, Azure DevOps Services, Git, Pull Request", "date": "2019-11-28 23:06:39 +0100", "snippet": "In my last post, I showed how to create a build pipeline for your .net and .NET Core project. Today, I want to use one of those pipelines to verify that new pull requests didn’t break my solution and I also want to show how to improve the code quality with branch policies.Branch PoliciesBranch policies let you define rules on your branch. These rules can be that commits can be only added through pull requests, a successful build validation or the approval of a reviewer. Another nice feature is that branches with policies can’t be deleted (except if you have special rights for that).Protect the Master Branch with PoliciesLet’s set up a policy for the master branch. In your Azure DevOps (on-prem or in the cloud), go to Branches, click the three dots next to the master branch and select branch policies. Open branch policies On the Branch policies for master config page, I enable the following settings:Require a minimum number of reviewersThis setting enforces that at least one reviewer approved the pull request. Microsoft found out in a research that 2 is the optimal number of reviewers. I only need one most of the time because my teams are usually small therefore needing two reviewers would be too much overhead. Additionally, I set the “Reset code reviewer votes when there are new changes”. This setting resets previously made approvals if new code is pushed to the pull request. Configure a minimum number of reviewers Check for linked work itemsThis feature indicates if a work item was linked with the pull request. I think the right setting would be to set it to required because every pull request should have a PBI. I have to admit that sometimes I create a pull request without a PBI, therefore I leave it as optional. Check for linked work items Check for comment resolutionI set this feature to required because when a comment is made, it must be resolved before a pull request can be completed. In my teams, the author of the comment resolves it except when the author specifies that the comment can be closed by the creator of the pull request. Check for comment resolution Build validationThe build validation is probably the most important step for a pull request because it runs a build when a pull request was created. If this build is not successful, the pull request can’t be completed. To add a build policy click on “+ Add build policy” and select the previously created build pipeline. Build validation Other settingsI leave all other settings as they are. I encourage you to at least go over them and even try them out. They should be self-explaining.Effects of Branch PoliciesNow that the master branch is protected by a branch policy, let’s test it.No more Commits to the Master BranchI made some changes and committed them to my local master branch. When I try to push the branch, I get the following error message: Error encountered while pushing to the remote repository: rejected master -&gt; master (TF402455: Pushes to this branch are not permitted; you must use a pull request to update this branch.). This means that I have to create a feature branch and create a pull request to merge my changes into the master branch.Creating a Pull Request with a failed Build due to my Branch PoliciesIn my last commit, I changed some tests. I created a feature branch and pushed it to Azure DevOps Services. When you click on Repos –&gt; Pull requests, Azure DevOps Services recognizes the new branch and suggests to create a pull request. To do that click on Create a pull request. Azure DevOps Services suggests to create a pull request On the New Pull Request, you can leave everything as it is and create the pull request by clicking on Create. This creates the pull request and automatically kicks off the build. Overview of the open pull request As you can see on the screenshot above, the build failed. This means that I broke something in the code. Without policies, it is way more likely that these defects get merged into the master branch and spread to all other developers. The screenshot also shows that someone commented and that I can’t finish the pull request until someone approved it, the comment is resolved and the build succeeded. Additionally, you can see that no work item is linked. This is only for information purposes and not required, as configured before.Finishing a pull requestI fixed the broken build and pushed my changes to my feature branch. Azure DevOps Services recognizes the changes and automatically starts a new build. After the build succeeded, the author of the comment resolves the comment and approves the pull request.To finish the pull request, I click on Complete, to complete it. In the Complete pull request window, I select Delete feature (the name of my branch) after merging. This deletes the feature branch automatically from the Azure DevOps Services Git. You could also set Auto-complete, which would finish your pull request automatically when all criteria are fulfilled. I set this all the time because I don’t want to go back and complete the pull request when the system can do it automatically for me. The pull request is accepted and can be completed Note: for the screenshot above, I configure the branch that the creator is allowed to approve the pull request themselves because I only have this one user in the project and was too lazy to create a second one.ConclusionToday, I showed how to protect your branch with policies. These policies can be used to enforce successful builds and the approval of a pull request from a reviewer. Having at least a second pair of eyes and an automatic build should increase the quality of the commits and enable you and your team to increase the development velocity and also helps to increase the quality of new features." }, { "title": "Create Automatic Builds for .NET and .NET Core Applications with Azure DevOps", "url": "/create-automatic-build-pipeline-for-net-core/", "categories": "DevOps", "tags": ".NET Core, ASP.NET Core MVC, ASP.NET MVC, Azure DevOps, Continuous Integration", "date": "2019-11-18 16:15:30 +0100", "snippet": "Automated build processes should be a no-brainer nowadays but unfortunately, I still come across quite many projects which do their builds manually. In today’s post, I want to give an introduction to setting up an automated build pipeline for a .net and .NET Core application with Microsoft’s Azure DevOps Server. The DevOps Server is the on-premise version of Azure DevOps Services, but they are so similar that its negligible which one you use for this demo. I will use Azure DevOps Services. You can create a free account here.Creating your first Build Pipeline for a .net Framework ApplicationIn your Azure DevOps project go to Pipelines and then again Pipelines. There click on Create Pipeline and the wizard starts. Start the create pipeline wizard Here you can use either the new YAML editor which is selected by default or the classic editor. I use the classic editor by clicking on Use the classic editor at the bottom. In the next window, select the source of your code. The editor offers a wide range of repositories like Azure Repos Git, GitHub or even Subversion. Since I manage my code in Azure DevOps Services, I select Azure Repos Git and the wizard automatically selects your current project, repository and default branch. Click Continue to get to the template selection. Select the repository for your pipeline Azure DevOps Services offers a wide range of different templates as a starting point for your pipeline. There are templates for ASP .NET, Docker, Azure Functions and many more. You don’t have to select a template and could start without one by selecting Empty job or by importing a YAML file by selecting YAML. Since I have an ASP .NET MVC application, I select ASP. NET. Select a template for your pipeline Click on Apply and the pipeline will be created for you. The created build pipeline Inspecting the Settings of the Build PipelineOn top of your build pipeline, you can see six tabs with settings. Let’s check them out and see what we can configure there.TasksThe Tasks tab is the heart of your build pipeline and the part where all the magic happens. I will go over the steps in more detail in the next section.VariablesIn the Variables tab, you can create variables that can be used in every step of the pipeline. For builds, I barely use variables but you could set the username and password of an external service that you want to call. Azure DevOps already comes with a couple of predefined variables like the build configuration or platform. Variables for the build pipeline With the checkbox on the right (Settable at queue time), you can configure that these variables can be changed when you create a build. On the left are also Variable groups. There you can also set variables. The difference is that you can reuse a variable group for several pipelines whereas the Pipeline variables are specific for your pipeline.TriggersThe Triggers tab configures continuous integration. On the following screenshot, you can see that I enabled continuous integration and trigger a build every time a change to a branch with the pattern release/* was made. (When I want to deploy I create a branch from master with the name release/sprint-XX. This puts the sprint-XX branch in the release folder and also triggers the build) Enable continuous integration It is also possible to schedule builds. For example, if you have integration tests that take a long time to finish, you don’t want to run them at every commit. You can run them every night, for example, Monday to Friday at 2 am or on the weekend. Schedule builds OptionsHere, you can edit the timeout for your builds and the build number format. I edited the number format to display the branch and then the date and build number. By default, only the date and build number are displayed. To do that, I am using the built-in variable Build.SourceBranchName.  Include the branch name in the build format number RetentionUnder this tab, you can configure your retention policies for your builds. The default is to keep 10 good builds for 30 days. I leave this as it is. Configure the retention policy of your build pipeline HistoryThe history tab shows not the history of your builds but the history of your build pipeline changes. I highly recommend you to always make a comment when you save changes in your pipeline. The history of the changes to the pipeline Inspecting the Build Tasks created by the ASP. NET TemplateNow that all the settings are as wished, let’s look at the steps of the deployment.PipelineHere you can configure the agent pool, agent and artifact name for your build. The agent pool groups agents together. An agent builds (and later deploys) your application. Since I am using Azure DevOps Services, I leave the agent pool at Azure Pipelines because I want to use the agents which are hosted by Microsoft. For the agent, I also leave it as it is. If you are running a .NET core build, you could switch to Ubuntu.Get sourcesUnder Get sources, you can change the project, repository and default branch for your build. You can also configure that a clean should be performed before the build. I set Clean to true and the Clean options to All build directories. Additionally, you could set a tag automatically for each build or each successful build. Configure the clean options Use NuGetThe first set of your build pipeline is used to set up the NuGet.exe which will be used in the next step to restore your NuGet packages. I leave all the settings at their default values.NuGet restoreThis step restores all the NuGet packages of your solution. By default the NuGet packages in all projects. You can change this by changing the Path to solution from *\\.sln to whatever fits you.Build solutionThe build solution step builds your application and also publishes it. The publish is necessary to deploy it later. You can also configure which version of Visual Studio should be used. You can choose from 2012 on all versions. Latest always selects the newest version.Test AssembliesHere, all test assemblies with test in their names are executed. You can select to run only impacted tests to speed up your build process. This setting executes only tests that were affected by the last commit. You can also configure to run all tests in isolation or even rerun failed tests.Publish symbols pathThis task publishes all *.pdb files which can be used for remote debugging.Publish ArtifactPublish Artifact is necessary for an automated deployment. This step publishes all the files which you want to deploy later. If you don’t do this, the release pipeline won’t find any files to deploy.Running your first buildNow that everything is set up, you can run your first build by clicking Save &amp; queue and then Save and run. Run your build This starts the build process. You can see the status by clicking on Agent job 1. On the following screenshot, you can see that the build step is already done and the tests are run right now. The status of the running build pipeline After a couple of minutes, your build should finish successfully.Creating a Build Pipeline for a .net Core ApplicationCreating a build pipeline for .NET Core is the same process as for a .net framework application. The only difference is that I select ASP.NET Core as the template this time. Select .NET Core as your build template You can see a difference in the created build pipeline though. The .NET Core build pipeline Why I like the .net Core Build Pipeline betterThere are several reasons why I like the .NET Core build pipeline better than the one for a .net framework application.The first thing which got my attention is that all tasks look the same. All tasks except the last one use the dotnet CLI. The only difference is the argument (restore, build, publish and test). This means that I know exactly what’sgoing on and due to the similarity of the tasks, they are easy to configure.An even better new feature is that it is possible to zip a project during the publishing process. Before that, you had to use a Powershell script or the Archive files of Azure DevOps to create the zip. But this meant that you have an additional step and also duplicate data since you have the “normal” published files and afterwards the zipped ones. Zipping the files is necessary to save storage space but more importantly to speed up the deployment process.ConclusionThis post showed how you can quickly set up an automated build pipeline for your .net or .NET Core application in Azure DevOps. This is the first step to increasing the development velocity and reducing the bugs and later on also the deployment time." }, { "title": "ASP.NET Core logging to a database with NLog", "url": "/asp-net-core-logging-to-a-database-with-nlog/", "categories": "ASP.NET", "tags": "ASP.NET Core MVC, Logging, NLog, SQL", "date": "2019-10-01 11:18:13 +0200", "snippet": "ASP.NET core has seen rapid development in the last years. Additionally, there were some breaking changes since version 1, for example, the project.json file got removed. Unfortunately, the documentation is lacking behind this rapid development. I had exactly this problem when I wanted to use NLog to log to my database. There was plenty of documentation but none of it worked because it was for .NET Core 1.x.Today, I want to talk about how I implemented NLog with ASP.NET core 2.2 and how I configured it to log to my database. You can find the source code for the following demo on GitHub.Installing NlogI created a new web project with .NET Core and the MVC template and added the NLog.Web.AspNetCore NuGet package. Next, I create a new file NLog.config in the root folder of my solution. This file will contain all the configs for NLog. Now it is time to fill the config file with some content. You can find the source code for this demo on GitHub.Implementing the Nlog configTo get started, create a database and then the Log table. You can find the script to create the table at the bottom of my config file. Now let’s inspect the config file: NLog internal logging The first section of the file is for internal logs of Nlog. These logs come in handy when you have a problem with Nlog. There you can configure what level of logging you want and where the log file should be created. You can also configure whether the file should be reloaded on save with autoReload. Configure the database connection The next section is for configuring the database connection. The variables are read from the appsettings.json from the NlogConnection section. You can see the appsettings.json section on the following screenshot. Settings for Nlog from appsettings.json The commandText section defines the insert statement. This is straight forward and you don’t have to edit anything. Setting up the insert statement for logging The last section lets you specify rules about your log. You can configure which logger should log where. In my example, every logger logs messages with the log level Info and higher into the database. Another example could be to log information from one logger to the database and the information from another one to a file. Rules for logging Using NlogUsing Nlog in your application is really simple. First, you have to tell your WebHost to use Nlog in the CreateWebHostBuilder by simply adding .UseNlog() at the end of the statement. Use Nlog in the WebHostBuilder That’s all you have to do. Now you can already use the logger in your application. To use the logger, inject the ILogger interface with the type of the class which uses it. The ILogger interface provides useful methods like LogInformation() or LogCritical(). Call one of the methods and insert your log message. Use ILogger to log messages Testing the implementationTo test that the logging is working, you have only to start your application and call one of the controllers which do some logging. Then you can check in your database in the Log table and you should see the log entries there.On the following screenshot, you can see that I called the Index and the Privacy method once which create a log entry for both calls. Log entries in the Log table ConclusionThis post showed how simple it is to set up and use NLog with your ASP.NET MVC Core application. All you have to do is installing the NuGet, adding the nlog.config file and use it in your application.You can find more information about NLog on their website and you can find the source code of today’s demo on GitHub." }, { "title": "Understanding Tag Helpers in ASP.NET Core MVC", "url": "/understanding-tag-helpers-in-asp-net-core-mvc/", "categories": "ASP.NET", "tags": ".NET Core, ASP.NET Core MVC, C#, Tag Helper", "date": "2019-05-11 11:56:04 +0200", "snippet": "Tag helpers are a new feature that has been introduced in ASP.NET Core MVC and are C# classes that transform HTML elements in a view. Common uses for tag helpers include URLs for forms using the application’s routing configuration, ensuring that elements of a specific type are styled consistently, and replacing custom shorthand elements with commonly used fragments of content.You can find the source code for the following on GitHub.Creating Tag HelpersTag helpers can be used to apply certain CSS classes to HTML. For example you could create a tag helper that transforms an element from &lt;button type=”submit” bs-button-color=”danger”&gt;Add&lt;/button&gt; to &lt;button type=”submit” class=”btn btn-danger”&gt;Add&lt;/button&gt;. The tag helper will recognize the bs-button-color attribute and use its value to set the class attribute on the element sent to the browser. This example might not look very useful, but it provides a foundation for explaining how tag helpers work.Defining the Tag Helper classTag helpers can be defined anywhere in the project, but it helps to keep them together because they need to be registered before they can be used. I usually create the tag helpers under Infrastructure/TagHelpers.Tag helpers are derived from the TagHelper class, which is defined in the Microsoft.AspNetCore.Razor.TagHelpers namespace. Creating a Tag Helper to transform a button The TagHelper class defines a Process method, which is overridden by subclasses to implement the behavior that transforms elements. The name of the tag helper combines the name of the element it transforms followed by TagHelper. The scope of a tag helper can be broadened or narrowed using attributes, which I will describe later in this post.Asynchronous tag helpers can be created by overriding the ProcessAsync method instead of the Process method, but this isn’t required for most helpers, which tend to make small and focused changes to HTML elements. The default implementation of ProcessAsync calls the Process method anyway.Receiving Context DataTag helpers receive information about the element they are transforming through an instance of the TagHelperContext class, which is received as an argument to the Process method and which defines the following properties: Name Description AllAttributes This property returns a read-only dictionary of the attributes applied to the element being transformed, indexed by name and by index. Items This property returns a dictionary that is used to coordinate between tag helpers. UniqueId This property returns a unique identifier for the element being transformed. Although you can access details of the element’s attributes through the AllAttributes dictionary, a more convenient approach is to define a property whose name corresponds on the attribute you are interested in, like in the example above: public string BsButtonColor { get; set; }.When a tag helper is being used, MVC inspects the properties it defines and sets the value of any whose name matches attributes applied to the HTML element. As part of this process, MVC will try to convert an attribute value to match the type of the C# property so that bool properties can be used to receive true and false attribute values and int properties can be used to receive numeric attribute values such as 1 and 2.The name of the attribute is automatically converted from the default HTML style, bs-button-color to the C# style BsButtonColor. You can use any attribute prefix except asp- and data-. Properties for which there are no corresponding HTML element attributes are not set, which means you should check to ensure that you are not dealing with null or default values. Using the HTML attribute name for tag helper properties doesn’t always lead to readable or understandable classes. You can break the link between the name of the property and the attribute it represents using the HtmlAttributeName attribute, which can be used to specify the HTML attribute that the property will represent.What happened to HTML Helpers?HTML helpers were methods accessed through Razor expressions that begin with @Html, for example. @Html.TextBoxFor(m =&gt; m.Name) would create a textbox for the name property. The problem with HTML helper expressions is that they don’t fit with the structure of HTML elements, which leads to awkward expressions, especially when adding CSS, for example. @Html.TextBoxFor(m =&gt; m.Name, new { @class = “form-control” }). Attributes have to be expressed in a dynamic object and have to be prefixed with @ if they are reserved C# words, like class. As the HTML elements that are required become more complex, the HTML helper expression becomes more awkward. Tag helpers remove this awkwardness, for example, &lt;input class=”form-control” asp-for=”Population” /&gt;.The result is a more natural fit with the nature of HTML and produces views that are easier to read and understand. MVC still supports HTML helpers, so you can use them for backward compatibility in views originally developed for MVC 5, but new views should take advantage of the more natural approach that tag helpers provide.Producing OutputThe Process method transforms an element by configuring the TagHelperOutput object that is received as an argument. The TagHelperOutput starts out describing the HTML element as it appears in the Razor view and is modified through the following properties: Name Description Attributes This property returns a dictionary containing the attributes for the output element. Content This property returns a TagHelperContent object that is used to set the content of the element. TagName This property is used to get or set the tag name for the output element. PreElement This property returns a TagHelperContext object that is used to insert content in the view before the output element. PostElement This property returns a TagHelperContext object that is used to insert content in the view after the output element. PreContent This property returns a TagHelperContext object that is used to insert content before the output element’s content. PostContent This property returns a TagHelperContext object that is used to insert content after the output element’s content. TagMode This property specifies how the output element will be written, using a value from the TagMode enumeration. SupressOuput() Calling this method excludes an element from the view.  Registering Tag HelpersTag helper classes can be used only after they have been registered using the Razor @addTagHelper expression. The set of views to which a tag helper will be applied depends on where the @addTagHelper expression is used. For a single view, the expression appears in the view itself. For a subset of views in an application, the expression appears in a _ViewImport.cshtml file in the folder that contains the views or a parent folder. Register the Tag Helper The first part of the argument specifies the name of the tag helper classes, with support for wildcards. The second part specifies the name if the assembly in which they are defined.Using a Tag HelperThe last step is to use the tag helper in the view, so it can transform the element. Using a Tag Helper If you start the application and click on create, you will see the red Add button.Broadening the scope of a tag helper means you don’t have to create tag helpers that repeat the same operation on different element types. Some care is required because it is easy to create a tag helper that will start matching elements too broadly in the future as the contents of the views in the application evolve. A more balanced approach is to apply the HtmlTargetElement attribute multiple times, specifying the complete set of elements that will be transformed as a combination of narrowly defined matches. Add multiple HtmlTargetElement attributes Ordering Tag Helper ExecutionAs a general rule, it is a good idea to use only one tag helper on any given HTML element. That’s because it is easy to create a situation where one tag helper tramples on the transformation applied by another, overwriting attributes values or content. If you do need to apply multiple tag helpers, then you can control the sequence in which they execute by setting the Order property, which is inherited from the TagHelper base class. Managing the sequence can help minimize the conflicts between tag helpers, although it is still easy to encounter problems.Advanced Tag Helper FeaturesIn the following sections, I will show you some neat advanced features to get the best out of tag helpers.Creating Shorthand ElementsTag helpers are not restricted to transforming the standard HTML elements and can also be used to replace custom elements with commonly used content. This can be a useful feature for making views more concise and making their intent more obvious. Adding your own HTML element The WolfgangsButton element isn’t part of the HTML specification and won’t be understood by browsers. Instead, I am going to use this element as a shorthand for generating the button elements that the form requires.When dealing with custom elements that are not part of the HTML specification, you must apply the HtmlTargetElement attribute and specify the element name. The convention of applying tag helpers to elements based on the class name works only for standard element names. The new Tag Helper to transform WolfgangsButton The Process method uses the properties of the TagHelperOutput object to generate a completely different element: the TagName property is used to specify a button element, the TagMode property is used to specify that the element is written using start and end tags, the Attributes. SetAttribute method is used to define a class attribute with Bootstrap styles, and the Content property is used to set the element content.Note that I set the type attribute on the screenshot above. This is because any attribute for which there is a property defined by a tag helper is omitted from the output element. This is usually a good idea because it stops the attributes used to configure tag helpers from appearing in the HTML sent to the browser. However, in this case, I used the type attribute to configure the tag helper, and want it to be present in the output element as well.Setting the TagName property is important because the output element is written in the same style as the custom element by default. Since I want the output element to contain content, I have to explicitly specify the TagMode.StartTagEndTag enum value so that separate start and end tags are used. The Content property returns an instance of the TagHelperContent class, which is used to set the content of elements. The following table describes the most important TagHelperContent methods: Name Description SetContent(text) This method sets the content of the output element. The string argument is encoded so that it is safe for inclusion in an HTML element. SetHtmlContent(html) This method sets the content of the output element. The string argument is assumed to be safely encoded. Use with caution. Append(text) This method safely encodes the specified string and adds it to the content of the output element. AppendHtml(html) This method adds the specified string to the content of the output element without performing any encoding. Use with caution. Clear() This method removes the content of the output element. Prepending and Appending Content and Elements with Tag HelpersThe TagHelperOutput class provides four properties that make it easy to inject new content into a view so that it surrounds an element or the element’s content. Name Description PreElement This property is used to insert elements into the view before the target element. PostElement This property is used to insert elements into the view after the target element. PreContent This property is used to insert content into the target element, before any existing content. PostContent This property is used to insert content into the target element, after any existing content Inserting Content around the Output ElementThe first TagHelperOutput properties are PreElement and PostElement, which are used to insert elements into the view before and after the output element. The content wrapper implementation This tag helper transforms div elements that have a title attribute, and it works by using the PreElement and PostElement to add a header and a footer element that will surround the output element. When generating new HTML elements, you can use C# string formatting to create the content you require, but this is an awkward and error-prone process for all but the simplest elements. A more robust approach is to use the TagBuilder class, which is defined in the Microsoft.AspNetCore.Mvc.Rendering namespace and allows elements to be created in a more structured manner. The TagHelperContent class defines methods that accept TagBuilder objects, which makes it easy to create HTML content in tag helpers.This tag helper uses the TagBuilder class to create an h1 element that is contained in a div element that has been styled with Bootstrap classes. There are optional bool include-header and include-footer attributes used to specify where the content is injected, and the default is to add elements before and after the output element. Adding surrounding content Next, I add a div tag around the RenderBody method. Preparing the RenderBody tag to render my element before and after it Inserting Content inside the Output Element with Tag HelpersThe PreContent and PostContent properties are used to insert content inside the output element, surrounding the original contents. Tag Helper to insert content inside of an element This tag helper operates on the td elements with the wrap attribute and inserts b and i elements around the output element’s content. If you run the application, you will see that the first column of cells in the table that lists the Country objects is shown in bold and italic.ConclusionIn this post, I showed how to transform HTML elements according to your desires using tag helpers. Then I talked about creating your own elements and also transforming then. Lastly, I added content around and inside elements. There are even more interesting topics about tag helpers like dependency injection or working with view models.For more details about the configuring ASP.NET Core, I highly recommend the book “Pro ASP.NET Core MVC 2“. You can find the source code for this demo on GitHub." }, { "title": "View Components in ASP.NET Core MVC", "url": "/view-components-in-asp-net-core-mvc/", "categories": "ASP.NET", "tags": ".NET Core, ASP.NET Core MVC, C#, View Component", "date": "2019-05-09 11:41:52 +0200", "snippet": "View components are a new feature in ASP.NET Core MVC which replaces the child action feature from the previous version. View components are classes which provide action-style logic to support partial views. This means that complex content can be embedded in views with C# code which can be easily maintained and unit tested.  You can find the source code for the following demo on GitHub.Understanding View ComponentsApplications commonly need to embed content in views that aren’t related to the main purpose of the application. Common examples include site navigation tools and authentication panels that let the user login without visiting a separate page. The common thread that all these examples have is that the data required to display the embedded content isn’t part of the model data passed from the action to the view.Partial views are used to create reusable markup that is required in views, avoiding the need to duplicate the same content in multiple places in the application. Partial views are a useful feature, but they just contain fragments of HTML and Razor, and the data they operate on is received from the parent view. If you need to display different data, then you run into a problem. You could access the data you need directly from the partial view, but this breaks the separation of concerns that underpins the MVC pattern and results in data retrieval and processing logic being placed in a view file.Alternatively, you could extend the view models used by the application so that it includes the data you require, but this means you have to change every action method and it is hard to isolate the functionality of action methods for effective testing.This is where view components come in. A view component is a C# class that provides a partial view with the data that it needs, independently from the parent view and the action that renders it. In this regard, a view component can be thought of as a specialized action, but one that is used only to provide a partial view with data. It can’t receive HTTP requests, and the content that it provides will always be included in the parent view.Creating a View ComponentView components can be created in three different ways which are: defining a Poco view component deriving from the ViewComponent base class using the ViewComponent attributeCreating Poco View ComponentsA Poco view component is a class that provides view component functionality without relying on any of the MVC APIs. As with Poco controllers, the kind of view component is awkward to work with but can be helpful in understanding how they work. A Poco view component is any class whose name ends with ViewComponent and that defines an Invoke method. VIew component classes can be defined anywhere in an application, but the convention is to group them together in a folder called Components at the root level of the project. The Poco view component You can add your view component to a view by calling the invoke method and passing the view component name: Calling the VIewComponent from the view This code adds the number of countries and its total population on top of the customer table. Testing the view component I know that this is a very simple example but I think it demonstrates how view components work very well.First, the PocoViewComponent class was able to get access to the data it required without depending on the action handling the HTTP request or its parent view. Second, defining the logic required to obtain and process the country summary in a C# class which is easily readable and can also be unit tested. Third, the application hasn’t been twisted out of shape by trying to include country objects in view models that are focused on Customer objects. In short, a view component is a self-contained chunk of reusable functionality that can be applied throughout the application and can be developed and tested in isolation.Note that you have to include the await keyword. Otherwise, you won’t see an error but only a string representation of a Task will be displayed.Deriving from the ViewComponent Base ClassPoco view components are limited in functionality unless they take advantage of the MVC API, which is possible but requires a lot more effort than the more common approach, deriving from the ViewComponent class. Deriving from the base class gives you access to context data and makes it easier to generate results. When you create a derived view component, you don’t have to put ViewComponent in the name. Calling the derived view component from the view Understanding View Component ResultsThe ability to insert simple values into a parent view isn’t especially useful, but fortunately, view components are capable of much more. More complex effects can be achieved by having the Invoke method return an object that implements the IViewComponentResult interface. There are three built-in classes that implement the IViewComponentResult interface: Name Description ViewViewComponentResult This class is used to specify a Razor view, with optional view model data. Instances of this class are created using the View method. ContentViewComponentResult This class is used to specify a text result that will be safely encoded for inclusion in an HTML document. Instances of this class are created using the Content method. HtmlContentViewComponentResult This class is used to specify a fragment of HTML that will be included in the HTML document without further encoding. There is no ViewComponent method to create this type of result There is special handling for two result types. If a view component returns a string, then it is used to create a ContentViewComponentResult object. If a view component returns an IHtmlContent object, then it is used to create an HtmlContentViewComponentResult object.Return a Partial ViewThe most useful response is the awkwardly named ViewViewComponentResult object, which tells Razor to render a partial view and includes the result in the parent view. The ViewComponent base class provides the View method for creating ViewViewComponentResult objects, and there are the following four versions of the method available: Name Description View() Using this method selects the default view for the view component and does not provide a view model View(model) Using the method selects the default view and uses the specified object as the view model. View(viewName) Using this method selects the specified view and does not provide a view model. View(viewName, model) Using this method selects the specified view and uses the specified object as the view model. These methods correspond to those provided by the Controller base class and are used in much the same way. The derived view component returning an IViewComponentResult object Selecting a partial view in a view component is similar to selecting a view in a controller but without two important differences: Razor looks for views in different locations and uses a different default view name if none is specified. Razor is looking for the partial view in the following locations: /Views/Home/Components/Derived/Default.cshtml /Views/Shared/Components/Derived/Default.cshtml /Pages/Shared/Components/Derived/Default.cshtmlIf no name is specified, then Razor looks for a file called Default.cshtml. Razor looks in two locations for the partial view. The first location takes into account the name of the controller handling the HTTP request, which allows each controller to have its own custom view. The second location is shared between all controllers. View for the view component Start the application and you will see the table, produced by the view component in green on top of the customer table. The rendered view from the view component Returning HTML FragmentsThe ContentViewComponentResult class is used to include fragments of HTML in the parent view without using a view. Instances of the ContentViewComponentResult class are created using the Content method inherited from the ViewComponent base class, which accepts a string value. In addition to the Content method, the Invoke method can return a string, and MVC will automatically convert to a ContentViewComponent. View component which returns HTML Getting Context DataDetails about the current request and the parent view are provided to a view component through properties of the ViewComponentContext class. The following table shows the available properties: Name Description Arguments This property returns a dictionary of the arguments provided by the view, which can also be received via the Invoke method. HtmlEncoder This property returns an HtmlEncoder object that can be used to safely encode HTML fragments. ViewComponentDescriptor This property returns a ViewComponentDescriptor, which provides a description of the view component. ViewContext This property returns the ViewContext object from the parent view. ViewData This property returns a ViewDataDictionary, which provides access to the view data provided for the view component. ViewComponent base class PropertiesThe ViewComponent base class provides a set of convenience properties that make it easier to access specific context information: Name Description ViewComponentContext This property returns the ViewComponentContext object. HttpContext This property returns an HttpContext object that describes the current request and the response that is being prepared. Request This property returns an HttpRequest object that describes the current HTTP request. User This property returns an IPrincipal object that describes the current user. RouteData This property returns a RouteData object that describes the routing data for the current request. ViewBag This property returns the dynamic view bag object, which can be used to pass data between the view component and the view. ModelState This property returns a ModelStateDictionary, which provides details of the model binding process. ViewContext This property returns the ViewContext object that was provided to the parent view. ViewData This property returns a ViewDataDictionary, which provides access to the view data provided for the view component. Url This property returns an IUrlHelper object that can be used to generate URLs. The context data can be used in whatever way it helps the view component to its work, including varying the way that data is selected or rendering different content or views. To demonstrate this feature, I changed my view component to read the route data and if there is a value for id, I return a message. Reading RouteData in the view component If I call my action with a value for the id parameter, the string is returned instead of the countries and population. Changing the output of the view component, depending on the route data Providing Context from the Parent View using ArgumentsParent views can provide additional context data as arguments in the Component.Invoke expression. This feature can be used to provide data from the parent view model or to give guidance about the type of content that the view component should produce. I extended my view component to take a bool parameter. If this parameter is true, I return a message, indicating that the parameter was received and processed. Working with the parameter in the view component Next, I change the main view to provide a parameter for my view component. Providing an argument to the view component Start the application and you will see that the previously defined string will be returned to the view. Changing the output of the view component, depending on the provided parameter  Unit Testing View ComponentsView components can be unit tested like every other C# class. I created a new test project and implemented a simple test to demonstrate how to test the Invoke method. Unit testing the view component Creating Asynchronous View ComponentsAll view components so far were executed synchronously. You can create an asynchronous view component by defining an InvokeAsync method that returns a Task. When Razor receives the Task from the InvokeAsync method, it will wait for it to complete and then insert the result into the main view. Creating an async view component Hybrid Controller / View Component ClassesView components often provide basic functionality to enhance the current view with additional data. If you have to do more complex operations, you can create a class that is a controller and a view component. This allows for related functionality to be grouped together and reduces code duplication. The hybrid controller &#8211; view component The ViewComponent attribute is applied to classes that don’t inherit from the ViewCompnent base class and whose name doesn’t end with ViewComponent, meaning that the normal discovery process wouldn’t normally categorize the class as view component. The Name property sets the name by which the class can be referred to when applying the class using the @Component.Invoke expression in the parent view.Since hybrid classes don’t inherit from the ViewComponent base class, they don’t have access to the convenience methods for creating IViewComponentResult object, which means that I have to create the ViewViewComponentResult object directly.Creating Hybrid ViewA hybrid class requires two sets of views: those that are rendered when the class is used as a controller and those that are rendered when the class is used as a view component. First, I add the view for the controller under Views/Country/Create.cshtml The view for the controller Next, I add a the partial view under Views/Shared/Components/HybridComponent/Default.cshml. Partial view for the hybrid controller Lastly, I invoke my hybrid controller from the view with the previously applied name. Invoking the hybrid controller &#8211; view component This renders the (super ugly) form. The rendered view of the hybrid controller ConclusionToday, I talked about view components and how they can help you extending your views and introducing more functionality and more information for the users. You can call them synchronously or asynchronously and also easily unit test them. Lastly, I showed how hybrid controller / view components can be used to group more complex code together.For more details about the configuring ASP.NET Core, I highly recommend the book “Pro ASP.NET Core MVC 2“. You can find the source code for this demo on GitHub." }, { "title": "Dependency Injection in ASP.NET Core MVC", "url": "/dependency-injection-in-asp-net-core-mvc/", "categories": "ASP.NET", "tags": ".NET Core, ASP.NET Core MVC, C#, Dependency Injection", "date": "2019-05-07 11:32:49 +0200", "snippet": "Dependency Injection is a technique that helps to create flexible applications and simplifies unit testing. .NET Core brings dependency injection out of the box, therefore you don’t have to use any third party tools like Autofac or Ninject anymore.Setting up the Demo ApplicationYou can find the source code of the following demo on GitHub.I created a repository which will provide basic operations for my Customer class like Add and Delete. Setting up a repository class for basic operations Next, I created a view which will display the customer’s name and age in a table. The view to display the customer data Lastly, I call this view from the controller with the CustomerRepository creating some data. Hard-coding the usage of the CustomerRepository in the controller The CustomerRepository is hard-coded into the controller. This means that if you want to call the view with a different repository, you have to change the code and recompile it. Also, unit testing is really hard if you don’t have any interfaces to fake. This is where dependency injection comes in handy.Preparing for Dependency InjectionThe term dependency injection (DI) describes an approach to creating loosely coupled components, which are used automatically by MVC. This means that controllers and other components don’t need to have any knowledge of how the types they require are created. Dependency injection might seem abstract in the beginning but let’s take a look at the modified HomeController: Preparing the Home Controller for dependency injection I am passing an IRepository object to the constructor of the class. From now on the class doesn’t have to care to get the right object. The right object will be passed. This behavior is also known as the Hollywood Principle. The only problem with this code is, that nobody tells MVC which object to pass as the IRepository and therefore it passes null which will lead to an exception. Exception when starting the application In the next section, I will configure MVC, so it knows which object it should inject as IRepository.Hollywood PrincipleThe Hollywood Principle says: “Don’t call us, We will call you”. This means don’t do anything until further notice. In a more technical term: don’t instantiate an object yourself and do work with it, wait until someone calls you with an object on which you can do your operation.For example, I have a calculator class which logs all the steps of the calculation. Instead of instantiating the logger itself, the calculator class expects an ILogger interface which it will use to log. If I want to use this calculator class now, I provide it with my logger. This can be a file logger, a console logger or a database logger. The class doesn’t care about. All it does is ILogger.Log(“My log message”). This behavior makes the classes loosely coupled which means they can be extended and tested (most of the time) easily.Configuring the Service ProviderIn the previous section, I added the IRepository interface to the Home controller but it ended in an exception. To fix this problem, all you have to do is the following line to the ConfigureServices method of the Startup class:services.AddTransient&lt;IRepository, ProductRepository&gt;(); Register the IRepository interface as ProductRepository There are three different Methods to register a service, depending on its scope: Transient Scoped SingletonI will explain the differences in the section “Understanding Service Life Cycles”. For now, add the line and that’s all you have to do to fix the previous exception. The application works again Using Dependency Injection for Concrete TypesDependency injection can also be used for concrete types, which are not accessed through interfaces. While this doesn’t provide the loose-coupling advantages of using an interface, it is a useful technique because it allows objects to be accessed anywhere in an application and puts concrete types under lifecycle management.In the following example, I created a new WeatherService class and added it to the Home controller. There, I created a new action which returns only the string provided by the service. It is not the most useful implementation but it shows how it works. Adding the weather service to the Home controller Next, I register the WeatherService in the Startup class. Since there is no mapping between a service type and an implementation type in this solution, you have to use an override of the AddTransient method which accepts a single type parameter that tells the service provider that it should instantiate the WeatherService class to resolve a dependency on this type. Register the WeatherService in the Startup class The advantages of this approach are that the service provider will resolve any dependencies declared by the concrete class and that you can change the configuration so that more specialized sub-classes are used to resolve dependencies for a concrete class. Concrete classes are managed by the service provider and are also subject to lifecycle features, which I will talk about in the next section.If you call the new action, you will see the weather information provided by the WeatherService. Getting data from the weather service Understanding Dependency Injection Service Life CycleIn the last example, I added a dependency using the AddTransient method. This is one of four different ways that type mappings can be defined. The following table shows the extension methods for the service provider dependency injection: Name Description AddTransient&lt;service, implType&gt;() This method tells the service provider to create a new instance of the implementation type for every dependency on the service type. AddTransient() &lt;/td&gt; This method is used to register a single type, which will be instantiated for every dependency. &lt;/tr&gt; AddTransient(factoryFunc) &lt;/td&gt; This method is used to register a factory function that will be invoked to create an implementation object for every dependency on the service type. &lt;/tr&gt; AddTransient(factoryFunc) or AddScoped() or AddScoped(factoryFunc) &lt;/td&gt; These methods tell the service provider to reuse instances of the implementation type so that all service requests made by components associated with a common scope, which is usually a single HTTP request, share the same object. These methods follow the same pattern as the corresponding AddTransient methods. &lt;/tr&gt; AddSingleton&lt;service, implType&gt;() or AddSingleton() or AddSingleton(factoryFunc) &lt;/td&gt; These methods tell the service provider to create a new instance of the implementation type for the first service request and then reuse it for every subsequent service request. &lt;/tr&gt; AddSingleton(instance) &lt;/td&gt; This method provides the service provider with an object that should be used to service all service requests. &lt;/tr&gt; &lt;/table&gt;&lt;/div&gt;###  Using the Transient Life CycleThe simplest way to start using dependency injection is to use the AddTransient method, which tells the service provider to create a new instance of the implementation type whenever it needs to resolve a dependency. The transient life cycle incurs the cost of creating a new instance of the implementation class every time a dependency is resolved, but the advantage is that you don&#8217;t have to worry about managing concurrent access or ensure that objects can be safely reused for multiple requests.In my experience, the transient life cycle is used for most of the applications.#### Using a Factory FunctionOne version of the AddTransient method accepts a factory function that is invoked every time there is a dependency on the service type. This allows the object that is created to be varied so that different dependencies receive instances of different types or instances that are configured differently.To demonstrate this behavior, I extended the ConfigureServices method of the Startup class. First, I inject an IHostingEnvironment object which indicates on which environment the application is running. Afterward, I check this variable and if it is development, I instantiate a ProductRepository as IRepository. Otherwise, I instantiate a CustomerRepository. Instantiate the IRepository interface depending on the hosting environment ### Using the Scoped Life CycleThis life cycle creates a single object from the implementation class that is used to resolve all the dependencies associated with a single scope, which generally means a single HTTP request. Since the default scope is the HTTP request, this life cycle allows for a single object to be shared by all the components that process a request and is most often used for sharing common context data when writing custom classes, such as routes.Note that there are also versions of the AddScoped method that accept a factory function and that can be used to register a concrete type. These methods work in the same way as the AddTransient method. Adding a scoped dependency ### Using the Singleton Life CycleThe singleton life cycle ensures that a single object is used to resolve all the dependencies for a given service type. When using this life cycle, you must ensure that the implementation classes used to resolve dependencies are safe for concurrent access. Adding a singleton dependency ## Dependency Injection in ActionsThe standard way to declare a dependency is through the constructor. Additionally to the standard way, MVC provides the functionality to inject an action, called action injection. Action injection allows dependencies to be declared through parameters to action methods. To be more precise, action injection is provided by the model binding system. All you have to do is using the FromService attribute before your parameter. Also, don&#8217;t forget to register your service in the Startup class. Using action injection MVC uses the services provider to get in instance of the CustomerService class which is used to load all the customers. Using action injection is less common than the constructor injection, but it can be useful when you have a dependency on an object that is expensive to create and that is only required in only one of the actions of a controller. Using constructor injection resolves the dependencies for all action methods, even if the one used to handle the request doesn&#8217;t use the implementation object.## Dependency Injection in PropertiesThe third way to use dependency injection provided by MVC is property injection. Here, a set of specialized attributes is used to receive specific types via property injection in the controllers and view components. You won&#8217;t need to use these attributes if you derive your controllers from the Controller base class because the context information is exposed through convenience properties.The following table shows the specialized property injection attributes: Name Description ControllerContext This attribute sets a ControllerContext property, which provides a superset of the functionality of the ActionContext class. ActionContext This attribute sets an ActionContext property to provide context information to action methods. The Controller classes expose the context information through an ActionContext property. ViewContext This attribute sets a ViewContext property to provide context data for view operations. ViewComponentContext This attribute sets a ViewComponentContext property for view components. ViewDataDictionary This attribute sets a ViewDataDictionary property to provide access to the model binding data. I have never used property binding and don&#8217;t see a use case where to use it. Therefore, I am not going into more detail here.## Manually Requesting an Implementation ObjectMVC provides the dependency feature for you. There can be occasions when it can be useful to create an implementation for an interface without relying on dependency injection. In these situations, you can work directly with the service provider. Resolve an object manually The HttpContext object returned by the property of the same name defines a RequestServices method that returns an IServiceProvider object. This is known as the service locator pattern, which some developers believe should be avoided. I also think that it should be avoided but there might be some cases where it is reasonable to use it. For example, if the normal way of receiving dependencies through the constructor can&#8217;t be used for some reasons.## ConclusionToday, I talked about the different types of dependency injection and how to use them with your ASP.NET Core MVC application. If you start with dependency injection, I would only use constructor injection since it is the most common form of it.For more details about complex configurations, I highly recommend the book &#8220;Pro ASP.NET Core MVC 2&#8220;. You can find the source code of this demo on GitHub." }, { "title": "Dealing with Complex Configurations in ASP.NET Core MVC", "url": "/dealing-with-complex-configurations-in-asp-net-mvc-core/", "categories": "ASP.NET", "tags": ".NET Core, ASP.NET Core MVC, C#", "date": "2019-05-05 11:14:45 +0200", "snippet": "If you have to deal with a large number of hosting environments, configuring all of them in the Startup class can become messy. In the following sections, I will describe different ways that the Startup class can be used for complex configurations. You can find the source code of the following demo on GitHub.Creating different External Configuration FilesThe default configuration for the application performed by the Program class looks for JSON configuration files that are specific to the hosting environment being used to run the application. A file called appsettings.production.json can be used to store settings that are specific to the production platform. Loading the appsettings.json file according to the environment When you load configuration data from a platform-specific file, the configuration settings it contains override any existing data with the same names. The appsettings.json file will be loaded when the application starts, followed by the appsettings.development.json file if the application is running in a development environment.Creating different Configuration MethodsSelecting different configuration data files can be useful but doesn’t provide a complete solution for complex configurations because data files don’t contain C# statements. If you want to vary the configuration statements used to create services or register middleware components, then you can use different methods, where the name of the method includes the hosting environment: Using different method names in the Startup class When ASP.NET Core looks for the ConfigureServices and Configure methods in the Startup class, it first checks to see whether there are methods that include the name of the hosting environment. You can define separate methods for each of the environments that you need to support and rely on the default methods being called if there are no environment-specific methods available. Note that the default methods are not called if there are environment-specific methods defined.Creating different Configuration Classes to handle Complex ConfigurationsUsing different methods means you don’t have to use if statements to check the hosting environment name, but it can result in large classes, which is a problem in itself. For especially complex configurations, the final progression is to create a different configuration class for each hosting environment. When ASP.NET looks for the Startup class, it first checks to see whether there is a class whose name includes the current hosting environment. Choose the Startup class at runtime Rather than specifying a class, the UseStartup method is given the name of the assembly that it should use. When the application starts, ASP.NET will look for a class whose name includes the hosting environment, such as StartupDevelopment or StartupProduction, and fall back to using the regular Startup class if one does not exist.ConclusionIn this post, I showed different approaches on how to handle the configuration of your application. You can use different files, different methods or even different classes, depending on how complex your configuration will be.For more details about complex configurations, I highly recommend the book “Pro ASP.NET Core MVC 2“. You can find the source code of this demo on GitHub." }, { "title": "Configure ASP.NET Core MVC", "url": "/configure-asp-net-core-mvc/", "categories": "ASP.NET", "tags": ".NET Core, ASP.NET Core MVC, C#", "date": "2019-05-03 11:17:40 +0200", "snippet": "Some configurations like the connection string usually change on every environment your application is running on. Instead of hard-coding this information into your application, ASP.NET Core enables you to provide configuration data through different sources such as environment variables, command-line arguments or JSON files. You can find the source code for this demo on GitHub.On the following screenshot, I configure my application in the BuildWebHost method of the program class to read the appsettings.json file, the environment variables and if available the command line arguments. Configure the application to read settings from a JSON file, environment variables and the command line parameter Appsettings.json is the conventional name for the configuration file but you can choose any name you like. The two additional parameters mark the file as optional and enable reloading when the file changes. This enables you to change the configuration during runtime without the need of restarting the web server like you had to do when you changed, for example, the web.config file. Only because you can change the configuration during runtime doesn’t mean that you should since it is a recipe for downtime.The ConfigureAppConfiguration method is used to handle the configuration data and its arguments are a WebHostBuilderContext object and an IConfigurationBuilder object. The WebHostBuilderContext class has two properties: HostingEnvironment ConfigurationThe HostingEnvironment provides information about the hosting environment in which the application is running whereas the Configuration property provides read-only access to the configuration data.The IConfigurationBuilder provides three extension methods: Name Description AddJsonFile This method is used to load configuration data from a JSON file, such as appsettings.json. AddEnvironmentVariables This method is used to load configuration data from environment variables. AddCommandLine This method is used to load configuration data from the command-line arguments used to start the application.  Creating the JSON Configuration FileThe most common uses for the appsettings.json file are to store your connection strings and logging setting, but you can store any data that your application needs. On the following screenshot, I add the ShortCircuitMiddleware section containing EnableBrowserShortCircuit with the value true to the appsettings.json file. Adding values to the appsettings.json file In JSON everything has to be quoted exception bool and number values. If you want to add a new section, add a comma after the closing bracket of the ShortCircuitMiddleware section. Be aware to not add a trailing comma at the end if you don’t have another section there. This and missing quotes are the most common mistakes in a JSON file. Adding more values to the appsettings.json file Using Configuration DataThe Startup class can access the configuration data by defining a constructor with an IConfiguration argument. When the UseStartup method is called in the Program class, the configuration data prepared by the ConfigureAppConfiguration is used to create the Startup object. Setting the Configuration in the Startup constructor The IConfiguration object is received by the constructor and assigned to a property called Configuration, which can then be used to access the configuration data that has been loaded fromenvironment variables, the command line, and the appsettings.json file. To obtain a value, you navigate through the structure of the data to the configuration section you require. The IConfigurationInterface defines the following member variables to do that: Name Description [key] The indexer is used to obtain a string value for a specific key. GetSection(name) This method returns an IConfiguration object that represents a section of the configuration data. GetChildren() This method returns an enumeration of the IConfiguration objects that represent the subsections of the current configuration object. Additionally, the IConfiguration interface provides the following extension methods to get and convert values from string into other data types: Name Description GetValue(keyName) &lt;/td&gt; This method gets the value associated with the specified key and attempts to convert it to the type T. &lt;/tr&gt; GetValue(keyName, defaultValue) &lt;/td&gt; This method gets the value associated with the specified key and attempts to convert it to the type T. The default value will be used if there is no value for the key in the configuration data. &lt;/tr&gt; &lt;/table&gt;&lt;/div&gt;With these extension methods, I read the configuration and if EnableBrowserShortCircuiting is true, I add the ShortCircuitMiddleware to my application. Use the configuration to decide if the middleware should be enabled or not &nbsp;It is important not to assume that a configuration value will be specified. It is good practice to program defensively and check for null using the null conditional operator to ensure that the ShortCircuitMiddleware section was received before its value is used.## Configure LoggingMany of the built-in middlewares already generate logging output. To set up the logging, you have to set it up in the Program class: Configure logging in the Program class The ConfigureLogging method sets up the logging system using a lambda function that receives a WebHostingBuilderContext object and an ILoggingBuilder object. The ILoggingBuilder interface provides the following extension methods: Name Description AddConfiguration This method is used to configure the logging system using the configuration data that has been loaded from the appsettings.json file, from the command line, or from environment variables. AddConsole This method sends logging messages to the console, which is useful when starting the application using the dotnet run command. AddDebug This method sends logging messages to the debug output window when the Visual Studio debugger is running. AddEventLog This method sends logging messages to the Windows Event Log, which is useful if you deploy to Windows Server and want the log messages from the ASP.NET Core MVC application to be incorporated with those from other types of application. ### Understanding the Logging Configuration DataConfiguration data for the logging is usually defined in the appsettings.json file. Configuring logging in the appsettings.json file ASP.NET has seven debugging levels: Name Description None This level is used to disable logging messages. Trace This level is used for messages that are useful during development but that are not required in production. Debug This level is used for detailed messages required by developers to debug problems. Information This level is used for messages that describe the general operation of the application. Warning This level is used for messages that describe events that are unexpected but that do not interrupt the application. Error This level is used for messages that describe errors that interrupt the application. Critical This level is used for messages that describe catastrophic failures. ###  Creating Custom Log MessagesThe logging messages in the previous section were generated by the ASP.NET Core and MVC components that handled the HTTP request and generated the response. This kind of message can provide useful information, but you can also generate custom log messages that are specific to your application. Create your own log message The ILogger interface defines the functionality required to create log entries and to obtain an object that implements this interface. The value of the constructor argument is provided automatically through dependency injection. The ILogger interface is in the Microsoft.Extensions.Logging namespace. This namespace defines extension methods for each logging level.## Configuring MVC ServicesWhen you call AddMvc in the ConfigureServices method of the Startup class, it sets up all the services that are required for MVC applications. This has the advantage of convenience because it registers all the MVC services in a single step but does mean that some additional work is required to reconfigure the services to change the default behavior. The AddMvc method returns an object that implements the IMvcBuilder interface and MVC provides a set of extension methods that can be used for advanced configuration. These extension methods are: Name Description AddMvcOptions This method configures the services used by MVC. For more details see the next table. AddFormatterMappings This method is used to configure a feature that allows clients to specify the data format they receive. AddJsonOptions This method is used to configure the way that JSON data is created. AddRazorOptions This method is used to configure the Razor view engine. AddViewOptions This method is used to configure how MVC handles views, including which view engines are used. The AddMvcOptions method configures the most important MVC services. It accepts a function that receives an MvcOptions object, which provides one of the following set of configuration properties: Name Description Conventions This property returns a list of the model conventions that are used to customize how MVC creates controllers and actions. Filters This property returns a list of the global filters. FormatterMappings This property returns the mappings used to allow clients to specify the data format they receive. InputFormatters This property returns a list of the objects used to parse request data. ModelValidatorProviders This property returns a list of the objects used to validate data. OutputFormatters This property returns a list of the classes that format data sent from API controllers. RespectBrowserAcceptHeader This property specifies whether the Accept header is taken into account when deciding what data format to use for a response These configuration options are used to fine-tune the way your MVC application works.## ConclusionIn this post, I talked about configuring your ASP.NET Core MVC application. I showed how to enable logging, add built-in MVC functionalities and how to create a JSON file from which the application can read its configuration. If you want to learn more about more complex configurations, check my post Dealing with Complex Configurations in ASP.NET MVC Core.For more details about the configuring ASP.NET Core, I highly recommend the book &#8220;Pro ASP.NET Core MVC 2&#8220;. You can find the source code for this demo on GitHub." }, { "title": "Middleware in ASP.NET Core MVC", "url": "/middleware-in-asp-net-core-mvc/", "categories": "ASP.NET", "tags": ".NET Core, ASP.NET Core MVC, C#", "date": "2019-05-01 11:17:11 +0200", "snippet": "Middleware is the term used for the components that are combined to form the request pipeline. This pipeline is arranged like a chain. The request is either returned by the middleware or passed to the next one until a response is sent back. Once a response is created, the response will travel the chain back, passing all middlewares again, which allows them to modify this response.It may not be intuitive at first, but this allows for a lot of flexibility in the way the parts of an application are combined. Middlewares in ASP.NET Core MVC (Source) You can find the source code of the following demo on GitHub.Creating a Content-Generating MiddlewareA middleware can be a pretty simple class since it doesn’t implement an interface and doesn’t derive from a base class. Instead, the constructor takes a RequestDelegate object and defines an Invoke method. The RequestDelegate links to the next middleware in the chain and the Invoke method is called when the ASP.NET application receives an HTTP request. All the information of the HTTP requests and responses are provided through the HttpContext in the Invoke method.On the following screenshot, you can see a simple implementation of a middleware which returns a string  if the request path is /contentmiddleware The content-creating middleware implementation The request pipeline or chain of middlewares is created in the Configure method of the Startup class. All you have to do is app.UseMiddleware(); A request is only passed through a middleware when it is registered in the Startup class. Register the content-creating middleware That’s already everything you have to do to use the middleware. Start the application and enter /contentmiddleware and you will see the response from the middleware. Response from the content-creating middleware Creating a Short-Circuiting MiddlewareA short-circuiting middleware intercepts the request before the content generating components (for example a controller) is reached. The main reason for doing this is performance. This type of middleware is called short-circuiting because it doesn’t always forward the request to the next component in the chain.  For example if your application doesn’t allow Chrome users, the middleware can check the client agent and if it is Chrome, a response with an error message is created. The short-circuiting middleware It is important to note that middlewares are called in the same order as they are registered in the Startup class. The middleware which is registered first will handle the request first. Short-circuiting middlewares should always be placed at the front of the chain. The order of the middlewares is important If you make a request from Chrome (also from Edge since it is using Chromium now), you will see the access denied message. Calls with Chrome are blocked Calls with a different browser, for example, Firefox still create a response. Calls with Firefox are allowed Creating a Request-Editing MiddlewareA request-editing middleware changes the requests before it reaches other components but doesn’t create a response. This can be used to prepare the request for easier processing later or for enriching the request with platform-specific features. The following example shows a middleware which sets a context item if the browser used is Edge. The request-editing middleware Creating a Response-Editing MiddlewareSince there is a request-editing middleware, it won’t be surprising that there is also a response-editing middleware which changes the response before it is sent to the user. This type of middleware is often used for logging or handling errors. The response-editing middleware Now register the middleware in your Startup class. It may not be intuitive but it is important that a response-editing middleware is registered first because the response passes all middlewares in the reverse order of the request. This means that the first middleware processes the request first and the response last. Register the response-editing middleware first If you start your application and enter an URL which your system doesn’t know (and don’t use Chrome), you will see your custom error text. Response created by the middleware ConclusionIn this post, I talked about middlewares. I explained what they are and what different types there are.For more details about complex configurations, I highly recommend the book “Pro ASP.NET Core MVC 2“. You can find the source code of the demo on GitHub." }, { "title": "Getting to know the Startup Class of ASP.NET Core MVC", "url": "/getting-to-know-the-startup-class-of-asp-net-core-mvc/", "categories": "ASP.NET", "tags": ".NET Core, ASP.NET Core MVC, C#", "date": "2019-04-29 11:15:53 +0200", "snippet": "Every .NET Core web application has a Program class with a static Main method.The Startup class of .NET core is the new version of the Global.asax file. This class is responsible for starting the application. The most important part of it is .UseStartup(). This delegate calls the Startup class in which all the configuration for the web application is handled. The UseStartup method relies on a type parameter to identify the class that will configure the .NET Core application. This means if you don&#8217;t want to use the default Startup class, you can edit the call, for example with .UseStartup().The Startup class has two methods, ConfigureServices and Configure, that tell ASP.NET Core which features are available and how they should be used.You can find the source code for this demo on GitHub.When the ASP.NET Core starts, the application creates a new instance of the Startup class and calls the ConfigureServices method to create its services. After the services are created, the application calls the Configure method. This method sets up middlewares (the request pipeline) which are used to handle incoming HTTP requests. Examples for middlewares are logging and authentication.Adding MVC functionality in the Startup classTo enable the MVC functionality, you have to add MVC service to your service collection and a default route to your application. This can be done with services.AddMvc() (optional with a specific version) and app.UseMvcWithDefaultRoute(). That’s already enough to start your web application. Add MVC functionality to the web application The add.MVC() method sets up every service that MVC needs. There is no need to add every needed service and therefore the configuration of your application stays small and simple.Taking a closer look at the Configure Method of the Startup classThe Configure method has two parameters of type IApplicationBuilder and IHostingEnvironment. The IApplicationBuilder is used to set up the functionality of the middleware pipeline whereas the IHostingEnvironment enables the application to differentiate between different environment types, for example, testing and production.Using the Application BuilderAlmost every application will use the IApplicationBuilder because it is used to set up the MVC or your custom middlewares. Setting up the MVC pipeline can be done by using UseMvcWithDefaultRoute or with UseMvc. The first method will set up with a default route, containing {controller}/{action}/{id?}. UseMvc can be used if you want to set up your own routes using lambda expressions.Usually, applications use the UseMvc method, even if the routes look the same as the default route. This approach makes the used routing logic more obvious to other developers and easier to add new routes later. Adding a the default route with UseMvc MVC also sets up content-generating middleware, therefore your custom middlewares should be registered first and the MVC one last.Using the Hosting EnvironmentThe IHostingEnvironment interface provides the following properties and information about the hosting environment: Name Description ApplicationName This returns the name of the application which is set by the hosting platform. EnvironmentName This string describes the current environment, for example, test or production. ContentRootPath This property returns the path that contains the application&#8217;s content and configuration files. WebRootPath The string which specifies the location of the container for static content. Usually, this is the wwwroot folder. ContentRootFileProvider This property returns an object that implements the IFileProvider interface and that can be used to read files from the folder specified by the ContentRootPath property. WebRootFileProvider This property returns an object that implements the IFileProvider interface and that can be used to read files from the folder specified by the WebRootPath property. The ContentRootPath and WebRootPath might be interesting properties but are not needed in most real-world applications because MVC provides a built-in middleware which can be used to deliver static content.Probably the most important property is EnvironmentName because it allows you to modify the configuration of the application based on the environment in which it is running. There are three conventional environments: development, staging, and production. You can set the current hosting environment by setting an environment variable called ASPNETCORE_ENVIRONMENT.You can set this variable by selecting ConfiguringAppsProperties from the Visual Studio Project menu and switching to the Debug tag. There double-click the Value field of the environment variable and change it, for example, to Staging. It is common practice to use Staging but the value is not case-sensitive, so you could also use staging. Additionally, you can use every name you want, these previously mentioned ones are only conventional ones. Setting the environment variable to Development You can use IHostingEnvironment.EnvironmentName in the Configure method to determine which hosting environment is being used. Also, you could use one of the extension methods of IHostingEnvironment: Name Description IsDevelopment() Returns true if the hosting environment name is Development. IsStaging() Returns true if the hosting environment name is Staging. IsProduction() Returns true if the hosting environment name is Production. IsEnvironment(env) Returns true if the hosting environment name matches the variable. On the following screenshot, I set up all my custom middlewares only if the application is running on the Development environment. This is useful for gathering debugging or diagnostic information. Register the middleware only if the hosting environment is Development In production, none of these middlewares would be added.Configuring Exception Handling in the Startup classIn a classic ASP.NET MVC application, you had to configure the exception handling in the web.config. The custom error section there often was configured to show detailed error pages and was changed to hide an exception and redirect on an error page on the production environment.With ASP.NET Core, you can easily configure this using the IHostingEnvironment and a built-in exception handler middleware. Configure exception handling depending on the hosting environment The UseStatusCodePages method adds descriptive messages to responses that contain no content, such as 404 – Not Found responses, which can be useful since not all browsers show their own messages to the user. The UseDeveloperExceptionPage method sets up an error-handling middleware component that displays details of the exception in the response, including the exception trace. This isn’t information that should be displayed to users, so the call to UseDeveloperExceptionPage is made only in the development hosting environment, which is detected using the IHostingEnvironmment object.On any other environment, the user gets redirected to /Home/Error. You can create the Error action in the Home controller and display a nice error page to the user, providing some helpful information.Enabling Browser LinkBrowser Link is a feature in Visual Studio that creates a communication channel between the development environment and one or more web browsers. You can use Browser Link to refresh your web application in several browsers at once, which is useful for cross-browser testing.The server-side part of Browser Link is implemented as a middleware component that must be added to the Startup class as part of the application configuration, without which the Visual Studiointegration won’t work. Browser Link is useful only during development and should not be used in staging or production because it edits the responses generated by other middleware components to insert JavaScript code that opens HTTP connections back to the server side so that it can receive reload notifications. Enable Browser Link You also have to install the Microsoft.VisualStudio.Web.BrowserLink NuGet package.Enabling Static ContentThe UseStaticFiles method adds a short-circuiting middleware which provides access to the files in the wwwroot folder so that the application can load images, JavaScript and CSS files. Enable static content I can’t think of an application which doesn’t need static content, no matter of the environment it is running in. Therefore, I add it for all environments. This change will also make the default page work since it is loading the necessary CSS files now.ConclusionIn this post, I showed different ways to configure your application using the Startup class. I talked about adding static content, exceptions and environment specific configurationsFor more details about the Startup class, I highly recommend the book “Pro ASP.NET Core MVC 2“. You can find the source code for this demo on GitHub." }, { "title": "Horizontally Scaling Pattern", "url": "/horizontally-scaling-pattern/", "categories": "Cloud", "tags": "Architecture Pattern, Cloud, Scaling", "date": "2019-03-05 09:18:50 +0100", "snippet": "In my last post, I talked about vertically scaling vs. horizontally scaling with a focus on vertically scaling. Today, I will talk about it again, but I will focus on horizontally scaling this time.Why use Scaling?Surveys say the faster a website is, the higher its generated revenue. Therefore scaling is a business concern. Google observed that a 500 millisecond longer response time on a website reduces the traffic by 20%. Amazon claims that a 100 milliseconds delay costs them 1% revenue. Microsoft reported that if an engineer can increase the speed of their search platform Bing by 20 milliseconds, he or she increased the revenue by his yearly salary.Additionally, to the revenue loss of a slow website, Google is also including the page speed in its index algorithm. The slower your website is, the lower your ranking will be.Vertically Scaling Up vs. Horizontally Scaling outVertically scaling means increasing the performance by increases the power of the hardware. This can be a faster CPU, more RAM or using SSDs instead of HDDs. This was a common approach in the past. If your computer was too slow, replace it with a faster one and the problem is solved. The problem with vertically scaling is that the available hardware is limited, therefore the scaling possibilities are limited.Horizontally scaling increases the performance of the application by adding more computing nodes. This means instead of a single web server you have multiple web server delivering your website to your visitors. The requests are handled by a load balancer which spreads the requests evenly to all server. The advantage of this approach is that you are not limited by hardware. If you need more performance, add more computing nodes. Horizontally scaling got popular with the rise of cloud providers like Azure or AWS.The advantage of horizontally scaling is that you can use several cheap new servers to handle the workload whereas a high-end CPU or SSD might cost way more than 10 average server, which still can provide more performance.Describing Scalability GoalsScalability goals help to formulate the requirements for an SLA. These could be being able to serve 100 concurrent users or that the response time is always below two seconds. Another requirement could be that the infrastructure automatically scales up and down with the goal that the CPU usage is always between 50 and 70 percent. The goals depend on the kind of application and its criticality.LimitationsScaling horizontally is almost limitless, especially when you are using a cloud provider. Your architecture changes if you are using a horizontally scaling approach. To distribute the workload to your servers you will need one or more load balancer and you also have to figure out a way to handle the session if you operate a website since a user can be processed by one server his or her next request is processed by a different server.Scaling Advantages of Cloud SolutionsAs already previously mentioned, the scaling capabilities in the cloud are almost endless. You might not be able to scale immediately if you need an immense amount of performance like Facebook or Google would need but you will get the needed extra hardware provided by your cloud provider. Cloud provider like Azure also offer load balancer to distribute the workload to different data centers around the world, depending on the location of the user and also can distribute the workload within a data center.Handling the user session is also very simple with Azure. You can either use sticky session which configures the load balancer to send a request from a user always to the same server or even better, you can save the session in a database or more preferably in a faster cache server like Redis. Either option can be configured within minutes.One of the biggest advantages with cloud providers is that you can easily scale down if you don’t need the extra performance anymore and therefore don’t have to pay for it. You can turn off your on-premise server but you still have the storage or initial investment costs.ConclusionThis post gave a quick introduction into vertical scaling, compared it with horizontal scaling and discussed the differences between scaling your on-premise server versus scaling a server in the cloud. If you want to learn more about cloud architecture patterns, I can recommend the book “Cloud Architecture Pattern” from O’Reilly." }, { "title": "Vertically Scaling Pattern", "url": "/vertically-scaling-pattern/", "categories": "Cloud", "tags": "Architecture Pattern, Cloud, Scaling", "date": "2019-02-28 19:40:33 +0100", "snippet": "Scalability of a system is a measure of the number of users it can handle at the same time. For a web server, for example, would this mean how many concurrent users it can serve. Serving more users can be achieved by vertically scaling up or horizontally scaling out. In this post, I will explain how vertically scaling up work. If you want to learn more about horizontally scaling out, you can read about it here.Why use Scaling?Surveys say the faster a website is, the higher its generated revenue. Therefore scaling is a business concern. Google observed that a 500 millisecond longer response time on a website reduces the traffic by 20%. Amazon claims that a 100 milliseconds delay costs them 1% revenue. Microsoft reported that if an engineer can increase the speed of their search platform Bing by 20 milliseconds, he or she increased the revenue by his yearly salary.Additionally, to the revenue loss of a slow website, Google is also including the page speed in its index algorithm. The slower your website is, the lower your ranking will be.Vertically Scaling Up vs. Horizontally Scaling outVertically scaling means increasing the performance by increases the power of the hardware. This can be a faster CPU, more RAM or using SSDs instead of HDDs. This was a common approach in the past. If your computer was too slow, replace it with a faster one and the problem is solved. The problem with vertically scaling is that the available hardware is limited, therefore the scaling possibilities are limited.Horizontally scaling increases the performance of the application by adding more computing nodes. This means instead of a single web server you have multiple web server delivering your website to your visitors. The requests are handled by a load balancer which spreads the requests evenly to all server. The advantage of this approach is that you are not limited by hardware. If you need more performance, add more computing nodes. Horizontally scaling got popular with the rise of cloud providers like Azure or AWS.Describing Scalability GoalsScalability goals help to formulate the requirements for an SLA. These could be being able to serve 100 concurrent users or that the response time is always below two seconds. Another requirement could be that the infrastructure automatically scales up and down with the goal that the CPU usage is always between 50 and 70 percent. The goals depend on the kind of application and its criticality.LimitationsScaling vertically is limited by the available hardware. Modern CPUs have up to 24 cores and you can maybe stack a terabyte of ram into your server. No matter how powerful your server is, it will always too little to handle all the users of Google or Facebook.Sometimes you have only certain peak times, like Black Friday or Christmas and the server is idle for the rest of the year. This is were hosting your application in the cloud comes in handy since it enables you to scale down to decrease the costs of your infrastructure when you don’t need it.Scaling Advantages of Cloud SolutionsScaling a cloud-native application is as easy is it possible could be. Cloud provider like Azure provides you with the option to automatically scale your server to a higher grade if a specific event occurred. This could be the CPU or RAM usage was higher than 80% for five minutes. The big advantage of applications in the cloud is that you only have to do some mouse clicks and you have a more powerful server. To scale your on-premise server, you have to order new hardware and install it or move your whole application to a more powerful server. This also means that you have the investment costs.ConclusionThis post gave a quick introduction into horizontal scaling, compared it with vertical scaling and discussed the differences between scaling your on-premise server versus scaling a server in the cloud. If you want to learn more about cloud architecture patterns, I can recommend the book “Cloud Architecture Pattern” from O’Reilly." }, { "title": "Release faster with Feature Toggles", "url": "/release-faster-feature-toggles/", "categories": "DevOps", "tags": ".NET Core, C#, Continous Deployment", "date": "2019-02-17 14:59:31 +0100", "snippet": "Nowadays, more and more teams do (finally) continuous integration and continuous deployment (CI/CD). Unfortunately, even if the development team was able to do CI/CD, the business department is often a limiting factor since they want to test the feature before it is deployed. So only teams without a limiting business department can do CI/CD? No, the solution to this problem are feature toggles. Today, I will show how you can CI/CD and still satisfy your bureaucratic business department.What are Feature Toggles?Feature toggles or feature flags enable you to turn on or off your features. The main advantage is that you can deploy features which are not tested and hide them behind a feature toggle. Another usage of feature toggles can be to show different features to different users.  To understand this concept better, let me explain you to the difference between these two types of feature toggles in the next sectionRelease Feature Toggles vs. Business Feature ToggleRelease feature toggles are used by the development team to deploy new features. These features can be untested or only needed at a specific time in the future. The advantage of feature toggles is that you can deploy all your features continuously and also can have them all in the same branch. So you don’t need several branches for future features. Release feature toggles stay in the application only for a short time and will be removed as soon as they are not needed anymore.Business feature toggles, in contrast, are long living toggles and can make your application individual to a user role for example. You could have a free user where you have a limit set of features and a paid version with all features. The additional features for the paid version are hidden behind release feature toggles.Configuring Feature TogglesThere are three different approaches on how to configure your feature toggles. Choose the one which fits your system best.Compiled configurationThe configuration of the toggles is done during the deployment. This can be done by a simple hard-coded if statement within your code. The downside of this approach is, that a new release is needed if you want to change the configuration.Local configurationAnother approach is to put the configuration in a local file like the appsettings.json or the web.config file. The advantage is that no new release is needed to change the configuration. The downside is that every server needs its own config file.Centralized configurationThe last configuration is the centralized configuration. Here you have your configuration on a central location like a database or a file share. The advantages of this approach are that no release is needed to apply changes and that all systems can be managed centrally.Manually implementing Feature TogglesIn this simple example, I will write the configuration in the appsettings.json file of my .NET core web application and configure the Startup class to read the value. Afterward, I will change the title of my page, depending on the value of the toggle. I am calling my toggle in the appsettings.json FeatureToggle and set the IsEnabled property to true. Enable the toggle in the appsettings.json file Next, I enable Options in the Startup class to read the settings individually and then read my value into the MyFeatureToggleClass. Set up reading the appsettings file I put the MyFeatureToggle class in a new folder called Toggles and its only property is a bool indicating whether it is enabled. The feature toggle class As a simple test if my set up is working, I read the value and if my toggle is enabled, I set the view title to “Toggle is enabled”. Otherwise, I display “Toggle is disabled”. Setting the title depending on the value of the toggle Start the application and click the Privacy menu item. Now you will see that the toggle is enabled. Test the enabled feature toggle Limitations of the manual approachA problem with the manual implementation is that you have magic strings in your code (in your Startup class) which can lead to runtime exceptions if you have a typo in it. Another exception can occur if you delete a feature toggle but forget to remove the check inside the code. Therefore, it is a good idea to take a look at one of the many open source libraries.Libraries for Feature TogglesThere is a number of open source libraries for feature toggles such as: NFeature FeatureSwitcher FeatureToggleIn the following section, I will introduce you to the FeatureToggle library.FeatureToggleTo install the FeatureToggle library, you only have to download the NuGet package. The advantage of using the library is that you don’t have any magic strings in your code and also no default values which might lead to unexpected behavior if no setting for the feature toggle is available.The FeatureToggle library provides the IFeatureToggle interface which has to be implemented by your feature toggle class. The biggest advantage of the library is that it comes with a couple of built-in feature toggles like: AlwaysOnFeatureToggle / AlwaysOffFeatureToggle EnabledOnOrAfterDateFeatureToggle / EnabledOnOrBeforeDateFeatureToggle EnabledBetweenDatesFeatureToggle EnabledOnDaysOfWeekFeatureToggle RandomFeatureToggleAlwaysOnFeatureToggle / AlwaysOffFeatureToggleThis is the simplest feature toggle available. It is either always on or off, depending on its name.EnabledOnOrAfterDateFeatureToggle / EnabledOnOrBeforeDateFeatureToggleThis toggle helps you to activate or deactivate a feature before or after a certain date. For example, this can be used to enable a sale coupon after a certain date.EnabledBetweenDatesFeatureToggleThe enable between date toggle is similar to the before mentioned one. It can be used to enable a feature only during a specific time span. For example for a sale between Christmas and New Years Eve.EnabledOnDaysOfWeekFeatureToggleWith this toggle, you can Enable your feature on one or several days of the week. For example, you can have a sale code which is only available from Friday until Sunday.RandomFeatureToggleAs the name already suggests, this toggle is enabled randomly. This is great to do A/B testing because it shows a feature only to a random group of people. For example, this group sees a new search. Measure the conversation rate and compare it to the old search feature and so you can see if the new search performs better than the old one or not.ConclusionIn this post, I showed how you can use Feature Toggles to continuously deploy your new features, even without activating them for the user. Additionally, I showed how to implement your own feature toggles and how to use the open source library FeatureToggle. You can find the code for the manually implemented feature toggles on GitHub.If you want to learn more about Feature Toggles, I can recommend the Pluralsight course “Implementing Feature Toggles in .NET with FeatureToggle” by Jason Roberts." }, { "title": ".NET Standard - Getting Started", "url": "/net-standard-getting-started/", "categories": "Programming", "tags": ".NET, .NET Core, .NET Standard, C#", "date": "2019-02-12 09:35:00 +0100", "snippet": "Today, some of my colleagues had a discussion about .NET Standard. Is it a new framework, an extension to classic .NET framework, or to .NET core? Confusion was great and in today’s post, I would like to shed light on the matter.What is .NET Standard?.NET Standard is a specification which defines a set of APIs which the .NET platform has to implement. It is not another .NET platform though. You can only build libraries, not executables with it. On the following screenshot, you can see that .NET Standard contains APIs from the classic .NET framework, .NET core and Xamarin. What is .NET Standard (Source) The following screenshot shows that it defines a set all APIs that all .NET frameworks implement. Implementation of .NET Standard (Source)  The difference to Portable Class LibrariesSome of you might remember portable class libraries, which sound like .NET Standard. Both technologies have the same idea but a portable class library needs to be recompiled every time you want to use it for a different target. .NET Standard doesn’t have to be recompiled to be used for a different target. Check out the following screenshot to compare the differences: Portable Class Library vs .NET Standard (Source) Portable class libraries are deprecated because .NET Standard is better in every way and therefore shouldn’t be used anymore.Choosing the right VersionA new version of .NET Standard always contains all previous APIs and additional ones. The following screenshot shows how a new version is built on all previous ones: Every version is built on the previous one (Source) A .NET platform implements a specific .NET Standard version, for example .NET Core 1.0 implements .NET Standard 1.6. The enforce this backward compatibility, every .NET Standard version is immutable.Which Version to choose?The best practice is to start with a high version number and implement all your features. Then target the lowest version possible. For example, start with 2.0 and then decrease to 1.6, then 1.5  until your project doesn’t compile anymore.Find out which Version a .NET Platform implementsMicrosoft has some great documentation about which .NET Standard version is implemented by which .NET framework version. .NET Standard 1.0 1.1 1.2 1.3 1.4 1.5 1.6 2.0 .NET Core 1.0 1.0 1.0 1.0 1.0 1.0 1.0 2.0 .NET Framework 4.5 4.5 4.5.1 4.6 4.6.1 4.6.1 4.6.1 4.6.1 If you are looking for a specific API, you can go to https://docs.microsoft.com/en-gb/dotnet/api/ and search for it.Version 1.6 has around 13,000 APIs whereas version 2.0 has already around 32,000 APIs which includes for example Primitives, Collections, Linq or Files Some APIs of 2.0 (Source)  Migrating an existing projectMigrating to .NET Standard just for the sake of migrating is not the best strategy. It makes sense to migrate if the heart of your library is .NET Standard compatible and if you want to use it on different .NET platform.How to migrateOpen the .csproj file of the classic .NET framework project you want to migrate and delete everything. Then copy a new Project tag with the target framework of netstandard in it. If you want to migrate a .NET core project, you only have to change the target framework to netstandard. For details see the following screenshot: Migrate to .NET Standard If you are migrating a .NET core project, you are already done. For your .NET framework project, you have to delete the AssemblyInfo.cs and the packages.config files. Then you have to reinstall your NuGet packages. The reason why you don’t have to do that for .NET core is because it uses package referencing and not the packages.config.Targeting multiple platformsIf you want to target multiple frameworks, for example, .NET Standard and .NET 4.6.1, you only have to change the TargetFramework tag in the .csproj file to TargetFrameworks and separate the different framework with a semicolon. Target multiple platforms If you use multiple target platforms, you can use if statements to use different code, depending on your target framework: Execute code depending on the target platform ConclusionIn this short post, I explained the basics of .NET Standard and pointed out why it is better than the deprecated portable class library. Additionally, I showed how to migrate your existing project and how to target multiple platforms. For more information, I can highly recommend the Pluralsight course “.NET Standard: Getting Started” by Thomas Claudius Huber." }, { "title": "xBehave - Getting Started", "url": "/xbehave-getting-started/", "categories": "Programming, DevOps", "tags": "ATDD, C#, TDD, xBehave, xUnit", "date": "2019-01-30 23:30:26 +0100", "snippet": "In my last post, I talked about writing unit tests using xUnit. Today, I want to go one step further and will talk about writing acceptance tests (ATDD) using xBehave. xBehave is an extension of xUnit for describing tests using natural language. The advantage of the natural language description is that non-technical stakeholder like a manager can read the test report and understand what was tested.My SetupFor writing tests I use the following NuGet packages and extensions: xUnit for unit testing xBehave for acceptance tests FluentAssertions for more readable assertions FakeItEasy to create fake objects xUnit Resharper Extension for xUnit shortcuts in Visual Studio xunit.runner.visualstudio for running the xBehave testsTo start, I use the code of my last demo which can be found on GitHub.Execute tests with xBehaveA test in xBehave always has the attribute Scenario. As previously mentioned, xBehave describes the tests using natural language. The default describing patter is Given, When, Then. Other patterns are possible like the classic Arrange, Act, Assert or xSpec style with Establish, Because, It. You could even extend xBehave and use your own attribute name and vocabulary. If you have more than one operation for the same, you can combine them with And, for example, “Given an employee” “And another employee”.GivenIn the Given or Arrange phase, you set up all the objects you need to run your test. In my example test, I only need one employee object, You could set up several objects and combine them with And. Setting up two employee objects for the tests WhenIn the When or Act phase, you call the methods, you want to test. In my example, I add the hours worked during the week to my employee. Executing a method with my test object ThenIn the Then or Assert phase, you check if the value or result is what you expected. In my example, I check if the salary of the employee has the value I expect. Evaluate the result of the previous operation Repeating stepsYou can always return to a previous phase. For example, after the salary check from above, you can have another method call with when and then another assert with then. Repeating testing steps Executing code before the testIn xUnit, the constructor of the class is called before a test is executed. xBehave is very similar, but instead of the constructor, it calls the method with the Background attribute. Usually, the method is also called Background but you can choose whatever name fits best for you. Setting up code before the test execution Adding parameters to your testIn xBehave you can add parameters to your test with the parameter Example. It works the same way as InlindeData in xUnit. You add values and then have matching parameters in the method signature. Add parameters to your test Instead of Example, you can use any attribute which inherits from DataAttribute, for example, MemberData.Variables inside your testIf you want to assign a value to a variable within your test, you can pass this variable as a parameter. Each parameter which does not have a corresponding example value (based purely on the number of values/parameters) continues to have its default value passed (null for reference types and zero values for value types). Adding a variable as a parameter to be used in the test Skipping a scenarioTo exclude a scenario from execution, you can apply the skip attribute and provide a message. It works the same way as in xUnit. Skipping a scenario Faking objects and calls within the testYou can set up fake objects and fake calls the same way as in xUnit, using a faking framework like FakeItEasy. Faking objects inside the test (Source xBehave) TeardownIf your code needs some cleanup after execution, xBehave provides you with the Teardown method. Provide a delegate to the Teardown method and xBehave will execute your method after the test was run or if an exception occured. On the following screenshot, you can see that in the Teardown method, the employee object calls the Destroy method which does whatever is necessary to clean up the employee object. Teardown an object ConclusionIn this post, I gave a quick overview of xBehave and explained how to set up your scenarios and how to clean up afterward.You can find the code of my demo on GitHub." }, { "title": "xUnit - Getting Started", "url": "/xunit-getting-started/", "categories": "Programming, DevOps", "tags": "C#, TDD, xUnit", "date": "2019-01-27 21:57:57 +0100", "snippet": "In this post, I will explain the basics of xUnit and how to write unit tests with it. xUnit is an open source testing framework for the .NET framework and was written by the inventor of NUnit v2. More details can be found on xUnit’s GitHub page. xUnit is used by .NET core as the default testing framework and its major advantage over NUnit is that every test runs in isolation, which makes it impossible that test influence each other.My SetupFor writing unit tests I use the following NuGet packages and extensions: xUnit for unit testing xBehave for acceptance tests (xBehave is based on xUnit) FluentAssertions for more readable assertions FakeItEasy to create fake objects xUnit Resharper Extension for xUnit shortcuts in Visual StudioThe code for today’s demo can be found on GitHub. Keep in mind that the tests are only for the demonstration of xUnit. The tests are barely useful.Execute a test with xUnitFor each class I want to test, I create a separate class to which I add tests in the name, for example, if I want to test the Employee class I name my test class EmployeeTests. To create a test method, you only have to add the Fact attribute to the method. Using the Fact attribute That’s all. You can run the test and if the constructor of your Employee class sets the salary to 1000, the test will pass. I like to name the object I want to test testee. Another common name is sut which stands for system under test.Reducing code duplicationIn the intro, I mentioned that every test runs in isolation in xUnit. This is done by creating a new instance for each test. Therefore the constructor is called for each test and can be used to initialize objects, which are needed for the tests. Since I will need the object of the Employee class in all my tests, I can initialize it in the constructor and don’t have to write the same code over and over in every test. Initialize the testee in the constructor Cleaning up after testsSometimes you have to do some cleanup like a database rollback or deleting a file after the tests were executed. Like the constructor, this can be done in a central place for all tests. To do that implement the IDisposable interface and implement the Dispose method. This method is called every time a test is finished. Implement the IDisposable interface Executing tests several times with different parametersOften you want to execute a test with different parameters, for example, if a valid age for your employee has to be between at least 18 and maximum 65 years, you want to test the edge cases (17, 18, 65, 66). Additionally, you might test negative numbers. You could write several asserts but this would be a lot of typing and not really practical. The solution for this is the Theory attribute in xUnit. A Theory allows you to pass values from different sources as parameters to your test method. With the InlineData attribute, you can add values for the parameter. Executing the same method with several input variables If you run this test method, five test cases will be executed.Skipping a testSometimes you don’t want a test to be executed. To ignore tests, add the Skip attribute and provide an info message. Skipping a test Grouping tests togetherI barely use this feature but sometimes you want to group certain tests together. This can be for example all tests from one class and only some tests from another class. To do that use the Trait attribute and provide a name and category. You can apply this attribute to a class or to a single test. Applying the Trait attribute to a test method Applying the Trait attribute to a test class If you run the tests and group the output by category, all traits with the same category will be grouped together. Grouped test output Add information to the test result outputBy default, no output is generated when a test finished. For reporting reasons, it can be useful to add some information on what the test did to the output of the test. This can be done with the ITestOutputHelper. Pass it as parameter in the constructor of your test class and initialize a private field with it. Using the ITestOutputHelper Next, use the WriteLine method of the ITestOutputHelper object to create the desired output. Creating a custom message for the test result When you run the test, you will see the message in the test result window. Output message of the test Share resources over multiple testsPreviously, I mentioned that for every test a new object is instantiated and therefore isolated from the other tests.  Sometimes you need to share a resource with several tests. This can be done with Fixtures. First, you have to create a so-called fixture class with the information you want to share. In my simple example, I set DateTime.Now to demonstrate that every test uses the same instance of the object. Fixture class to share the time property Next, I am creating a collection class with the CollectionDefiniton attribute and the ICollectionFixture interface with my previously created fixture class. Creating the collection to share date across tests Finally, I add the Collection attribute with the previously set name to my test class and pass the fixture class in the constructor. Passing the collection to the test class To demonstrate that the _timeFixture object stays the same, I run a couple of tests with Thread.Sleep(1500) and both tests will output the same time. Tests using the fixture object Both tests will print the same output. The same output of the fixture object Provide test data from a classPreviously, I showed how to use the Theory attribute to pass several parameters for the test. If you want the same data for several tests, you would have to enter it several times. This is error-prone and unpractical. Therefore, you can place these values in a class and just add a reference to the class.Create a new class with a static property and only a getter which yield returns all your test data as object arrays. Class with test data For your test, use the MemberData instead of the InlineData attribute and provide the name of the property and the type of the class containing your test data. Using test data from a class Provide test data with a custom attributeA custom attribute works the same way as the MemberData attribute but it is even less to write in your test. Create a new class and inherit from the DataAttribute class. Then override the GetData method. Override the GetData method from the DataAttribute class After you created the class, add the name of the class (without Attribute) as the attribute to your Theory. xUnit will recognize your attribute and call the GetData method. Use the custom attribute Provide test data from an external sourceThe last method to provide data for your tests is from an external source. To read the data from a csv file, I placed the csv file in the root folder of my project and created a class with a static property. In the getter of the property, I read the file, split the values and return them as object arrays. Don’t forget to set the Copy to Output Directory property of the csv file to Copy always or Copy if newer. Otherwise, the file won’t be copied when you compile your code and therefore won’t be found at runtime. Read data from a csv file Now use the MemberData attribute for your test to add the name of the property and the type of your class. Provide data from an external file to your test ConclusionIn this post, I gave a quick overview of xUnit and explained how to get data from several sources and how to reduce duplicate code. For more information on xUnit, I can recommend the Pluralsight course “Testing .NET Core Code with xUnit.net: Getting Started” from Jason Robert.You can find the code of my demo on GitHub." }, { "title": "TDD Kata", "url": "/tdd-kata/", "categories": "Programming", "tags": "C#, TDD, Unit Testing, xBehave, xUnit", "date": "2018-12-09 16:31:21 +0100", "snippet": "Nowadays it should be a no-brainer to write automated tests when developing or changing features. Unfortunately, the reality is different. In the last couple of weeks, I worked with several teams which never wrote a line code and also only tried to get a quick result without thinking about maintainability, testability or technical debt. Introducing TDD is a process which can’t happen overnight. Therefore, I want to talk about some examples which helped me to learn and practice TDD.In my mind, there shouldn’t be even a discussion if you write tests and if you care about the architecture and principle like the Single Responsible Principle or Separation of Concerns.I guess if the team has little technical knowledge, they might not understand these concepts and don’t know where to start with writing tests. Therefore, I want to post some of the practice examples I did when I started to learn TDD. I will describe the problem, rate the difficulty and also link my solution to each example. For writing tests, I use xUnit, xBehave, FluentAssertions, and FakeItEasy.BowlingThe game consists of 10 frames as shown above.  In each frame, the player has two opportunities to knock down 10 pins. The score for the frame is the total number of pins knocked down, plus bonuses for strikes and spares. A spare is when the player knocks down all 10 pins in two tries.  The bonus for that frame is the number of pins knocked down by the next roll. So in frame 3 above, the score is 10 (the total number knocked down)plus a bonus of 5 (the number of pins knocked down on the next roll.) A strike is when the player knocks down all 10 pins on his first try.  The bonus for that frame is the value of the next two balls rolled. In the tenth frame, a player who rolls a spare or strike is allowed to roll the extraballs to complete the frame.  However, no more than three balls can be rolled in the tenth frame.Write a class named “Game” that has two methods roll(pins : int) is called each time the player rolls a ball.  The argument is the number of pins knocked down. score() : int is called only at the very end of the game.  It returns the total score for that game.Rating: EasyMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/BowlingSource: http://butunclebob.com/ArticleS.UncleBob.TheBowlingGameKataCalcStatsYour task is to process a sequence of integer numbers to determine the following statistics: minimum value maximum value number of elements in the sequence average valueFor example: [6, 9, 15, -2, 92, 11] minimum value = -2 maximum value = 92 number of elements in the sequence = 6 average value = 21.833333Rating: EasyMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/BowlingSource: http://www.blog.btbw.pl/java/code-kata-10-java-calc-statsEven or OddGiven an array of numbers, determine whether the sum of all of the numbers is odd or even.Give your answer in string format as ‘odd’ or ‘even’. If the input array is empty consider it as: [0] (array with a zero).Example: oddOrEven([0]) returns “even” oddOrEven([2, 5, 34, 6]) returns “odd” oddOrEven([0, -1, -5]) returns “even”Rating: EasyMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/EvenOrOddSource: https://www.codewars.com/kata/odd-or-evenFizzBuzzImagine the scene. You are eleven years old, and in the five minutes before the end of the lesson, your math teacher decides he should make his class more “fun” by introducing a “game”. He explains that he is going to point at each pupil in turn and ask them to say the next number in sequence, starting from one. The “fun” part is that if the number is divisible by three, you instead say “Fizz” and if it is divisible by five you say “Buzz”.So now your math teacher is pointing at all of your classmates in turn, and they happily shout “one!”, “two!”, “Fizz!”, “four!”, “Buzz!”… until he very deliberately points at you, fixing you with a steely gaze… time stands still, your mouth dries up, your palms become sweatier and sweatier until you finally manage to croak “Fizz!”. Doom is avoided, and the pointing finger moves on.Of course, in order to avoid embarrassment in front of your whole class, you have to get the full list printed out so you know what to say. Your class has about 33 pupils and he might go round three times before the bell rings for break time. Next math lesson is on Thursday. Get coding!Write a program that prints the numbers from 1 to 100. But for multiples of three print “Fizz” instead of the number and for the multiples of five print “Buzz”. For numbers which are multiples of both three and five print “FizzBuzz “.Sample output:12Fizz4BuzzFizz78FizzBuzz11Fizz1314FizzBuzz1617Fizz19Buzz… etc up to 100Rating: EasyMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/FizzBuzzSource: http://codingdojo.org/kata/FizzBuzz/Leap yearWrite a function that returns true or false depending on whether its input integer is a leap year or not.A leap year is divisible by 4 but is not otherwise divisible by 100 unless it is also divisible by 400.Rating: EasyMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/LeapYearSource: https://gist.github.com/alastairs/1142957List comparatorYou have two arrays in this kata, every array contains only unique elements. Your task is to calculate the number of elements in the first array which are also in the second array.Rating: EasyMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/ListComparatorSource: https://www.codewars.com/kata/array-comparatorPrime FactorWrite a class named “PrimeFactors” that has one static method: generate.The generate method takes an integer argument and returns a List. That list contains the prime factors in numerical sequence.For example: 100 should return 2, 2, 5, 5 2 should return 2 Smaller than 2 should return an empty listRating: EasyMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/PrimeFactorsSource: http://butunclebob.com/ArticleS.UncleBob.ThePrimeFactorsKataQueueImplement your own queue (existing .NET implementations are not allowed (list, array, …)). The queue should have the following methods: Enqueue(object): Adds an element to the queue. Dequeue(): Returns the first element and removes it from the queue. Peek(): Returns the first element without removing it.Rating: Easy – MediumMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/QuequeRecently used listDevelop a recently-used-list class to hold strings uniquely in Last-In-First-Out order. The most recently added item is first, the least recently added item is last. Items can be looked up by index, which counts from zero. Items in the list are unique, so duplicate insertions are moved rather than added. A recently-used-list is initially empty.Rating: Easy – MediumMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/RecentlyUsedListSource: https://tddkatas.codeplex.com/documentationStackImplement your own stack (existing .NET implementations are not allowed (list, array, …)The stack should have the following methods: Push(object): Adds an element to the stack. Pop(): Returns the last element and removes it from the stack. Peek(): Returns the last element without removing it.Rating: Easy – MediumMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/StackString calculatorCreate a simple String calculator with a method int Add(string numbers) The method can take 0, 1 or 2 numbers, and will return their sum (for an empty string it will return 0) for example ” ” or “1” or “1,2” Start with the simplest test case of an empty string and move to 1 and two numbers Remember to solve things as simple as possible so that you force yourself to write tests you did not think about Remember to refactor after each passing test Allow the Add method to handle an unknown amount of numbers Allow the Add method to handle new lines between numbers (instead of commas). &lt;ol type=\"1\"&gt; the following input is ok:  “1\\n2,3”  (will equal 6) the following input is NOT ok:  “1,\\n” (not need to prove it &#8211; just clarifying) &lt;/ol&gt; Rating: EasyMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/StringCalculatorSource: http://osherove.com/tdd-kata-1/TennisThis kata is about implementing a simple tennis game. I came up with it while thinking about Wii tennis, where they have simplified tennis, so each set is one game. The scoring system is rather simple: Each player can have either of these points in one game 0 15 30 40 If you have 40 and you win the ball you win the game, however, there are special rules. If both have 40 the players are deuce. After deuce, the winner of a ball will have advantage and game ball. If the player with advantage wins the ball he wins the game If the player without advantage wins they are back at deuce. A game is won by the first player to have won at least four points in total and at least two points more than the opponent. The running score of each game is described in a manner peculiar to tennis: scores from zero to three points are described as “love”, “fifteen”, “thirty”, and “forty” respectively. If at least three points have been scored by each player, and the scores are equal, the score is “deuce”. If at least three points have been scored by each side and a player has one more point than his opponent, the score of the game is “advantage” for the player in the lead.Rating: MediumMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/TennisSource: http://codingdojo.org/kata/Tennis/TreeImplement a binary tree search algorithm.Rating: Easy – MediumMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/TreeWord wrapperYou write a class called Wrapper, that has a single static function named wrap that takes two arguments, a string, and a column number. The function returns the string, but with line breaks inserted at just the right places to make sure that no line is longer than the column number. You try to break lines at word boundaries.Like a word processor, break the line by replacing the last space in a line with a newline.Rating: EasyMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/WordWrapperSource: http://codingdojo.org/kata/WordWrap/Human readable timeWrite a function, which takes a non-negative integer (seconds) as input and returns the time in a human-readable format (HH:MM:SS) HH = hours, padded to 2 digits, range: 00 – 99 MM = minutes, padded to 2 digits, range: 00 – 59 SS = seconds, padded to 2 digits, range: 00 – 59The maximum time never exceeds 359999 (99:59:59).You can find some examples in the test fixtures. Assert.AreEqual(TimeFormat.GetReadableTime(0),”00:00:00″); Assert.AreEqual(TimeFormat.GetReadableTime(5),”00:00:05″); Assert.AreEqual(TimeFormat.GetReadableTime(60),”00:01:00″); Assert.AreEqual(TimeFormat.GetReadableTime(86399),”23:59:59″); Assert.AreEqual(TimeFormat.GetReadableTime(359999),”99:59:59″);Rating: EasyMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/HumanReadableTimeSource: https://www.codewars.com/kata/52685f7382004e774f0001f7/train/csharpTic-Tac-Toe CheckerIf we were to set up a Tic-Tac-Toe game, we would want to know whether the board’s current state is solved, wouldn’t we? Our goal is to create a function that will check that for us!Assume that the board comes in the form of a 3×3 array, where the value is 0 if a spot is empty, 1 if it is an X, or 2 if it is an O, like so:[[0,0,1],[0,1,2],[2,1,0]]We want our function to return -1 if the board is not solved yet, 1 if X won, 2 if O won, or 0 if it’s a cat’s game (i.e. a draw).You may assume that the board passed in is valid in the context of a game of Tic-Tac-Toe.Rating: Easy – MediumMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/TicTacToeCheckerSource: https://www.codewars.com/kata/tic-tac-toe-checkerHashi solverWrite a program that evaluates a provided solution for the game Hashi. The program must check the following conditions: Every island has the needed amount of bridges. Every bridge is connected to two islands. No bridge is crossing with another one.A description of the game and the rules can be found on Wikipedia.Rating: HardMy Solution: https://github.com/WolfgangOfner/TDD-Kata/tree/master/HashiConclusionIn this post, I gave several practical examples to learn TDD and also linked my results to every question. Bear in mind that my solution is the result of several iterations and refactoring. I didn’t come up with that on the first try. That also highlights the beauty if tests. I refactored my code, ran the tests and knew that I didn’t break anything.You can find all my solutions on GitHub." }, { "title": "Change the TFS workflow", "url": "/change-tfs-workflow/", "categories": "DevOps", "tags": "TFS, Witadmin, Workflow", "date": "2018-11-18 16:28:12 +0100", "snippet": "Changing the workflow of a work item in TFS is pretty simple, though for me it was hard to get started due to the complicated documentation. In this post, I will talk about adding new states to a work item, editing the transitions of the states and lastly about editing the process config template.Getting startedTo make changes to your TFS, you have to download, edit and then upload the xml configurations of the work item or process, you want to update. This can be done with the witadmin console application. Start witadmin as administrator under %programfiles(x86)%\\Microsoft Visual Studio\\2017\\Professional\\Common7\\IDE\\CommonExtensions\\Microsoft\\TeamFoundation\\Team Explorer. Instead of TeamExplorer, your location might be, Professional, Enterprise or Community, depending on the version you’ve installed.Adding new states and transitions to a TFS work itemAs already mentioned, with witadmin you can download the configuration xml file for the work item you want to edit. The instruction to do that is witadmin exportwitd /collection:COLLECTION /p:PROJECT /f:FILENAME /n:TYPENAME. The type name is for example Bug for “Product Backlog Item”.Adding a new stateAfter you downloaded the xml file, open it and find the section. Under the section, you can add a new state. Add a new state to a work item The value of the state is the text you will see in the TFS.Adding a new transitionThe new state can’t be selected right now because there is no transition for it. Transitions tell the TFS which state can be followed by the current one. For example, it makes sense that after new comes approved, but it wouldn’t make much sense if after new came implemented.Below the states, you will find the section. There you can add your transitions. A transition always has a from and to state. Note that the transition for New is from empty (from=&#8221;&#8221;). Additionally, you can add a default reason which will be displayed when you select this state. For example when you transition from new to approved the default message could be something like &#8220;Customer approved PBI&#8221;. Add a new transition for the new state Upload the type definitionWith all changes made, you can upload the type definition with witadmin. Before you do that, you should validate your xml file with witadmin importwitd /collection:COLLECTIONNAME /f:XMLFILE /p:PROJECT /v. If everything is good, you can upload the xml file with the same instruction but without the /v.Edit the process templateIf you go to your TFS after uploading the type definition, you will see your new states and also the new transitions. But after selecting a new state, your TFS won’t be able to display the item anymore. Therefore, you have to add the new states to the process config template xml file.Adding the new stateDownload the config file with witadmin exportprocessconfig /collection:COLLECTIONNAME /p:PROJECT /f:FILENAME. Open the xml file and find the work item, you added the state to, for example, Bug or Backlog Item. Add the state in the States section. Add the new state in the process config file Upload the process templateAfter adding the states to the work items, you can upload the xml file to your TFS. Before you do that, you should validate it with witadmin importprocessconfig /collection:COLLECTION /p:PROJECT /f:FILENAME /v. If the file is valid, you can upload it without the /v parameter.ConclusionIn this short post, I showed how to add new states to your TFS and how to configure your process template, so your TFS can handle them. For more information, see Microsoft’s documentation." }, { "title": "Integrate your Bot in Third-Party Applications", "url": "/bot-third-party-integration/", "categories": "Cloud", "tags": "Azure, Bot, Microsoft Bot Framework, Slack", "date": "2018-10-17 12:12:52 +0200", "snippet": "It is really easy to integrate your bot, hosted on Azure to third-party tools like Slack or Skype. Today, I will show how to integrate your bot with Slack, Facebook, and your website. For this demo, I will use the code from my last post, in which I showed how to return different types of answers to enhance the experience of the users. You can find the code on GitHub.Integrate your Bot in a third-party applicationYou can integrate your bot with several third-party applications. To see a list, which applications are available, go to your bot and open the Channels blade under the Bot management menu. Available channels The available channels are: Web Email (Office 365) Facebook Messenger GroupMe Kik Skype for Business Slack Telegram TwilioIntegrate the Bot with SlackFollow these steps to integrate your bot with Slack: Create a new Slack App and enter a name for your app. Create a Slack App Select OAuth &amp; Permissions and Click Add New Redirect URL. Enter https://slack.botframework.com and click Add. Click Save URLs. Add a redirect URL to Slack Select Bot Users and click Add a User. Optionally change the Display name and Default username. Switch the slider for Always Show My Bot as Online to On. Click Add Bot User. Add a bot user Select Event Subscriptions and switch the slider to On. Enter the as Request URL https://slack.botframework.com/api/Events/{YourBotHandle}, in my case this is https://slack.botframework.com/api/Events/WolfgangBotMediaDemo. Click Add Workspace Event and add the following events: member_join_channel member_left_channel message.channels message.groups message.im message.mpim Click Save Changes. Add events Enter your credentials Select Basic Information. There, you can find your App ID, Client ID, Client Secret, Signing Secret, and Verification Token. The Slack app credentials Click on Install your App to your workspace and then on Install App to Workspace. On the next window, click Authorize and your app gets installed. Install your App to Slack Open your bot in the Azure portal and select the Channels blade under the Bot management menu. Click on Slack. Enter your Client ID, Client Secret, and Verification Token from Slack. Click Save. Enter the Slack credentials in the Azure Portal A new window opens. Click Authorize. Authorize the App in Slack That’s it. Now you can chat with your chatbot in Slack. Note that some answers might be displayed differently than on the web. For example, the image carousel is displayed as several images underneath each other. Testing the bot in Slack Integrate the Bot with Facebook MessengerConnecting the bot to the Facebook Messenger is as simple as it was with Slack. The only downside is that Facebook has to approve your bot before you can use it. This usually only takes a couple of hours.To integrate your bot with the Facebook Messenger, follow these steps: Create a new Facebook App. Under Settings –&gt; Basic, you can find your App ID. Facebook App ID and Secret Click on Dashboard and then on Set Up on the Messenger Tilde. In the Token Generation section, create a new page and then save the Page Access Token. Facebook Page Token Click on Setup Webhooks and check the following Subscription Fields: messages messaging_postback messaging_optins message_delieveries Before you close the window, go back to your bot in the Azure portal and open the Channels blade under the Bot management menu. Click on Facebook Messenger Enter the App ID, App Secret, Page ID, and Page Access Token. You can find the Page ID on your Page under the About section. Configure the Facebook Messenger Channel Copy the Callback URL and Verify token to the open window on the Facebook page. Set up the Callback URL on Facebook Click on Save in the Azure Portal. Click on Verify and Save on the Facebook page. Subscribe your page to the Webhook. Subscribe your page to the Webhook Scroll down and send a request for approval. Once your application is approved, people can test it. As long as it’s not approved, only you can use the chat.Integrate your bot into your websiteIntegrating your bot into your website is as simple as it could be. In the Azure Portal in your bot, click on Channels under the Bot management and select Web Chat. There you have your Secret keys and the code to embed the code on your website. You only have to replace the string YOUR_SECRET_HERE with your actual secret and you are good to go. Integrate your application into your website ConclusionIn this post, I showed how to integrate your Azure bot with Slack, Facebook Messenger, and your own website. Besides the approval from Facebook, it only takes a couple of minutes. Keep in mind that different chats display some replies differently.For more information on Azure bot, check out my other posts about bots." }, { "title": "Extending the answers of the Chat Bot", "url": "/extending-answers-chat-bot/", "categories": "Cloud", "tags": "Azure, Bot, C#, Microsoft Bot Framework", "date": "2018-10-16 12:00:50 +0200", "snippet": "The Azure chat bot supports different answer types like videos, images or links. In my last post, I showed you how to make your bot smarter and understand the user’s input. In this post, I will show you how to return different types of answers.Adding different reply types to the chat botTo make your chat bot more appealing to the users, you should interact with your users with more than just text. The following demo is done with the Azure Bot Framework v3. For every return type, I create a simple intent which does nothing but return the desired type. The code can be downloaded from GitHub. Create a new chat bot For every return type, I will create a new dialog and call this dialog in the intent. The calling method looks like this: Forwarding to a new dialog Reply with an internet videoReplying with a video hosted somewhere on the internet is pretty simple. You only have to create a message, add a text to this message and then a VideoCard as attachment. Before you can send the reply, you have to convert it to JSON. Return a video from the internet When you test your chat bot, you will see the video as the reply of the chat bot. Testing the internet video return Reply with a Youtube videoReplying with a Youtube video is almost the same as replying with a normal video. Instead of a VideoCard, a normal Attachment is returned. Return a Youtube video When testing the reply, you can see the embedded Youtube video in the chat. Testing the Youtube video reply Reply with a file from the internetReturning a file from the internet is basically the same as returning a Youtube video. Additionally, a name is added to the returning Attachment. Return a file from the internet Testing the file reply returns a pdf file which you can download. Testing the file reply Reply with an image from the internetReplying with an image is the same as the pdf reply. The only difference is the ContentType which is image/png (or jpeg, gif and so on). Return an image from the internet The test shows the image in the chat. Testing the image reply Reply an image carouselThe image carousel is a great feature to enhance the user experience in your chat bot. To reply with an image carousel add a HeroCards to the list of Attachment. To make the code easier, I create a helper method, which creates the HeroCard and returns it as Attachment. HeroCard helper method The HeroCard is a big image with the same text and a button. Additionally, you could create a ThumbnailCard, which has a small image, some text, and a button. Here is the helper method for the ThumbnailCard: ThumbnailCard helper method Next, I set the AttachmentLayout to AttachmentaLayoutTypes.Carousel and add a List of Attachment to the reply Attachments: Return an image carousel of HeroCards and ThumbnailCards I don’t like mixing a HeroCard with a ThumbnailCard but for the sake of this demo, I combined them. Testing the HeroCard and ThumbnailCard reply On the left side, you can see the HeroCard and on the right one, the ThumbnailCard.ConclusionIn this post, I showed how to have different types of answers to enhance the user experience with your chat bot. In the next part, I will show you how to deploy your chat bot to Slack and to Facebook." }, { "title": "Azure Bot with language understanding", "url": "/azure-bot-language-understanding/", "categories": "Cloud", "tags": "Azure, Bot, C#, Luis, Microsoft Bot Framework", "date": "2018-10-15 12:00:04 +0200", "snippet": "With the Azure bot framework and luis, Microsoft offers two great features to implement language understanding in your chat bot. This is the second part of my chat bot series and in this post, I will show you how to add a language understanding to your chat bot using luis. You can find the first part here.Adding new answers to your Azure botIf you created your Azure bot the same way as described in part 1, it will already have luis integration. To add new language features, go to the luis homepage and login with your Azure credentials. After you logged in, you should see your chat bot. Open it and you will see 4 already existing intents. List of predefined intents An intent is something the user wants to do. To teach your app what the user wants, you add a new intent and then add different utterances. Utterances are different phrases the user might use. For example, if the user is asking about the weather, he might ask “what is the weather today”, “Is it hot” or “Do I need an umbrella”. The more utterances you add to an intent, the better the application understands what the user wants. A rule of thumb is that there should be at least 5 utterances to train the Azure bot.Create a new IntentTo create a new intent, click on + Create new intent and enter a name, for example, Weather. Now type in examples (utterances), the user might say. Add a new intent After you are done, click on Train in the right top corner and after the training on Publish.Handle the new intent in your Azure bot applicationNow that you have created a new intend, your bot has to handle it. To do something with the new intent, you have to add it to the switch statement in the OnTurnAsync method in the BasicBot.cs file. The simplest way to add the new intent is to add a new case with “Weather” and then return a message. Handle the intent in the bot Additionally, I remove the welcome message by commenting the following code: Remove welcome message Check in your code and after it is deployed, you can test it. Testing the weather intent The new intent is working, but as you can see, the bot is not really smart. It would be nice if I could get different answers depending on the city.Making the Azure bot smarterCurrently, the bot always gives the same answer if the user asks for the weather. It would be nice if the bot would give a different answer, depending on the city, the user is asking about.Add an entity to the intentThe bot should react to the city, the user enters. Therefore the bot has to know that the city is a special input. In Luis this is called an entity. After defining an entity, the C# code can behave differently, depending on the value of the intent.To add the entity, add a new utterance to the intent, for example, is it sunny in zurich? Then click on zurich and enter City to create the City entity. In the next window set the Entity type to simple and click done. Define an entity in luis Then I enter all previously used utterance but this time with a city. After all utterances are entered, I click on each city and select the city entity. This marks it as an entity. Applying the city entity Click on Train and then on PublishHandle the new entity in your bot codeThe following code shows how to get the entity of the luisResult and then give an answer, according to the entity. Handle the entity in the Azure bot Obviously, this code is not really useful, it is only supposed to show what you can do with entities. A real world example could be to compare the entity with a list of cities where you have a store and if the entity is a city where you have a store, you call a weather service and show the weather for the day.ConclusionIn this post, I showed how to implement language understanding in your Azure Bot using luis. Additionally, I showed how to use entities and make the answers of your Azure bot smarter. You can find the code of the demo on GitHub.In part 3 of this series, I will talk about different reply types like videos, images or files." }, { "title": "Create a Bot Application with Azure", "url": "/bot-application-azure/", "categories": "Cloud", "tags": "Azure, Bot, C#, Microsoft Bot Framework", "date": "2018-10-14 16:02:01 +0200", "snippet": "I’ve been curious about how chat bots work and this week I finally took the time to sit down and look into it. In this and the following post, I will talk about what I learned and I how solved some problems. Before I could start, I had to decide which platform I want to use. Therefore I looked at solutions in AWS and Azure. Since I am a fan of Azure and AWS only supports English as the user language, I decided to go with Azure for my chat bot application.Create your first Azure Chat Bot ApplicationYou can create a bot application with Visual Studio or directly in the Azure Portal. If you want to use Visual Studio, you need Visual Studio 2017 and the Bot Builder Template which you can download here.  To create a bot application in the Azure Portal, follow these steps: In the Azure portal click on +Create a resource, search for Web App Bot and click Create. Create a Web App Bot in the Azure Portal Enter the basic information for your bot. Select a template. For this demo, I will select the Basic Bot template because it is simple but also supports language understanding. Select the LUIS App location. It depends on this setting, which luis website you have to use later to configure luis. For example, if you select Europe the URL is https://eu.luis.ai, if you select US the URL is https://luis.ai. Click Create. Create your new bot application Test your botAfter the bot application is deployed, open it and select Test in Web Chat under the Bot management menu. Type in Hi and wait for a response. Sometimes you don’t get an answer on the first try. Then you have to enter a second message. Test your bot Congratulation, you just talked to your own bot for the first time.Edit the bot answersIn this section, I will show you how to edit existing answers of your bot and how to deploy it to Azure.Edit your bot in Visual StudioTo edit the source code of your code, download it. Click on Download Bot source code on the Build blade under the Bot management menu. Download the source code of your bot To edit an existing answer, follow these steps: Open the solution you just downloaded in Visual Studio 2017. Open GreetingDialog.cs under Dialogs/Greeting and find the line with Text = “What is your name?”. In my case, it was on line 104 but Microsoft updates the default bot quite often, so it might be somewhere else for you. Change the string to whatever you want, for example, “Howdy, tell me your name”. Save the file.Deploy your bot to AzureThere are different ways to deploy your bot application to Azure. The simplest is to right-click your solution and select Publish. There you can already see all settings being set and you only have to click Publish. This approach is fine when you are alone but I want to show you a more sophisticated way which includes version control and automatic deployments. I will use GitHub as my version control and every time I check code in, it will be automatically deployed. To configure Azure to do that, follow these steps: Open the App Service of your bot application and select Deployment options under the Deployment menu. Add a source for your version control, in my case GitHub, but you could also use Bitbucket, Team Services or many more. Enter your credentials, select a project and configure the branch you want to use for the deployment. Click OK. Add GitHub as your deployment source Next, push your bot application to your GitHub project. After you pushed your changes to GitHub, you will see the deployment under the Deployment options. Sync GitHub with Azure With the changes deployed, it is time to test them. Open your Web App Bot Application and select the Test in Web Chat blade under the Bot management menu. Type in Hi and you should see the text, you changed previously. Test the changes in your bot application As you can see, the changes worked and the bot uses the new phrase to “ask” for the name of the user.ConclusionToday, I showed you how to create a simple chat bot with Azure and how to deploy it using GitHub. In my next post, I will show you how to enable language understanding with luis.You can find the code of the demo on GitHub. " }, { "title": "Design and Implement DevOps", "url": "/design-and-implement-devops/", "categories": "DevOps, Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-08-09 22:22:12 +0200", "snippet": "DevOps is a combination of development (Dev) and Operations (Ops). The ultimate goal of DevOps is automation and repeatability which allows for increased deployment frequency without the burden that manual deployments bring.Instrument an application with telemetryApplication Insights is an extensible analytics service for application developers on multiple platforms that helps you understand the performance and usage of your live applications. With it, you can monitor your web application, collect custom telemetry, automatically detect performance anomalies, and use its powerful analytics tools to help you diagnose issues and understand what users do with your app. It works with web applications hosted on Azure, on-premise, or in another cloud provider.To get started, you only need to provision an Application Insights resource in Azure, and then install a small package in your application. The things can instrument are not limited just to the web application, but also any background components, and JavaScript within its web pages. You can also pull telemetry from host environments, such as performance counters, Dicker logs, or Azure diagnostics.The following telemetry can be collected from server web apps: HTTP requests Dependencies such as calls to SQL Databases, HTTP calls to external services, Azure Cosmos DB Exceptions and stack traces Performance Counters, if you use Status Monitor, Azure monitoring or the Application Insights collected writerThe following telemetry can be collected from client web pages: Page view counts AJAX calls requests made from a running script Page view load data User and session counters Authenticated user IDsThe standard telemetry modules that run out of the box when using the Application Insights SDK send load, performance and usage metrics, exception reports, client information such as IP address, and calls to external services.Discover application performance issues by using Application InsightsSystem performances depends on several factors. Each factor is typically measured through key performance indicators (KPIs), such as the number of database transactions per second or the volume of network requests.Application Insights can help you quickly identify any application failures. It also tells you about any performance issues and exceptions.When you open any Application Insights resource you see basic performance data on the overview blade. Clicking on any of the charts allows you to drill down into the related data to see more detail and related requests, as well as viewing different time ranges.If your application is built on ASP.NET or ASP.NET Core, you can turn on Application Insight’s profiling to view detailed profiles of live requests. In addition to displaying hot paths that are using the most response times, the Profiler shows which lines in the application code slowed down performance.To enable the Profiler, follow these steps: In your Application Insights resource, select the Performance blade and select Profiler. Create a Profiler Select Add Linked Apps from the top of the Configure Application Insights Profiler blade. Select the application you wish to link to see all its available slots. Click Add to link them to your Application Insights resource. After linking, select Enable Profiler from the of the Configure Application Insights Profiler blade. Deploy Visual Studio Team Services with Continuous Integration (CI) and Continuous Delivery (CD)Continuous Integration is a practice by which the development team members integrate their work frequently, usually daily. An automated build verifies each integration, typically along with tests to detect integration errors quickly, while it’s easier and less costly to fix them. The output, or artifacts, generated by the CI systems are fed to the release pipeline to streamline and enable frequent deployments.Continuous Delivery is a process where the full software delivery lifecycle is automated including tests, and deployment to one or more test and production environments. Azure App Services supports deployment slots, into which you can deploy development, staging, and production builds from the CD process. Automated release pipelines consume the artifacts that the CI systems produce, and deploys them as new version and fixes to existing systems.Your source code must be hosted in a version control system. VSTS provides GIT and Team Foundation Version Control. Additionally, GitHub, Subversion, Bitbucket, or any other Git repository can be integrated with the Build service.To configure the CI/CD pipeline from the Azure portal, follow these steps: In the Azure portal click on +Create a resource, search for Web App and click Create. Provide a name, subscription and APP Service plan. Click Create. Create a new Web App After the web app is deployed, open it in the Azure portal and select the Deployment Center blade under the Deployment menu. Select VSTS and then click Continue. Create Continuous Delivery with VSTS On the next page, select App Service Kudu as your build server and click Continue. Provide your VSTS Account, select a project, repository, and branch you want to use for the deployment and click Continue. After everything is set up, Azure Continuous Delivery executes the build and initiates the deployment. Deploy CI/CD with third-party platform DevOps tools (Jenkins, GitHub, Chef, Puppet, TeamCity)Azure allows you to continuously integrate and deploy with any of the leading DevOps tools, targeting any Azure service.Out of the box, Azure App Services integrates with source code repositories such as GitHub to enable a continuous DevOps workflow.Follow these steps to enable continuous deployment from a GitHub repository: Publish your application source code to GitHub. In the Azure portal, open your web app and select Deployment options under the Deployment menu. On the Deployment option blade, select GitHub, authorize Azure to use GitHub, select a project and a branch. Click OK. Enable continuous deployment from GitHub When you push a change to your repository, your app is automatically updated with the latest changes.ConclusionThis post gave a short overview of how to use various tools in Azure to implement DevOps. DevOps is a mix of Development and Operations and should help to deploy faster and with fewer errors due to automation. Azure supports different DevOps tools like VSTS or GitHub.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Design and implement third-party PaaS", "url": "/implement-third-party-paas/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-07-24 11:47:46 +0200", "snippet": "Azure support many third-party offerings and services through the Azure Marketplace. These can be deployed through the Azure portal, using ARM, or other CLI tools.Implement Cloud Foundry as a third-party PaaSCloud Foundry is an open source PaaS for building, deploying and operating 12-factor applications developed in various languages and frameworks. It is a mature container-based application platform allowing you to easily deploy and manage production grade application on a platform that supports continuous delivery and horizontal scale, and support hybrid and multi-cloud scenarios.There are two forms of Cloud Foundry available on Azure: Open source Cloud Foundry (OSS CF) is an entirely open source version of Cloud Foundry managed by the Cloud Foundry Foundation. Pivotal Cloud Foundry (PCF) is an enterprise distribution of Cloud Foundry from Pivotal Software Inc., which adds on a set of proprietary management tools and enterprise support.Deploy Cloud Foundry on Azureto deploy a basic Pivotal Cloud Foundry on Azure, follow these steps: Before you can create a Cloud Foundry cluster, you must create an Azure Service Principle. Follow the instructions on GitHub. In the Azure portal click on +Create a resource, search for Pivotal Cloud Foundry on Microsoft Azure and click Create. Deploy Pivotal Cloud Foundry on Azure Provide a storage account name prefix, paste your SSH public key, upload the azure credentials.json file, enter the Pivotal Network API token, choose a resource group, and location. Click OK. After the validation is done, click OK again. On the Buy blade, click Purchase. Implement OpenShiftThe OpenShift Container Platform is a PaaS offering from Red Hat built on Kubernetes. It brings together Docker and Kubernetes and provides an API to manage these services. OpenShift simplifies the process of deploying, scaling, and operating multi-tenant applications onto containers.There are two forms of OpenShift: The open source OpenShift Origin The enterprise-grade Red Hat OpenShift Container PlatformBoth are built on the same open source technologies, with the Red Hat OpenShift Container Platform offering enterprise-grade security, compliance, and container management.The prerequisites for installing OpenShift include: Generate an SSH key pair (Public / Private), ensuring that you do not include a passphrase with the private key. Create a Key Vault to store the SSH Private Key. Create an Azure Active Directory Service Principal. Install and configure the OpenShift CLI to manage the cluster.Prerequisites for deploying a Red Hat OpenShift Container Platform include: OpenShift Container Platform subscription eligible for use in Azure. You need to specify the Pool Id that contains your entitlements for OpenShift. Red Hat Customer Portal login credentials. You may use either an Organization ID and Activation Key, or a username and password. It is more secure to use the Organization ID and Activation Key.Deploy a Red Hat OpenShift Container PlatformYou can deploy from the Azure Marketplace templates, or using ARM templates. To deploy a Red Hat OpenShift Container Platform from the Marketplace, follow these steps: In the Azure portal click on +Create a resource, search for Red Hat OpenShift Container Platform and click Create. Deploy a Red hat OpenShift Container Platform On the Basics blade, provide the VM Admin username, paste your SSH public key, select a resource group and location. Click OK. On the Infrastructure blade, provide an OCP cluster name prefix, select a cluster size, provide the resource group name for your key Vault, as well as the Key Vault name and its secret name you specified in the prerequisites. Click OK. On the OpenShift Container Platform Settings blade, provide an OpenShift Admin user password, enter your Red Hat subscription manager credentials, specify whether you want to configure an Azure Cloud Provider, and select your default router subdomain. Click OK.  Wait until the validation passes and click OK. Click Purchase. Provision applications by using Azure Quickstart TemplatesAzure Quickstart Templates are community contributed Azure Resource Manager templates that help you quickly provision applications and solutions with minimal effort. You can search for templates in the gallery at https://azure.microsoft.com/resources/templatesBuild applications that leverage Azure Marketplace solutions and servicesThe Azure Marketplace is an online applications and services marketplace that enables start-ups and independent software vendors to offer their solutions to Azure customers around the world.Pricing varies based on the product type. Pricing models include: Free Free Software Trial BYOL – Bring your own license Monthly Fee Usage BasedConclusionThis post gave a short overview of how to use third-party platforms as a Service in Azure and how to leverage the existing templates.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Design and Implement Azure Service Fabric apps", "url": "/implement-azure-service-fabric-apps/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-07-15 21:03:41 +0200", "snippet": "Azure Service Fabric is a platform that makes it easy to package, deploy, and manage distributed solutions at scale. It provides an easy programming model for building microservice solutions with a simple, familiar, and easy to understand development experience that supports stateless and stateful services and actor patterns. In addition, to providing a packaging and deployment solution for these native components, Service Fabric also supports the deployment of guest executable and containers as part of the same managed and distributes system. Native and executable component Description Stateless Services Stateless-Fabric-aware services that run without a managed state. Stateful Services Stateless-Fabric-aware services that run with a managed state where the state is close to the compute. Actors A higher level programming model built on top of stateful services. Guest Executable Can be any application or service that may be cognizant or not cognizant of Service Fabric. Containers Both Linux and Windows containers are supported by Service Fabric and may be cognizant or not cognizant of Service Fabric.  Create a Service Fabric applicationA Service Fabric application can consist of one or more services. The application defines the deployment package for the service, and each service can have its own configuration, code, and data. A Service Fabric cluster can host multiple applications, and each has its own independent deployment upgrade lifecycle.To create a new Service Fabric application, follow these steps: Open Visual Studio and select File -&gt; New -&gt; Project. In the New Project dialog, select Service Fabric Application within the Cloud category. Provide a name and click OK. Create a Service Fabric application Select Stateful Service from the list of services and provide a name. Click OK. Select a Template for your Fabric Service Expand the PackageRoot folder in the Solution Explorer and you will find the ServiceManifest.xml file there. This file describes the service deployment package and related information. It includes a section that describes the service type that is initialized when the Service Fabric runtime starts the service. The service type description Configure your Service Fabric application A service type is created for the project. In this case, the type is defined in the Simulator.cs file. This service type is registered in Program.cs when the program starts so that the Service Fabric runtime knows which type to initialize when it creates an instance of the service. Registering a service type in the main method The template produces a default implementation for the service type, with a RunAsync method that increments a counter every second. This counter value is persisted with the service in a dictionary using the StateManager, available through the service base type StatefulService. This counter is used to represent the number of leads generated for the purpose of this example. RunAsync which increments a counter every second The service will run, and increment the counter as it runs persisting the value, but by default, this service does not expose any methods for a client to call. Before you can create an RPC listener you have to add the required NuGet package, Microsoft.ServiceFabric.Services.Remoting. Create a new service interface using the IService marker interface from the previously installed NuGet, that indicates this service can be called remotely. Create the ISimulatorService interface Implement the previously created interface on the Simulator service type, and include an implementation of the GetLeads method to return the value of the counter. Implementation of the GetLeads method To expose this method to clients, add an RPC listener to the service. Modify the CreateServiceReplicaListeners method in the Simulator service type implementation, to add a call to the CreateServiceReplicaListeners method. Modify the CreateServiceReplicaListeners method Add a web front end to a Service Fabric applicationIn this section, I will create a web front end to call the stateful service endpoint which I create previously.To add a web app to your Service Fabric application, follow these steps: Right-click the Services node of your Service Fabric application and select Add and then New Service Fabric Service… In the template dialog, select Stateless ASP.NET Core, provide a name and click OK. Create a web app in your Service Fabric app On the next page select Web Application (Model-View-Controller) and click OK. Select the MVC template for your web app Expand the PackageRoot folder in the Solution Explorer and you will find the ServiceManifest.xml file there. This file describes the service deployment package and related information. It includes a section that describes the HTTP endpoint where your web app will listen for requests. The HTTP endpoint description The new WebApp type is defined in the WebApp.cs, which inherits from StatelessService. For the service to listen for HTTP requests, the CreateServiceInstanceListeners() method sets up the WebListener. The WebApp class The next step is to call the stateful service that returns the leads counter value, from the stateless web app. Make a copy of the service interface defined for the service type, ISimulatorService. The copied ISimulatorService interface Modify the ConfigureServices instruction in the WebApp.cs to inject an instance of Fabric client. The modified CreateServiceInstanceListeners method Modify the HomeController to use the FabricClient via dependency injection. Inject FabricClient into the HomeController Modify the Index method in the HomeController to use the FabricClient instance to call the Simulator service Modify the Index method to call the Simulator service Update the Index.cshtml view to display the counter for each partition. Modify the Index.cshtml view Deploy and run your Web App To run the web app and stateful service, you can publish it to the local Service Fabric cluster. Right-click the Service Fabric application node in the Solution Explorer and select Publish. From the Publish Service Fabric Application dialog, select a target profile matching one of the local cluster options, and click Publish. Deploy to a local cluster with an error message If you get an error message as on the screenshot shown above, start PowerShell as administrator and run the following code: &amp; &#8220;$ENV:ProgramFiles\\Microsoft SDKs\\Service Fabric\\ClusterSetup\\DevClusterSetup.ps1&#8243;. This creates a local cluster. The installation takes a couple of minutes. Once the installation is done, close and re-open the Publish window and the error should be gone. Deploy your application and then access your web app at http://localhost:8527 (or whatever port you configured in the ServiceManifest.xml in your web app). You can find the code of the demo on GitHub.Build an Actor-based serviceThe actor model is a superset of the Service Fabric stateful model. Actors are simple POCO objects that have many features that make them an isolated, independent unit of compute and state with single-threaded execution.To create a new Service Fabric application based on the Actor service template, follow these steps: Open Visual Studio and select File -&gt; New -&gt; Project. In the Cloud category select Service Fabric Application, provide a name and click OK. Select Actor Service from the templates list, provide a name and Click OK. Create an Actor service Monitor and diagnose servicesThe Azure portal offers several features to monitor and evaluate the performance or resource consumption of your application at runtime.Deploy an application to a containerService Fabric can run processes and containers side by side, and containers can be Linux or Windows based containers. If you have an existing container image and wish to deploy this to an existing Service Fabric cluster, you can follow these steps to create a new Service Fabric application and set it up to deploy and run the container in your cluster. Open Visual Studio and select File -&gt; New -&gt; Project. In the Cloud category select Service Fabric Application, provide a name and click OK. Select Container from the templates list, provide a name and container image and Click OK. Expand the PackageRoot folder in the Solution Explorer and you will find the ServiceManifest.xml file there. Modify the Resources section to add a UriScheme, Port and Protocol setting for the service point. Add a UriScheme, Port and Protocol to the ServiceManifest.xml file Open the ApplicationManifest.xml file. Create a policy for the container to host a PortBinding by adding the Policies section to the ServiceManifestImport section. Additionally, indicate the container port for your container. Create a PortBinding policy for the container The application is configured and ready to be published. Migrate apps from cloud servicesYou can migrate your existing cloud service, both web and worker roles to Service Fabric applications.Scale a Service Fabric appTo scale a Service Fabric app, you have to understand Instances, Partitions, and Replicas.By default, the Service Fabric tooling produces three publish profiles that you can use to deploy your application. Publish profile Description Local.1Node.xml To deploy against the local 1-node cluster. Local.5Node.xml To deploy against the local 5-node cluster. Cloud.xml To deploy against a Cloud cluster. The publish profiles indicate the settings for the number of instances and partitions for each service. Publish profile parameter Description WebApp_InstanceCount Specifies the number of instances the WebApp service must have within the cluster. Simulator_PartitionCount Specifies the number of partitions (for the stateful service) the Simulator service must have within the cluster. Simulator_MinReplicaSetSize Specifies the minimum number of replicas required for each partition that the WebApp service should have within the cluster. Simulator_TargetReplicaSetSize Specifies the number of target replicas required for each partition that the WebApp service should have within the cluster. Create, secure, upgrade, and save Service Fabric Cluster in AzureTo publish your Service Fabric application to Azure in production, you will create a cluster and have to learn how to secure it. Also, you should know how to upgrade applications with zero downtime, and configure the application to scale following Azure’s best practicesConclusionThis post gave an overview of Azure Service Fabric and its features and how to deploy it.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Implement Azure Functions and WebJobs", "url": "/implement-azure-functions-webjobs/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-07-04 20:00:59 +0200", "snippet": "Azure Functions is a serverless compute service that enables you to run code-on-demand without having to explicitly provision or manage infrastructure. Use Azure Function to run a script or piece of code in response to a variety of events from sources such as: HTTP requests Timers Webhooks Azure Cosmos DB Blob Queues Event HubWhen it comes to implementing background processing tasks, the main options in Azure are Azure Functions and WebJobs. Azure Functions are built on top of WebJobs.If you already have an app service running a website or a web API and you require a background process to run in the same context, a WebJob makes the most sense. You can share compute resources between the website or API and the WebJob and the also share libraries between them.If you want to externalize a process so that it runs and scales independently from your web application or API environment, Azure Functions are the more modern serverless technology to choose.Create Azure Functions In the Azure portal click on +Create a resource, search for Function App and click Create. Provide a name, subscription, resource group, hosting plan, location, and storage plan. Click Create. Create an Azure Function Implement a Webhook functionTo implement a Webhook function, follow these steps: Open Visual Studio and make sure that you have the Azure Functions and Web Jobs Tool extension installed. Click New Project, expand the Visual C# and Cloud node, select Azure Functions and provide a name. Select the Empty template and click Create. Right-click on your solution, select Add and then New Item. Select Azure Function, provide a name and then select Generic WebHook. Create a Webhook for your Azure function Click OK to generate an initial implementation for your function. Start the application. In the output, you can see the Webhook URL. The local URL of your Azure Function Post a JSON payload to the function using any tool that can issue HTTP requests to test the function. Test your Azure Function Create an event processing functionTo create an event processing function, follow these steps: Open your Function App in the Azure portal and click + sign on the Functions blade. Select Timer and C# and click Create this function. Create an event processing function This creates a function that runs based on a timer. You can edit the .json file to adjust the settings for the function. Implement an Azure connected functionTo create an Azure connected function using Azure Queues, follow these steps: Open your Function App in the Azure portal and click + sign on the Functions blade. Click on Queue trigger. Select a language, provide a name for the function and queue and select a storage account. Click Create. Create a queue triggered function The function will be created with a simple implementation already which is triggered when a message is queued. If you already had a storage account, the information will be filled in automatically in the Integrate tab. If not, then you have to enter your storage account, the account key, and the connection string. Add the storage account information to your function To test the function, click Run on the main tab. On the right side, you can see the input and on the bottom, you can see the output. Test your function with the queue Integrate a function with storageTo create a function integrated with Azure Storage Blobs, follow these steps: Open your Function App in the Azure portal and click + sign on the Functions blade. Click on Blob trigger. Select a language, provide a name for the function and queue and select a storage account. Click Create. Create an integrated function with storage The function will be created with a simple implementation already which is triggered when a message is queued. Azure fills out the storage account in the Integrated tab if you already have one. If not, then you have to enter your storage account, the account key, and the connection string. Add the storage account information to your function with blob storage To test the function, click Run on the main tab. On the right side, you can see the input and on the bottom, you can see the output. When you run the test for the first time, it will fail probably. If it fails, you probably don&#8217;t have the workitem.txt file in your blob storage. Design and implement a custom bindingFunction triggers indicate how a function is invoked. There is a number of predefined triggers, some already discussed in previous sections, including: HTTP triggers Event triggers Queues and topic triggers Storage triggersEvery function must have one trigger. The trigger is usually associated with a data payload that is supplied to the function. Bindings are a declarative way to map data to and from function code. Using the Integrate tab, you can provide connection setting for such a data binding activity.You can also create custom input and output bindings to assist with reducing code bloat in your functions by encapsulating reusable, declarative work into the binding.Debug a FunctionYou can run your Azure Function locally in your Visual Studio 2017 and debug it like any other C# application.API ProxiesIf you have a solution with many functions, you will find it can become work to manage given different URLs, naming, and versioning potentially related to each other. An API Proxy acts as a single point of entry to functions from the outside world. Instead of calling the individual function URLs, you provide a proxy as a facade to your different function URLs.Furthermore, API proxies can modify the requests and responses on the fly.API proxies make sense in HTTP-bound Azure Functions. They may work for other event-driven functions, however, HTTP triggers are best suited for their functionality. As an alternative, you can use API Management for a fully featured solution.Integrate with App Service PlanFunctions can operate in two different modes: With a consumption plan where your function is allocated dynamically to the amount of computing power required to execute under the current load. With an App Service plan where your function is assigned a specific app service hosting plan and is limited to the resources available to that hosting plan.ConclusionThis post showed different variations of Azure Functions and how to create them. Since there are so many applications for Azure Functions, it is impossible to go too much into detail. You should know enough to get started on your own though.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Implement API Management", "url": "/implement-api-management/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-07-01 14:55:25 +0200", "snippet": "Azure API Management is a turnkey solution for publishing, managing, securing, and analyzing APIs to both external and internal customers in minutes. You can create an API gateway for backend services hosted anywhere. Many modern APIs protect themselves by rate-limiting consumers, meaning, limiting how many requests can be made in a certain amount of time. Traditionally, there is a lot of work that goes into that process. When you use API Management to manage your API, you can easily secure it and protect it from abuse and overuse with an API key, JWT validation, IP filtering, and through quotas and rate limits.If you have several APIs as part of your solution, and they are hosted across several services or platforms, you can group them all behind a single static IP and domain, simplifying communication, protection, and reducing maintenance of consumer software due to API locations changing. You also can scale API Management on demand in one or more geographical locations. Its built-in response caching also help with improving latency and scaling.Hosting your APIs on the API Management platform also makes it easier for developers to use your APIs, by offering self-service API key management, and an auto-generate API catalog through the developer portal. APIs are also documented and come with code examples, reducing developer on-boarding time using your APIs.The components of API ManagementAPI Management is made up of the following components: The API gateway is the endpoint that: Accepts API calls and routes them to your backends. Verifies API keys, JWT tokens, certificates, and other credentials. Enforces usage quotas and rate limits. Caches backend responses where set up. Logs call metadata for analytics. The publisher portal is the administrative interface where you set up your API program. Use it to: Define or import API schemas. Package APIs into products. Get insights from analytics. Manage users. The developer portal serves as the main web presence for developers, where they can: Read the API documentation. Try out an API via the interactive console. Create an account and subscribe to get API keys. Access analytics on their own usage. Create managed APIsBefore you can create APIs, you must first create a service instance.Create an API Management serviceTo create an API Management service, follow these steps: In the Azure portal click on +Create a resource, search for API Management and click Create. Provide a name, subscription, resource group, location, organization name, and administrator email. Click Create. Create an API Management service in the Azure Portal Add a productA product contains one or more APIs, as well as constraints such as a usage quota and terms of use. You can create several products to group APIs with their own usage rules. Developers can subscribe to a product once it is published, and then begin using its APIs.To add and publish a new product, follow these steps: On the API Management service, select the Products blade under the API Management menu. On the Products blade, click +Add. Provide a name and description. The remaining fields are settings for the level of protection and can stay as they are. Change the state to published. Click on Select API and select Echo API. Click Create. Add a product to you API Management instance Create a new APITo create a new API, follow these steps: On the API Management service, select the APIs blade under the API Management menu. On the APIs blade, select Blank API (or choose one of the existing templates). Provide a display name, name, the Web Service URL, which is the HTTP endpoint of your API and an API URL affix. Select HTTP, HTTPS or both as an URL schema. Add the previously added product in the Products drop-down. Click Create. Create a new API Add an operation to your APIBefore you can use your new API, you must add one or more operations. Their operations do things like enable service documentation, the interactive API console, or set operation limits.To add an operation, follow these steps: On your previously created API, click on +Add operation. Provide a name, URL and HTTP verb. Optionally, you can create parameters. Click Create. Add an operation to your API Configure API Management policiesAPI Management policies allow you as the publisher, to determine the behavior of your APIs through configuration, requiring no code changes. There are many built-in policies, like allowing cross-domain calls, authenticate requests or setting rate limits. The policies statements you choose affect both inbound requests and outbound responses. Policies can be applied globally, or scoped to the Product, API, or Operation level.To configure a policy, follow these steps: In the API Management service, select the APIs blade and click on All APIs. Click on +Add on the Inbound processing tab. Click Code View on the top right corner. Add your desired policy, for example, to enable CORS from all domains. After you are done, click Save Add a policy for CORS To see all available policies, go to the Azure documentation.Protect APIs with rate limitsProtecting your published APIs by throttling incoming requests is one of the most attractive offerings of API Management. Limiting incoming requests helps you controlling your resource costs, preventing you from unnecessarily scaling up your services to meet unexpected demand. Rate limiting, or throttling, is common practice when providing APIs. Oftentimes, API publishers offer varying levels of access to their APIs. For instance, you may choose to offer a free tier with very restrictive rate limits, and various paid tiers offering higher request rates. This is where API Management’s product comes into play.Create a product to scope rate limits to a group of APIsTo create a rate limited API, follow these steps: On the API Management service, select the Products blade under the API Management menu. On the Products blade, click +Add. Provide a name and description. Add your previously created API. Click Create. Create a new Product to limit the requests After the new product is published, open it and select Policies. Add the following code to the inbound rule: &lt;rate-limitcalls=&#8221;10&#8243;renewal-period=&#8221;60&#8243;&gt; &lt;/rate-limit&gt; &lt;quotacalls=&#8221;200&#8243;renewal-period=&#8221;604800&#8243;&gt; &lt;/quota&gt; Click Save Advanced rate limitingIf you want to avoid high-usage consumers limit access to occasional users, by using up the pool of available resources, consider using the rate-limit-by-key and quota-by-key policies. These are more flexible rate limiting policies that allow defining an expression to track traffic usage by user-level information such as IP address or user identity.The following code limits the rate by the users IP address:&lt;rate-limit-by-key calls=”10″ renewal-period=”60″ counter-key=”@(context.Request.IpAddress)” /&gt;Add caching to improve performanceCaching is a great way to limit your resource consumption, like bandwidth, as well as reduce latency for infrequently changing data.To add caching to your API, follow these steps: In the API Management service, click the APIs blade and select the Echo API. Click on the Operations blade and select Retrieve resource (cached). Switch to the Caching tab. Here you can configure caching and also copy the code from the code view to use it in your own API.Monitor APIsAPI Management provides a few methods by which you can monitor resource usage, service health, activities, and analytics. If you want real-time monitoring, as well as richer debugging, you can enable diagnostics on your logic app and send events to OMS with Log Analytics, or to other services, such as Azure Storage, and Event Hubs. Select the Diagnostics logs under the Monitoring menu from your API Management service, and then select Turn on diagnostics to archive your gateway logs and metrics to a storage account, stream to an Event Hub, or send to Log Analytics on OMS.Like all Azure resources, you can view several metrics under the Metrics blade or create alerts under the Alerts blade.ConclusionIn this post, I introduced Azure API Management. I showed how to create an API Management service, and add APIs, products and operations to it and how to enable and configure policies on different scopes. Additionally to the configuration, I mentioned how to monitor APIs and how to enable alerts.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Develop Azure App Service Mobile App", "url": "/develop-azure-app-service-mobile-app/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-06-29 22:21:13 +0200", "snippet": "A Mobile App in Azure App Service provides a platform for the development of mobile applications, providing a combination of backend Azure hosted services with device side development frameworks that streamline the integration of the backend services.Mobile Apps enables the development of applications across a variety of platforms, targeting native iOS, Android, and Windows apps, cross-platform Xamarin and Cordova.Azure Mobile Apps provide functionality for: Functionality Description Authentication and authorization Enables integration with identity providers including Azure Active Directory, Google, Twitter, Facebook, and Microsoft. Data Access Enables access to tabular data stored in an Azure SQL Database or an on-premise SQL Server via an automatically provisioned and mobile-friendly OData v3 data source Offline sync Enables reads as well as create, update, and delete activity to happen against the supporting tables even when the device is not connected to the network, and coordinates the synchronization of data between local and cloud stores as dictated by the application logic. Push notifications Enables the sending of push notifications to app users via Azure Notifications Hubs, which in turn supports the sending of notifications across the most popular push notifications services for Apple, Google, Windows and Amazon devices.  Create a mobile appThe following steps are necessary to create a mobile app: Identify the target device platforms you want your app to target. Prepare your development environment. Deploy an Azure Mobile App Service instance. Configure the Azure Mobile App Service. Configure your client application. Augment your project with authentication/authorization, offline data sync, or push notification capabilities.Identify the target device platformDecide if you want to develop your Mobile App as native Android, Cordova, native iOS, Windows, Xamarin Android, Xamarin Forms or Xamarion iOS application.Prepare your development environmentThe requirements for your development environment vary depending on the device platforms you wish to target. For example, iOS developers use macOS and Xcode whereas Android developers macOS or Windows and Android Studio use.Deploy an Azure Mobile App ServiceTo create an Azure Mobile App Service instance, follow these steps: In the Azure portal click on +Create a resource, search for Mobile App and click Create. Provide a name, subscription, resource group and, service plan. Click Create Create a Mobile App Configure the Mobile App After the Mobile App is deployed, follow these steps to configure it: Open your Mobile App in the Azure portal and click on Quick Start under the Deployment menu. On the Quickstart blade, select the platform you wish to target, for example, Windows (C#). Click on “You will need a database in order to complete this quick start. Click here to create one. On the data Connections blade, click +Add to create a new SQL database or select one from the list if you already have one. Select a backend language, for example, C#. Download the zip file and unpack it. Compile the application and then deploy it to your Mobile App to Azure. To deploy your app right click on the project in Visual Studio, select Publish and then select your previously created Mobile App by clicking on Select Existing in the Azure App Service tab. Deploy your Mobile App to Azure After the deployment is finished, the browser is opened and displays that your Mobile App is running. The Mobile App is deployed and running Configure your client applicationYou can create a new application from the Quickstart blade, or by connecting an existing application. To download the template application, click on Create a new app and then download under section 3 Configure your client application on the Quickstart blade. Download templates for your Mobile App Add authentication to a mobile appTo enable the integration of identity providers like Facebook or Twitter, follow these steps: For each identity provider, you want to support, you need to follow the provider’s specific instructions to register your app and retrieve the credentials needed to authenticate using that provided. Open your Mobile App and select Authentication / Authorization under the Settings blade. On the Authentication / Authorization configure the provider you want to use. For example, for Twitter, you have to enter your API Key and API Secret which you get on Twitter’s developer page. In the Allowed external redirect URLs textbox, enter your callback URL, for example, myapp://myauth.callback. Enable Twitter authentication and authorization Click Save. In your C# application decorate all controller and/or action which should be only accessed be logged in users with the [Authorize] attribute. Add your authentication logic to the project. Run your application in your local simulator or device to verify the authentication flow. The steps 6-8 are only an overview because an exact explanation would be too much for here. If you don’t know how the Authorize attribute works, create a new ASP.NET MVC project form the template in Visual Studio. There it is implemented and you can see that an action or controller with this attribute redirects the user to the login page.Add offline sync to a Mobile AppThe offline data sync capability comes from a mix of client-side SDK and service-side features. These features include, for example, support for conflict detection when the same record is changed on both the client and backend, and it allows for the conflicts to be resolved.For the conflicts to be resolved you need a table that leverages Mobile App easy tables on the service side. This is usually a SQL Database exposed by Mobile Apps using the OData endpoint.On the client side, the Azure Mobile App  SDKs provide an interface referred to as a SyncTable that wraps access to the remote easy table. When using a Synctable all the CRUD operations work from a local store, whose implementation is device platform specific. For example, iOS uses the local store based on Core Data whereas Xamarin and Androids local store is based on SQL lite. Changes to the data are made through a sync context object that tracks the changes that are made across all tables. This sync context maintains an operation queue that is an ordered list of create, update and delete operations that have been performed against the data locally.Add push notifications to a Mobile AppPush notifications enable you to send app-specific messages to your app running across a variety of platforms. In Azure Mobile Apps, push notification capabilities are provided by Azure Notification Hubs. Notification Hubs abstract your application from the complexities of dealing with the various push notification systems (PNS) that are specific to each platform. Notification Hubs support the sending of notifications across the most popular push notification services for Apple, Google, Windows, and Amazon.To add push notifications, follow these steps: Open your Mobile App and select Push under the Settings menu. On the Push blade, connect to a Notification Hub. If you don’t have one yet, you can create one on the blade. After your Notification Hub is connected, click on Configure push notification services. On the Push notification services blade, enter the configuration of the notification service you want to use. Click Save. Configure your app project to respond to push notifications.ConclusionIn this post, I showed how to leverage Azure Mobile App services to easily create a mobile app using modern features like push notifications and offline sync.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Develop an Azure App Service Logic App", "url": "/develop-azure-app-service-logic-app/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-06-28 22:20:26 +0200", "snippet": "Azure Logic Apps is a fully managed iPaaS (integration Platform as Service) that helps you simplify and implement scalable integrations and workflows from the cloud. When you create a Logic App, you start out with a trigger, for example, “When a tweet contains a certain hashtag”, and then you act on that trigger with many combinations of actions, condition logic, and conversions.There is a huge amount of built-in connectors like Twitter, Office 365, Azure Blob Storage or Salesforce.Create a Logic App connecting SaaS servicesThe probably biggest strengths of Logic Apps is its ability to connect a large number of SaaS service to create your own custom workflows. In the following demo, I will connect Twitter with an outlook mailbox to email certain tweets as they arrive.To create a Logic App, follow these steps: In the Azure portal click on +Create a resource, search for Logic App and click Create. Provide a name, subscription, resource group and location. Click Create. Create a Logic App After the Logic App is created, open it to view the Logic Apps Designer. This is where you design or modify your Logic App. You can select from a series of commonly used triggers, or from several templates you can use as a starting point.Create your own templateFollowing, I will create a template from scratch: Select Blank Logic App under Templates. All Logic Apps start with a trigger. Select Twitter from the list. Click Sign in to connect Twitter with your account and authorize the Logic App to access your account. After you are logged in, enter your search text to return certain tweets (for example #Azure), and select an interval and frequency for how often to check for items. Connect the Logic App with Twitter The next step is to add another action by clicking + New step and select Add an action. Search for Gmail and select Gmail &#8211; send email. In the configuration window enter the recipient of the email, a subject, and the body. In the body, you could add for example the name of the person who tweeted and the tweet text. Configure the Gmail connector to send an email Save the template. You can test it immediately by clicking Run. Send a tweet and you should get an email to the configured email. The received email from the Gmail connector If you set everything up correctly, you should receive an email. First, I wanted to use Outlook 365 to send emails but I couldn’t log in although I tried three different accounts. Gmail only worked with the second account I tried. I couldn’t find anything about that on Google nor do I have any idea why it didn’t work.Create a Logic App with B2B capabilitiesLogic Apps support business-to-business (B2B) workflows through the Enterprise Integration pack. This allows organizations to exchange messages electronically, even if they use different protocols and formats. Enterprise integration allows you to store all your artifacts in one place, within your integration account, and secure message through encryption and digital signature.Create an integration accountTo create an integration account, follow these steps: In the Azure portal click on +Create a resource, search for Integration Account and click Create. Provide a name, subscription, resource group, pricing tier and location. Note that your integration account and Logic App must be in the same location, otherwise you can’t link them. Click Create. Create a new integration account Add partners to your integration accountMessages between partners are called agreement. You need at least two partners in your integration account to create an agreement. Your organization must be the host partner, and the other partner(s) are guests. Guest partners can be outside organizations or even a different department in your organization.To add a partner to your integration account, follow these steps: Open your integration account and click on the Partners blade under the Settings menu. On the Partners blade click on +Add and provide a name, qualifier, and value to help identify documents that transfer through your apps. As the qualifier, you have to select, AS2Identity, otherwise you can’t create an AS2 agreement in the next section. Click OK. Add a new partner I added another partner and both are added to the list on the Partners blade. Both partners are listed Add an agreementAfter the partners are associated with the integration account, you have to allow them to communicate using industry standard protocols through agreements. These agreements are based on the type of information exchanged, and through which protocol or standard they will communicate: AS2, X12 or EDIFACT.To create an AS2 agreement, follow these steps: Open your integration account and click on the Agreements blade under the Settings menu. On the Agreementsblade click on +Add and provide a name and select AS2 as agreement type. Select your previously created host and guest partner and their identity. Click OK Create a new AS2 agreement Link your Logic App to your Enterprise Integration accountAfter the integration account is set up, you can link it with your Logic App to create B2B workflows. As mentioned before, the integration account and Logic App must be in the same region to be linked.To link them, follow these steps: Open your Logic App and click on the Workflow settings blade under the Settings menu. On the Workflow settings blade, select your integration account in the “Select an Integration account” drop-down menu. Click Save. Link your Logic App and integration account Use B2B features to receive data in Logic AppsAfter the Logic App and integration account are linked, you can create a B2B workflow using the Enterprise Integration Pack., following these steps: Open your Logic App and click on the Logic App Designer blade under the Development Tools menu. On the Logic App Designer click on Blank Logic App under templates. Search for http and select Request – When a HTTP request is received. Click on + New step, then on Add an action and search for AS2. Select AS2 – Decode AS2 message. Provide a connection name, select your integration account and click Create. Add the decode AS2 message form In the body section select the Body from the HTTP request and as Headers select Headers. If you can&#8217;t select Headers, click on the small button on the right to switch to text mode. Setting the Decode AS2 Message body and headers information form in the Logic Click on + New step, then on Add an action and search for X12. Select X12 &#8211; Decode X12 Message. Enter a connection name, select your integration account and click Create. Enter a connection name and select your integration account Since the message content is JSON-formated and base64-encoded, you must specify an expression as the input. Enter the following expression in the X12 flat file message to decode textbox: @base64ToString(body(&#8216;Decode_AS2_Message&#8217;)?[&#8216;AS2Message&#8217;]?[&#8216;Content&#8217;]) Decode the AS2 message Click on + New step, then on Add an action and search for response. Select Request &#8211; Response. Paste @base64ToString(body(&#8216;Decode_AS2_message&#8217;)?[&#8216;OutgoingMdn&#8217;]?[&#8216;Content&#8217;]) into the Body textbox. Configure the response Click Save. If you configured everything right, your template will be saved, if not you get an error message, which is usually helpful. Create a Logic App with XML capabilitiesOften, businesses send and receive data between organizations in the XML format. Schemas are used to transform data from one format to another. Transforms are also known as maps, which consist of source and target XML schemas. Linking your Logic App with an integration account enables your Logic App to use Enterprise Integration Pack XML capabilities.The XML features in the Enterprise Integration Pack are: XML feature Description XML validation XML validation is used to validate incoming and outgoing XML messages against a specific schema. XML transform XML transform is used to convert data from one format to another. Flat file encoding/decoding Flat file encoding/decoding is used to encode XML content prior sending or to convert XML content to flat files. XPath XPath is used to extract specific properties form a message, using an XPath expression. Add schemas to your integration accountSince schemas are used to validate and transform XML messages, you must add one or more to your integration account before working with the Enterprise Integration Pack XML feature within your linked logic app.To add a new schema, follow these steps: Open your integration account and select the Schemas blade under the under the Settings menu. On the Schemas blade, click +Add. Provide a name, select whether it is a small or large file and upload an XSD file. (You can find a sample XSD here) Click OK. Add a schema to your integration account Add maps to your Integration AccountIf your Logic App should transform data from one format to another one, you have to add a map (schema) first.To add a new schema, follow these steps: Open your integration account and select the Maps blade under the under the Settings menu. On the Maps blade, click +Add. Enter a name and upload an XSLT file (You can find a sample XSLT here). Click OK. Add a map to your integration account Add XML capabilities to the linked Logic AppAfter adding an XML schema and map to the integration account, the application is ready to use the Enterprise Integration Pack’s XML validation, XPath Extract, and Transform XML operations in Logic App.Follow these steps to use XML capabilities in your Logic App: Open the Logic App Designer in your Logic App. Select Blank Logic App under Templates. Search for http and select Request – When a HTTP request is received. Click on + New Step and select Add an action. Search for xml and select XML – XML Validation. Select Body as Content and the previously uploaded schema Add XML validation to your Logic App Click on + New step and select Add an action. Search for xml and select Transform XML &#8211; Transform XML. Select the HTTP Body as Content and your previously added map. Add XML transformation to your Logic App Unfortunately the Azure Portal displayed various errors at this point and Microsoft’s documentation was outdated.Trigger a Logic App from another appThe most common type of triggers are those that create HTTP endpoints. Triggers based on HTTP endpoints tend to be more widely used due to the simplicity of making REST-based calls from practically any web-enabled development platform. Trigger Description Request The HTTP endpoint responds to incoming HTTP request to start the Logic App&#8217;s workflow in real time. Very versatile, in that it can be called from any web-based application external webhook events, even from another Logic App with a request and response action. HTTP Webhook A webhook is an event-based trigger that does not rely on polling for new items. Register subscribe and unsubscribe methods with a callback URL are used to trigger the Logic App. Whenever an external or app or service makes an HTTP POST to the callback URL, the Logic App fires, and includes any data passed into the request. API Connection Webhook The API connection trigger is similar to the HTTP trigger in its basic functionality. However, the parameters for identifying the action are slightly different Create an HTTP endpoint for your Logic AppTo create an HTTP endpoint to receive an incoming request for a Request Trigger, follow these steps: Open the Logic App Designer in your Logic App. Select Blank Logic App under Templates. Search for http and select Request – When a HTTP request is received. Optionally enter a JSON schema for the payload that you expect to be sent to the trigger. This schema can be added to the Request Body JSON Schema field. To generate the schema, select the Use sample payload to generate schema link at the bottom of the form. This displays a dialog where you can type in or paste a sample JSON payload. The advantage of having a schema defined is that the designer will use the schema to generate tokens that your logic app can use to consume, parse, and pass data from the trigger through your workflow. Enter a JSON schema After saving, the HTTP Post URL is generated on the Receiver trigger. This is the URL your app or service uses to trigger your logic app. The URL contains a Shared Access Signature (SAS) token used to authenticate the incoming requests. The generated HTTP POST URL for the trigger Create custom and long-running actionsYou can create your own APIs that provide custom actions and triggers. Because these are web-based APIs that use REST API endpoints, you can build them in any language you like.API apps are preferred because to host your APIs since they will make it easier to build, host, and consume your APIs used by Logic Apps. Another recommendation is to provide an OpenAPI (Swagger) specification to describe your REST API endpoints, their operations, and parameters. This makes it much easier to reference your custom API from a Logic App workflow because all of the endpoints are selectable within the designer. you can use libraries like Swashbuckle to automatically generate the OpenAPI file for you. You can read more about Azure’s API Service in Design Azure App Services API Apps.If your custom API has long-running tasks to perform, it is more than likely that your Logic App will timeout waiting for the operation to complete. This is because the Logic App will only wait around two minutes before timing out. If your task takes several minutes or even hours to complete, you need to implement a REST-based async pattern on your API. These types of patterns are already fully supported natively by the Logic Apps workflow engine, so you don’t need to worry about implementing it there.Long-running action patternsThe asynchronous polling pattern and the asynchronous webhook pattern allow your Logic App to wait for long-running tasks to finish.Asynchronous pollingThe asynchronous polling pattern works the following way: When your API receives the initial request to start work, it starts a new thread with the long-running task, and immediately returns an HTTP Response 202 Accepted code with a location header. This immediate response prevents the request from timing out and causes the workflow engine to start polling for changes. The location header points to the URL for the Logic Apps to check the status of the long-running job. By default, the engine checks every 20 seconds, but you can also add a “Retry-after” header to specify the number of seconds until the next poll. After the allotted time of 20 seconds, the engine poll the URL on the location header. If the long-running job is still going, you should return another 202 Accepted with a location header. If the job was completed, return a 200 OK code with any relevant data. The Logic App will continue its workflow with this data.Asynchronous WebhooksThe asynchronous webhook pattern works by creating two endpoints on your API controller the following way: Endpoint Description Subscribe The Logic App engine calls the subscribe endpoint defined in the workflow action for your API. Included in this call is a callback URL created by the Logic App that your API stores for when work is complete. When your long-running task is complete, your API calls back with an HTTP POST method to the URL, along with any returned content and headers, as input to the Logic App. Unsubscribe The unsubscribe endpoint is called any time the logic app run is canceled. When your API receives a request to this endpoint, it should unregister the callback URL and stop any running processes. Monitor Logic AppsTo monitor your Logic Apps, you can use out-of-the-box tools within your Log App to detect any issues it may have. For real-time event monitoring and richer debugging, you can enable diagnostics and send events to OMS with Log Analytics, or to other services, such as Azure Storage or Event Hubs.Select Metrics under the Monitoring menu and select the metrics you want to look at such as runs started or runs succeeded. You can configure the type of event and the time span you want to look at. Monitoring your Logic App On the Overview blade, you can see the history of the runs, the status, start time and duration.ConclusionIn this post, I showed different operations which can be done with Logic Apps. Working with XML messages and transformation didn’t work well. Additionally the documentation from Microsoft is outdated and not helpful. My favorite part was using the Logic App to interact with Twitter and send an email when a certain hashtag was used. With Azure’s built-in connectors it would be easy to add image recognition or text recognition using AI. For example, I could analyze the images and if they are inappropriate, notify someone via email.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Design Azure App Service API Apps", "url": "/design-azure-app-service-api-apps/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-05-08 22:07:41 +0200", "snippet": "Azure API Apps provide a quick and easy way to create and consume scalable RESTful APIs, using the language of your choice. The Azure portal helps you to enable CORS to support access to your API from any client and Swagger support makes generating client code to use your API simple.Create and deploy API AppsThere are different ways to create and deploy your API Apps. You could use Visual Studio to create a new API Apps project and publish it to a new API app. Additionally to Visual Studio, you can use the Azure portal, Azure CLI, or PowerShell to provision a new API App service.Creating a new API App from the Azure portalTo create a new API app, follow these steps: In the Azure portal click on +Create a resource, search for API App and click Create. On the API App blade, provide a name, subscription, resource group and App Service plan. Click Create. Create a new API App After the API App is deployed, you can download a sample project in ASP.NET, NodeJs or Java by clicking on Quickstart under the Deployment menu. Download a sample project Creating and deploying a new API App with Visual StudioTo create a new API App open Visual Studio 2017 and create a new ASP.NET project with the Azure API App template. Create a new ASP.NET application with the Azure API App template Visual Studio creates a new API App project and adds NuGet packages, such as: Newtsonsoft.Json for deserializing requests and serializing responses to and from API app. Swashbuckle to add Swagger for rich discovery and documentation for your API REST endpoints.Follow these steps to deploy your API App from Visual Studio: Right-click on your project in the Solution Explorer and click Publish. In the Publish dialog, select the Create New option on the App Service tab and click Publish. On the Create App Service blade, provide an App name, subscription, resource group and hosting plan. Create App Service Click Create. After the API is deployed, the browser will be automatically opened. Automate API discovery by using SwashbuckleSwagger is a popular, open source framework backed by a large ecosystem of tools that helps you design, build, document, and consume your RESTful APIs.Generating Swagger metadata manually can be a very tedious process. If you build your API using ASP.NET or ASP.NET Core, you can use the Swashbuckle NuGet package to automatically do this for you, saving a lot of time creating the metadata and maintaining it. Additionally, Swashbuckle also contains an embedded version of swagger-ui, which it will automatically serve up once Swashbuckle is installed.Use Swashbuckle in your API App projectTo work with swagger, follow these steps: Go to https://swagger.io/swagger-ui/ and either download swagger or use the live demo. I used the live demo and entered the path to my API definition, https://wolfgangapi.azurewebsites.net/swagger/docs/v1. You can find the definition in the Azure portal by clicking on API definition under the API menu. Click Explore and the available API endpoints will be displayed. The available endpoints of your API If you get an error message, you probably have to enable CORS. You can do that in the Azure portal, in your Web API by clicking on CORS under the API menu. For more information see Enable CORS to allow clients to consume API and Swagger interfaces. To test your API endpoints, click on one of them, for example, GET /api/Values. Click on Try it out. Click Execute and the result will be displayed after a couple of seconds. Test your API with swagger Enable CORS to allow clients to consume API and Swagger interfacesTo enable CORS (Cross-Origin Resource Sharing), follow these steps: In the Azure portal on your API App service, click on CORS under the API menu.  On the CORS blade, enter the allowed origins or enter an asterisk to allow all origins. Click Save. Enable CORS Use Swagger API metadata to generate client code for an API AppThere are tools available to generate client code for your API Apps that have Swagger API definitions, like the Swagger.io online editor.To generate client code for your API App that has Swagger API metadata, follow these steps: In the Azure portal, go to your API App and select API definition under the API menu. On the API definition, copy the API definition. Copy the API definition With the API definition copied, go to http://editor.swagger.io/ to use the Swagger.io online editor. Click on File &#8211;&gt; Import URL and paste your API definition. Click OK. Paste the API definition into the swagger editor The discovered API endpoints will be displayed on the right side. Click on Generate Client and select your desired language. Download the client application for your desired language The client application will be downloaded as a .zip file. Monitor API AppsApp Service provides built-in monitoring capabilities, such as resource quotas and metrics. You can set up alerts and automatic scaling based on these metrics. Additionally, Azure provides built-in diagnostics to assist with debugging. A combination of the monitoring capabilities and logging should provide you with the information you need to monitor the health of your API App, and determine whether it is able to meet capacity demands.Using quotas and metricsThe resource limits of API Apps are defined by the App Service plan associated with the app.If you exceed the CPI and bandwidth quotas, your app will respond with a 403 HTTP error. Therefore, you should keep an eye on your resource usage. Exceeding memory quotas causes an application reset, and exceeding the file system quota will cause write operations to fail, even to logs. If you need more resources, you can upgrade your App Service plan.Enable and review diagnostics logsTo enable and review diagnostics logs, see my last post Design Azure App Service Web App.ConclusionThis post showed how to create an Azure API App in the Azure portal and how to create and deploy an API application using Visual Studio 2017. Next, I showed how to test your API endpoints with Swagger and how to create a client application using the online editor of Swagger. During the testing process, I showed how to configure CORS, otherwise, you won’t be able to test your endpoints.The last section explained how to monitor your API Apps and how quotas work.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Design Azure App Service Web App", "url": "/design-azure-app-service-web-app/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-05-07 18:47:57 +0200", "snippet": "Azure Web App provides a managed service for hosting your web applications and APIs with infrastructure services such as security, load balancing, and scaling provided as part of the service. Additionally, it has integrated DevOps experience from code repositories and from Docker image repositories. You pay for computing resources according to your App Service Plan and scale settings.Define and manage App Service plansAn App Service plan defines the supported feature set and capacity of a group of virtual machine resources that are hosting one or more web apps, logic apps, mobile apps, or API apps.Each App Service plan is configured with a pricing tier. There are four pricing tiers available: Free, Shared, Basic, and Standard). An App Service plan is unique to the region, resource group, and subscription.Creating a new App Service planTo create a new App Service plan, follow these steps: In the Azure portal, click on +Create a resource, search for App Service Plan and click Create. On the New App Service Plan blade, provide an App service plan name, subscription, resource group, operating system, location, and pricing tier. Click Create. Create a new App Service plan Creating a new Web AppTo create a new Web App, follow these steps: In the Azure portal, click on +Create a resource, search for Web App and click Create. On the Web App blade, provide an App name, subscription, resource group, OS, and  App Service plan. You can take either the previously created one or create a new one. Click Create. Create a new Web App Review App Service plan settingsAfter you have created a new App Service plan, you can change the settings, following these steps: In the Azure portal, open your App Service plans. There you can see all your App Service plans, the number of apps deployed for each plan and its pricing tier. An overview of my App Service plans Click on your App Service plan to manage its settings. You can scale up or scale out your Service plan or you can integrate a VNET. Configure Web App settingsYou can configure the following groups of settings for your Web App application: Load balancing IIS related settings App settings and connection strings Slot management Debugging Application type and library versionsConfigure Web App settings in the Azure PortalTo manage your Web App settings, follow these steps: In the Azure portal, open the Webb App, you want to configure and select Application settings under the Settings menu. On the Application settings, you can choose the following settings: Select language support for .NET, PHP, Python, or Java and set the desired version of the language. Choose between a 32 or 64 bit runtime environment. Enable or disable web sockets. Enable Always On. This configures that the web application will be kept in the memory all the time. Therefore the load time for the next request is reduced. Chose the type of pipeline for IIS. Integrated is the more modern pipeline and Classic would only be used for legacy applications. Change the language and runtime settings of your application Turn on ARR Affinity to enable sticky sessions. Sticky session means that a user will always be routed to the same host machine. The downside of sticky sessions is that the performance might be lower. ARR Affinity settings When you first create your web app, the auto swap settings are disabled. You must first create a new slot and from the slow, you may configure auto swap to another slot. The next setting is a really cool one, remote debugging. Enable remote debugging if you run into situations where deployed applications are not functioning as expected. Remote debugging can be enabled for Visual Studio 2012 &#8211; 2017. Remote debugging The application settings override any settings with the same name from your application. The connection strings override the matching ones from your application as well. The value of the connection string is hidden unless you click on it. Application and connection settings Configure Web App certificates and custom domainsWhen you first create your web app, it is accessible through yourwebappname.azurewebsites.net. To map to a more user-friendly domain, you must set up a custom domain name.If you use HTTPS, you need to utilize an SSL certificate. You can use your SSL certificate with your web app in one of two ways: You can use the built-in wildcard SSL certificate that is associated with the *.azurewebsites.net domain. More commonly you use a certificate you can purchase for your custom domain from a third-party authority.Mapping custom domain namesThe mapping is captured in domain name system (DNS) records that are maintained by your domain registrar. Two types of DNS records effectively express this purpose: A records (address records) map your domain name to the UP address of your website. CNAME records (alias records) map a subdomain of your custom domain name to the canonical name of your website,CNAME records enable you to map only subdomains.Configuring a custom domainYou need to access your domain registrar setup for the domain while also editing the configuration for your web app in the Azure portal. Note that custom domain names are not supported by the Free App service plan pricing tier.To configure a custom domain, follow these steps: In the Azure portal, go to your Web App and select Custom domains under the Settings menu. Custom domains are not available on the Free pricing tier. On the Custom domains blade, click on +Add hostname and enter your Hostname. Then choose if you want to set up an A record or CNAME record. Add a hostname for your custom domain To set up an A record, select A Record and follow the instruction in the blade. The instructions are: First add a TXT record at your domain name registrar, pointing to the default Azure domain for your web app, to verify you own the domain name. The new TXT record should point to yourwebappname.azurewebsites.net. Additionally, add an A record pointing to the IP address shown on the blade, for your web app. To set up a CNAME record, select CNAME Record and follow the instruction in the blade. The instructions are: If using a CNAME record, following the instructions provided by your domain name registrar, add a new CNAME record with the name of the subdomain, and for the value, specify your web app&#8217;s default Azure domain with yourwebappname.azurewebsites.net. Save your DNS changes. Note that it may take up to 48 hours to propagate the changes across DNS. Click Add Hostname again to configure your custom domain. Enter the domain name and select Validate again. If validation passed, select Add Hostname to complete the assignment. Configure SSL certificatesTo configure SSL certificates for your custom domain, you need to have access to an SSL certificate that includes your custom domain name, including the CNAME if it is not a wildcard certificate.To assign an SSL certificate, follow these steps: In the Azure portal, go to your Web App and select SSL Configuration under the Settings menu. On the SSL settings blade, import an existing certificate, or upload a new one. Select Add Binding to set up the correct binding. You can set up binding that point at your naked domain (programmingwithwolfgang.com), or to a particular CNAME (www.programmingwithwolfgang.com), as long as the certificate supports it. Select ServerName Indication (SNI) or IP Based SSL as SSL Type.Manage Web Apps by using the API, Azure PowerShell, and Xplat-CLIAdditionally, to managing your Web App in the Azure portal, programmatic or script-based access is available.The following options are available: Configuration Method Description Azure Resource Manager (ARM) Azure Resource Manager provides a consistent management layer for the management tasks you can perform using PowerShell, Azure CLI, Azure portal, REST API, and other development tools. REST API The REST API enables you to deploy and manage Azure infrastructure resources using HTTP request and JSON payloads. PowerShell PowerShell provides cmdlets for interacting with ARM to manage infrastructure resources. The PowerShell modules can be installed to Windows, macOS, or Linux. Azure CL Azure CLI (also known as XplatCLI) is a command line experience for managing Azure resources. This is an open source SDK that works on Windows, macOS, and Linux platforms to create, manage, and monitor web apps. Implement diagnostics, monitoring, and analyticsWithout diagnostics, monitoring, and analytics, you can’t effectively investigate the cause of a failure, nor can you proactively prevent potential problems before your users experience them. Web Apps provide multiple forms of logs, features for monitoring availability and automatically sending an email alert when the availability crosses a threshold, features for monitoring your web app resource usage, and integration with Azure Analytics via Application Insights.App Services are also governed by quotas depending on the App Service plan you have chosen. Free and Shared apps have CPU, memory, bandwidth, and file system quotas. When reached, the web app no longer runs until the next cycle, or the App Service plan is changed. Basic, Standard, and Premium App Services are only limited by file system quotas based on the SKU size selected for the host.Configure diagnostic logsThere are five different types of logs: Log Description Event Log The Event logs are equivalent to the Windows Event Log on a Windows Server. It is useful for capturing unhandled exceptions. Only one XML file is created per web app. Web server logs Web server logs are textual files that create a text entry for each HTTP request to the web app. Detailed error message logs These HTML flies are generated by the web server and log the error messages for failed requests that result in an HTTP status code of 400 or higher. One error message is captured per HTML file. Failed request tracing logs Additionally to the error message, the stack trace that led to a failed HTTP request is captured in these XML documents that are present with an XSL style sheet for in-browser consumption. One failed request trace is captured per XML file. Application diagnostic logs These text-based trace logs are created by web application code in a manner specific to the platform the application is built in using logging or tracing utilities. To enable these diagnostic settings, follow these steps: In the Azure portal, open your Webb App and click Diagnostic logs under the Monitoring menu. On the Diagnostics logs blade, enable your desired logs. If you enabled Application Logging, you have to provide a storage account. If you enabled Web server logging, you have to provide either a storage account or you can log to the file system. Click Save. Configure logging for your Web App application If you enabled Application Logging or Web server logging to the file system, you can see the logs as a stream by clicking on Log stream under the Monitoring blade. Application log stream Configure endpoint monitoringYou can monitor many different resources such as: Average Response Time CPU Time HTTP 2xx Data In Data Out HTTP 3xx HTTP 4xx Requests HTTP Server ErrorsTo customize the shown metrics, follow these steps: In the Azure portal, open your Webb App and click on one of the graphs on the Overview blade. On the Metrics blade, add or remove all metrics, you want to display on the graph. Enter a title, select a chart type and set a time range. Customize the metrics graph Additionally, you can set an alarm, so you get an email when a certain threshold is reached, for example, 90% CPU usage. Click Save and close. Design and configure a Web App for scale and resilienceApp Services can scale up and down by adjusting the instance size and scale out or in by changing the number of instances serving requests. Scaling can be configured to be automatic. The advantage of automatic scaling up or out is that at peaks the resources are increased. As a result, your customers don’t experience any problems with the performance. During less busy hours, scaling down or helps you to save costs.Additionally, to scaling within the same datacenter, you can also scale a web app by deploying to multiple regions around the globe and then utilizing Microsoft Azure Traffic Manager to direct web app traffic to the appropriate region based on a round-robin strategy or according to performance. Alternately, you can configure Traffic Manager to use alternate regions as targets for failover if the primary region becomes unavailable.Configure scalingTo configure scaling for your web app, follow these steps. In the Azure portal, open your Service Plan and click Scale up under the Settings blade. On the Scale up blade, select your desired pricing tier and click Apply. Select Scale out under the Settings blade and select the desired instance count to scale out to. Configure scaling out If you select Enable autoscale, you can create conditions based on metrics and rules in order for the site to automatically scale out or in. ConclusionThis post talked about how App Service is created and how the settings can be changed. Then, I showed that you can configure your Web App directly in the Azure portal and that you can override the settings from your application can be overridden.Next, I showed how to apply a custom domain and an SSL certificate to your Web App. Note that certificates and custom domains are not supported by the Free pricing tier.After the custom domain and SSL was configured, I showed how to enable various logs and how to customize the monitoring graphs to give you exactly the information you need.The last section talked about scaling out or up to increase the used resources and scaling in or down to save costs during less busy hours.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Implement Azure Search", "url": "/implement-azure-search/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-05-02 17:05:35 +0200", "snippet": "Azure Search is a Platform as a Service offering that gives developers APIs needed to add search functionality to their applications. Primarily this means full-text search. The typical example is how Google and Bing search works. They don’t care what tense you use, it spell checks for you, and finds similar topics based on the search term. It also offers term highlighting and can ignore noise words, as well as many other search-related features. Applying these features inside your application can give your users a rich and comforting search experience.Create a service indexThere are four different types of Azure Search accounts: Free, Basic, Standard, and High-density. The free tier only allows 50 MB of data storage and 10,000 documents. The higher you go with the pricing tier, the more documents you can index and the faster a search returns results. Computer resources for Azure Search are sold through Search Units (SUs). The basic level allows 3 search units whereas the high-density tier offers up to 36 SUs. Additionally, all of the paid pricing tiers offer load-balancing over three or more replicas.Create an Azure Search serviceTo create an Azure Search service, follow these steps: In the Azure portal click on +Create a resource, search for Azure Search and then click on Create. On the New Search Service blade, provide an URL, subscription, resource group, location, and pricing tier. Click Create. Create a new Azure Search Service Scale an existing Azure Search ServiceYou can only scale an Azure Search Service with a paid pricing tier. In your Azure Search Service, click on Scale under the Settings menu. On the Scale blade, select your desired partitions and replicas. Click Save. Scale your Azure Search Service Replicas distribute workloads across multiple nodes. Partitions allow for scaling the document count as well as faster data ingestion by spanning your index over multiple Azure Search Units.Implement Azure Search using C#To add data to Azure Search, create an index. An index contains documents used by Azure Search. For instance, a car dealer might have a document describing each car they sell. An index is similar to a SQL Server table and documents are similar to rows in those tables.You can find the code for the following demo on GitHub. To add data to an index and search it using C#, follow these steps: Create a new C# console application with Visual Studio 2017. Install the Microsoft.Azure.Search NuGet package. Set up the connection to your Azure Search Service. Note that the serviceName is only the name you entered. Not a URI. Set up the connection to the Azure Search Service account Add the System.ComponentModel.DataAnnotations reference to your project. Create a POCO for the cars you want to be searchable: The car class for the search The next step is to create an index. The following code will create an index object with field objects that define the correct schema based on the car POCO. The FieldBuilder class iterates over the properties of the Car POCO using reflections. Create an index object for the search Next, I create a batch of cars to upload: Create a car array This array of cars are the documents which will be searched. Create a batch object and upload it. Upload the car array When I started with the Azure Search, I had problems finding results, although I uploaded my documents. Then I figured out that it is not working when you add the index and documents and search through it right away. You have to wait for a second to make it work. Therefore I added Thread.Sleep(1000). Search for a brand That’s all you have to do to set up the search. The next step is to search something. To do that, create a SearchParameters object containing all properties, which the search should return. Then execute the search with the string you are looking for and your SearchParameters object. Execute the search The search should return two objects. You can iterate through the results and, for example, print the type of the car. Iterate through the search result ConclusionThis post provided an overview of the Azure Search Service and how it can be implemented using C#.You can find the code of the demo on GitHub.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Implement Redis Cache in Azure", "url": "/implement-redis-cache-azure/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-05-01 20:10:55 +0200", "snippet": "Redis Cache is a key-value store NoSQL database. Its implementation is very similar to Azure Table Storage. The main difference is Redis is very high performing by keeping the data in memory most of the time. By default, Redis also doesn’t persist the data between reboots. The main purpose of keeping Redis Cache in memory is for fast data retrieval and aggregations. Redis is typically used to augment the data store you have already selected. It will make lookups much faster and also helps to reduce the cost of your storage since it reduces the number of queries.Redis has many uses, but primarily it is a temporary storage location of data that has a longer lifespan. That data needs to be expired when it is out of data and re-populated.Azure Redis Cache is the Azure product built around Redis and offering it as a Platform as a Service product.Choose a Redis Cache tierTo create a Redis Cache, follow these steps: In the Azure portal select +Create a resource, search for Redis Cache and click Create. On the New Redis Chache blade, provide a DNS name, subscription, resource group, location, and pricing tier. Click Create. Create a new Redis Cache There are three pricing tiers: Pricing tier Description Basic The Basic tier is the cheapest and allows up to 53 GB of Redis Cache database size. Standard The Standard tier has the same storage limit as Basic but includes replication and failover with master/slave replication. This replication is automatic between two nodes Premium The Premium tier allows a database size of 530 GB and also offers persistence, which means that the data will survive a power outage. It also includes a much better network performance, allowing up to 40,000 client connections. Implement data persistenceRedis persistence allows you to save data to a disk instead of just memory. Additionally, you can take snapshots of your data for backups. This allows your Redis Cache to survive hardware failure and power outages. Redis persistence is implemented through a relational database model, where data is streamed out to binary into Azure Storage blobs.To configure the frequency of the snapshots, follow these steps: In the Azure portal in your Redis Cache, click on Redis data persistence under the Settings menu. Note that you need a premium tier cache to do that. On the Redis data persistence blade, select the Backup Frequency and select a storage account. Click OK. Configure data persistence Implement security and network isolationThe primary security mechanism is done through access keys. The premium tier offers enhanced security features. This is done primarily through virtual networks (VNET) and allows you to hide your Redis Cache behind your application and not have a public URL that is open to the internetThe VNET is configured at the bottom of the New Redis Cache blade. You can’t configure it after it has been created. Additionally, you have to use an existing VNET which is in the same data center as your Redis Cache. The Redis Cache must be created in an empty subnet.Tune cluster performanceWith the premium tier, you can implement a Redis Cluster. This allows you to split the dataset among multiple notes, allowing you to continue operations when a subset of the nodes experiences failure, gives more throughput, and increases memory size as you increase the number of shards. Redis clustering is configured when you create the Azure Redis Cache.Once the cache is created, Redis distributes the data automatically.Integrate Redis caching with ASP.NET session and cache providersRedis Cache is an excellent place to store session data. To implement this, install the Microsoft.Web.RedisSessionStateProvider NuGet package. Once added to the project, add this line to your web.config file under the providers section:&lt;add name=”MySessionStateStore” type=”Microsoft.Web.Redis.RedisSessionStateProvider” host=”YourHostURL” accessKey=”YourAccessKey” ssl=”true” /&gt;Replace the host with the URL of your cache and the accessKey with your key. You can find the keys on the Access Keys blade and the URL on the Overview blade.ConclusionIn this short post, I showed how to create a Redis Cache and how to configure data persistence and security using the premium pricing tier.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Implement Azure Cosmos DB DocumentDB", "url": "/implement-azure-cosmos-db-documentdb/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-04-30 00:48:42 +0200", "snippet": "Azure Cosmos DB DocumentDB is a JSON document store NoSQL database, similar to MongoDB. JSON document stores are the fastest growing NoSQL solutions. The reason why it’s growing so fast is that it doesn’t require assembling or disassembling object hierarchies into a flat relational design. Since the release of Azure Cosmos DB, it has added support for key-value (Table API) and graph (Gremlin).One of the key advantages of JSON is that it can express an object model that developers often create in code. Object models have parent nodes and child node. When relational database developers create an API to store JSON, they have to undergo a process called shredding when they remove each individual element and store them in flat tables that have relationships with each other. This process is time-consuming, offers little business value and is prone to errors. Because of these drawbacks, developers often turn towards JSON document store where saving a document is as easy as pressing the save icon in an application.Choose the Azure Cosmos DB API surfaceAzure Cosmos DB is a multi-model database that has several different APIs. You can choose between Table, DocumentDB, and GraphDB.Azure Cosmos DB Table API provides the same functionality as Azure Storage tables. If you have an existing application that uses Azure Storage tables, you can easily migrate that application to use Azure Cosmos DB. This will allow you to take advantage of better performance, global data distribution, and automatic indexing of all fields, thus reducing significant management overhead of your existing Azure Storage table application.Azure Cosmos Document DB is an easy to implement JSON document storage API. It is an excellent choice for mobile application, web application, and IoT applications.Azure Cosmos DB allows for rapid software development by cutting down the code the developer has to write to either shred their object model into a relational store, or manage the consistency of manual indexing in Azure Storage Tables. It also is compatible with MongoDB. You can migrate an existing MongoDB application easily to Azure Cosmos DB DocumentDB.Azure Cosmos DB support Gremlin, a popular graph API, This allows developers to write applications that take advantage of Graph traversal of their structures. Graph databases allow you to define the relationship between entities that are stored. For example, you can declare that one entity likes another one and owns a different one. You could say that Wolfgang likes McDonald’s and owns an iPhone.  Graph databases excel at defining relationships and exploring the network of those relationships. As a result, they have been popular as engines for social media applications. Because Azure Cosmos DB support the Gremlin API, it is easy to port an existing application to use Azure Cosmos DB.Create Cosmos DB API Database and CollectionsEach Cosmos DB account must have at least one database. A database is a logical container that can contain collections of documents and users. Collections primarily contain JSON documents. Collections should store JSON documents of the same type and purpose, just like a SQL server table. The difference to tables is that collections don’t enforce that documents have a particular schema.A collection should have documents of the same properties and data types, but it is not required to. Azure Cosmos DB DocumentDB gracefully handles none existing columns on a document. For example, if you are looking for all customer with the country code CH, and a JSON document doesn’t have this property, Azure Cosmos DB just ignores the document and doesn’t return it.Collections can also store procedures, triggers, and functions. Triggers are application logic that is executed either before or after an insert, delete or update operation. Unlike SQL server, where these components are written in T-SQL, Azure DocumentDB stores procedures, triggers, and functions in JavaScript.Demo using C#The code for the following demo can be found on GitHub. To create an Azure Cosmos DB account, follow these steps: In the Azure portal, click Create a new Service and then search for Azure Cosmos DB. On the Azure Cosmos DB blade, provide a unique ID, subscription, resource group and location. Select an API, for example, SQL and click Create. Create an Azure Cosmos DB with a SQL API After Azure Cosmos DB is deployed, open Visual Studio 2015 or 2017 and create a new C# console application. Install the Microsoft.Azure.DocumentDB NuGet Package. Create a DocumentClient variable with your Azure Cosmos DB account URI and primary key. To find your URI and primary key go to your Azure Cosmos DB and select Keys under the Settings menu. I also created two constant variables for the database and the collection. Create variables for the database, collection, and client of your Azure Cosmos DB account If the database doesn&#8217;t exist yet, create it with the following code: Create the database if it does not exist yet Create the collection, if it does not exist yet. After the database is created, create the collection if it does not exist yet Add a new class, Customer, with following properties: The customer class will be used to add a customer to the database Add an item to your collection. Add an item to the collection Retrieve a list of customers with a LINQ expression. Get a list of customers which fit the LINQ expression Query documentsRetrieving documents from Azure Cosmos DB DocumentDB is where the magic really happens. With Cosmos DB, the wiring up of persistence store to the object model happens without any data layer code. The main way to retrieve data from Azure Cosmos DB is through LINQ.In the previous section, I showed how to query data using C#. You can also create queries in the Azure portal using SQL. To do that follow these steps: In the Azure portal on your Azure Cosmos DB account, select Data Explorer. On the Data Explorer blade select your collections and then click on New SQL Query. Enter your query, for example, SELECT * FROM c and click Execute Query. Create a query in the Azure portal using SQL Create a Graph API databaseThe code for the following demo can be found on GitHub. To create an Azure Cosmos DB  account, follow these steps: In the Azure portal, click Create a new Service and then search for Azure Cosmos DB. On the Azure Cosmos DB blade, provide a unique ID, subscription, resource group and location. Select Gremlin (graph) as API and click Create. Create an Azure Cosmos DB with a Graph API After Azure Cosmos DB is deployed, open Visual Studio 2015 or 2017 and create a new C# console application. Install the Microsoft.Azure.DocumentDB and Microsoft.Azure.Graphs NuGet Packages. Enter your URI and primary key and connect to Azure Cosmos DB. To find your URI and primary key go to your Azure Cosmos DB and select Keys under the Settings menu. Connect to your Azure Cosmos DB Next, create a database and a collection within the database. Create a database with a collection After the database and collection are created, add the nodes and edges to a dictionary. Additionally, I add a delete command to delete all entries before adding new ones. Add nodes and edges With all nodes and edges in the dictionary, you can query each entry to add them to the collection. Add nodes and edges to the collection Now, you can see the added nodes and edges as a graph in the Data Explorer of your Azure Cosmos DB. The added nodes and edges displayed as a graph Additionally, you can create queries in C#. For example, searching for all people Wolfgang knows. Search for all people Wolfgang knows Implement MongoDB databaseAzure Cosmos DB can be used with applications that were originally written with a MongoDB. Existing MongoDB drivers are compatible with Azure Cosmos DB. Ideally, you would switch from MongoDB to Azure Cosmos DB by just changing the connection string. Additionally, you can use existing MongoDB tooling with Azure Cosmos DB.Manage scaling of Cosmos DB, including managing partitioning, consistency, and Request Units (RUs)Collections are assigned a specific amount of storage space and transactional throughput. Transactional throughput is measured in Request Units (RUs). Collections are also used to store similar documents together. A company can choose to organize their documents into a collection in any manner that logically makes sense. A software company might create a single collection per customer. A different company may choose to put heavy load documents in their own collection so they can scale them separately from other collections.In “Implement Azure SQL database”, I already talked about sharding. Sharding is a feature of Azure Cosmos DB as well. You can shard automatically by using a partition key. Azure Cosmos DB will automatically create multiple partitions for you, Partitioning is completely transparent to your application. All documents with the same partition key value will always be stored on the same partition. Azure Cosmos DB may store different partition keys on the same partition or it may not. The provisioned throughput of a collection is distributed evenly among the partitions within a collection.Partitioning is always done at the collection, not at the Azure Cosmos DB account level. You can have a collection that is a single partition alongside multiple partition collections. Single partition collections have a 10 GB storage limit and can have up to 10,000 RUs.  When you create them, you do not have to specify a partition key.Create a single partition collectionTo create a single partition collection, follow these steps: In the Azure portal on your Azure Cosmos DB account(SQL API), click +Add Collection on the Overview blade. On the Add Collection blade, provide a name for the database and collection, set the Storage capacity and the throughput, and leave the partition key empty. Add a collection to your Azure Cosmos DB For multiple partition collections, it is important that you choose the right partition key. A good partition key will have a high number of distinct values without being unique to each individual document. Partitioning based on geographic location, a large data range, department, or customer type is a good idea. The storage size for documents with the same partition key is 10 GB. The partition key should also be in your filters frequently.A partition key is also a transaction boundary for stored procedures. Choose a key on documents that often get updated together with the same partition key value.ConsistencyRelational databases have a little bit of baggage as it relates to data consistency. Users of those systems have the expectation that when they write data, already of that data will see the latest version of it. That strong consistency level is great for data integrity and notifying users when data changes, but creates problems with concurrency. Writers have to lock data as they write, blocking readers of the data until the write is over. This creates a line reader waiting to read until the write is over. Having writes block readers gives the readers the impression that the application is slow.This has particularly created issues when scaling out relation databases. If a write occurs on the partition and it hasn’t replicated to another partition, readers are frustrated that they are seeing bad or outdated data. It is important to understand that consistency has long had an inverse relationship with concurrency.Many JSON document storage products have solved that trade off by having a tunable consistency model. This allows the developer to choose between strong and eventual consistency. Strong consistency slows down reads and writes while giving the best data consistency between users. Eventual consistency allows the readers to read data while writes happen on a different replica, but isn’t guaranteed to return current data. Things are faster because replicas don’t wait to get the latest updates from a different replica.The five levels of consistencyThere are five consistency levels in the Cosmos DocumentDB: Consistency level Description Strong Strong consistency slows down reads and writes while giving the best data consistency between users. Bounded Staleness Bounded staleness consistency tolerates inconsistent query results, but with a freshness guarantee that the results are at least as current as a specified period of time. Session Session consistency is the default in DocumentDB. Writers are guaranteed strong consistency on writers that they have written. Readers and other writer sessions are eventually consistent. Consistent Prefix Consistent prefix consistency guarantees that readers do not see out of order writes. Meaning the writes may not have arrived yet, but when they do, they’ll be in the correct order. Eventual Eventual consistency allows the readers to read data while writes happen on a different replica, but isn&#8217;t guaranteed to return current data. Things are faster because replicas don&#8217;t wait to get the latest updates from a different replica. Manage multiple regionsIt is possible to distribute data in Azure Cosmos DB globally. The main reason to do that is to get the data closer to the users, therefore, they have a lower latency. A downside is that each replica will add to your Azure Cosmos DB costs.In a single geo-location Azure Cosmos DB collection, you can’t really see the difference in consistency choices from the previous section. Data replicates so fast that the user always sees the latest copy of the data (with few exceptions). When replicating data around the globe, choosing the correct consistency level becomes more important.To distribute data globally, follow these steps: In the Azure portal, on your Azure Cosmos DB account, click on Replicate data globally under the Settings menu. On the Replicate data globally blade, select the regions to add by clicking on the map. These new regions are read regions. Read regions often outnumber write regions, which can drastically improve the performance of your application. Enable geo-configuration for your Azure Cosmos DB Click Save. After the new regions are set up, click on Automatic Failover on the top of the Replicate data globally blade. On the Automatic Failover blade, move the slider to on and optionally change the priority of the read regions by dragging them up or down. Click OK. If your write region is unavailable, Azure will automatically switch the write region to your highest read region. You can also use C# code to set the preferred region. To do that use this code: Set the preferred region For more information, see the documentation.Implement stored proceduresAs previously mentioned, like relational databases, Azure Cosmos DB collections can have stored procedures, triggers, and user-defined functions. The difference is that in Azure Cosmos DB, they are written in JavaScript. Batch operations executed on the server will avoid network latency and will be fully atomic across multiple documents in that collection’s partition. Operations in a stored procedure either all succeed or all fail.Access Azure Cosmos DB from REST interfacesAzure Cosmos DB has a REST API that provides a programmatic interface to create, query, and delete databases, collections, and documents. So far, I only used the Azure Document DB SDK in C#, but it is possible to call the REST URIs directly without the SDK. The SDK makes these calls simpler and easier to implement. SDKs are available for Python, JavaScript, Node.js, and Xamarin. These SDKs all call the REST API underneath. Using the REST API allows you to use a language that might not have an SDK, like Elixir.The REST API allows you to send HTTPS requests using GET, POST, PUT, and DELETE to a specific endpoint.Manage Azure Cosmos DB securitySecurity is probably the biggest concern when using the cloud. Microsoft invests therefore heavily in its security. In this section, I will talk about the different security features of the Azure Cosmos DB.Encryption at restEncryption at rest means that all physical files used to implement Cosmos DB are encrypted on the hard drives they are using. Anyone with direct access to those files would have to unencrypt them in order to read the data. This also applies to all backups of Azure Cosmos DB databases. There is no need for configuration of this option.Encryption in flightEncryption in flight is required when using Azure Cosmos DB. All REST URI calls are done over HTTPS. This means that anyone sniffing a network will only see encryption round trips and not clear text data.Network firewallAzure Cosmos DB implements an inbound firewall. This firewall is off by default and needs to be enabled. You can provide a list of IP addresses that are authorized to use Azure Cosmos DB. You can specify the UP addresses one at a time or in a range. This ensures that only an approved set of machines can access Azure Cosmos DB. These machines will still need to provide the right access key in order to gain access.The enable the firewall and configure IP addresses, follow these steps: In the Azure portal, go to your Azure Cosmos DB account and click on Firewall under the Settings blade. Select the Selected networks checkbox and enter a single IP address or a range. Click Save. Add IP addresses to your firewall Users and permissionsAzure Cosmos DB support giving access to users in the database to a specific resource or using Active Directory users. The Role Based Access Control supports different roles like Owner, Contributor or Reader. Based on the role, a user has different rights. For example, a user with read rights can only access the resource for reading whereas the Owner can do everything.Active DirectoryFollow these steps to use the Active Directory to give an user access to the Azure Cosmos DB: In the Azure portal, go to your Azure Cosmos DB account and click on Access control (IAM). On the Access control (IAM) blade, click on +Add. On the Add permissions blade, select a role and a user. You can search for users by name or email address. Click Save. Grant a user the DocumentDB Account Contributor access ConclusionIn this post, I talked about the different types of the Azure Cosmos DB. First, I showed how to create a database, a collection and query data using C# for the SQL API. Then, I showed the same for the Graph API using C# and also the Azure portal. Next, I explained what Request Units are and how to scale your database by increasing them. Another way to increase the performance is to use sharding which splits up your database into several parts. Using multiple regions for your Azure Cosmos DB can increase the performance and also decrease the latency for users around the globe.In the last section, I explained some security features of Azure Cosmos DB. Files can be encrypted at rest and also during the transfer. Additionally should be the firewall activated to restrict the access. Azure supports different roles with different access rights which can be configured with the Active Directory.You can find the demo for the Azure Cosmos DB (SQL API) here and the demo for the Azure Cosmos DB (Graph API) here.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Implement Azure SQL database", "url": "/implement-azure-sql-database/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-04-26 21:52:05 +0200", "snippet": "Microsoft Azure offers with Azure SQL database a great alternative to an on-premise SQL database. In this post, I will talk about the advantages of having the database in the cloud, how to get data into the cloud and how to use Elastic pools to share resources between several databases. The last section will be about implementing graph functionality in an Azure SQL database.Create an Azure SQL databaseTo create an Azure SQL database in the Azure portal, follow these steps: In the Azure portal go to SQL databases and click +Add. On the SQL Database blade provide: Name Subscription Resource group Source (Blank database, Demo database or a backup) Server Pricing tier After you entered all information, click on Create. Create a new Azure SQL database Choosing the appropriate database tier and performance levelAzure offers three different pricing tiers to choose from. The major difference between them is in a measurement called database throughput units (DTUs). A DTU is a blended measure of CPU, memory, disk reads, and disk writes. SQL database is a shared resource with other Azure customers, sometimes performance is not stable or predictable. As you go up in performance tiers, you also get better predictability in performance.The three pricing tiers are: Pricing tier Description Basic The Basic tier is meant for light workloads. I use this tier for testing and at the beginning of a new project. Standard The Standard tier is used for most production online transaction processing (OLTP) databases. The performance is more predictable than the basic tier. In addition, there are 13 performance levels under this tier, levels S0 to S12. Premium The Premium tier continues to scale at the same level as the Standard tier. In addition, performance is typically measured in seconds. For instance, the basic tier can handle 16,600 transactions per hour. The standard S2 level can handle 2,570 transactions per minute. The top tier premium of premium can handle 75,000 transactions per second. Each tier has a 99,99 percent up-time SLA, backup and restore capabilities, access to the same tooling, and the same database engine features.Analyzing metricsTo review the metrics of your Azure SQL database, follow these steps: In the Azure portal go to your Azure SQL database. On the Overview blade, click on the Resource graph. This opens the Metrics blade. There select the desired metrics, you wish to analyze. Configure the metrics graph Note that there is nothing on the graph because I just created the database and haven’t used it yet.Configure and performing point in time recoveryAzure SQL database does a full backup every week, a differential backup each day, and an incremental log backup every five minutes. The incremental log backup allows for a point in time restore, which means the database can be restored to any specific time of the day. This means that if you accidentally delete a customer’s table from your database, you will be able to recover it with minimal data loss if you know the time frame to restore from that has the most recent copy.The further away you get from the last differential backup determines the longer the restore operation takes. When you restore a new database, the service tier stays the same, but the performance level changes to the minimum of that tier.The retention period of the backup depends on the pricing tier. Basic retains backups for 7 days, Standard and Premium for 35 days. A deleted database can be restored, as long as you are in within the retention period.Restore an Azure SQL databaseTo restore an Azure SQL database, follow these steps: In the Azure portal, on the Overview blade of your database, click on Restore. On the Restore blade, select a restore point, either Point-in-time or Long-term backup retention. If you selected Point-in-time, select a date and time. If you selected Long-term backup retention, select the backup from the drop-down list. Optionally change the Pricing tier. Click on OK to restore the backup. Restore an Azure SQL database Restore a deleted Azure SQL databaseTo restore a deleted Azure SQL database, follow these steps: In the Azure portal, go to your SQL server and select the Deleted databases blade under the Setting menu. On the Deleted databases blade, select the database, you want to restore. Select a deleted Azure SQL database to restore On the  Restore blade, change the database name if desired and click OK to restore the database. Restore the deleted Azure SQL database Enable geo-replicationBy default, every Azure SQL database is copied three times across the datacenter. Additionally, you can configure geo-replication. The advantages of geo-replication are: You can fail over to a different data center in the event of a natural disaster or other intentionally malicious act. Online secondary databases are readable, and they can be used as load balancers for read-only workloads such as reporting. With automatic asynchronous replication, after an online secondary database has been seeded, updates to the primary database are automatically copied to the secondary database.Create an online secondary databaseTo enable geo-replication, follow these steps: Go to your Azure SQL database in the Azure portal and click on Geo-Replication under the Settings menu. Select the target region. On the Create secondary blade, enter the server and pricing information. Click on OK. Create an online secondary database Import and export schema and dataTo export the metadata and state data of a SQL server database, you can create a BACPAC file.Export a BACPAC file from a SQL databaseTo create a BACPAC file, follow these steps: Open SQL Server Management Studio and connect to your database. Right-click on the database, select Tasks and click on Export Data-tier Application. Export your source database In the Export Data-tier Application wizard, you can export the BACPAC file to Azure into a blob storage or to a local storage device. Select a location for the export file After you selected the export destination, click Next and then Finish to export the file. Export successful Import a BACPAC file into Azure SQL databaseTo import a BACPAC file into your Azure SQL database, follow these steps: Open SQL Server Management Studio and connect to your Azure SQL server. You can find the server name in the Azure Portal on your SQL server. Go to Properties under the Settings menu. If you can’t connect to your server, you may have to allow your UP address in the firewall. Select Firewalls and virtual network under the Settings menu and enter your IP address or a range of IPs. Configure the firewall of your SQL server After you are connected to the server, right-click on the database folder and select Import Data-tier Application. Start the Import Data-tier Application wizard In the Import Data-tier Application wizard, select the BACPAC file from a local disk or from your Azure storage account. Select your BACPAC file for the import On the Database Settings blade, enter a database name and the pricing tier. Enter a database name and select the pricing tier Click Next and then Finish. The summary of the import wizard After the import process is finished, you can see the database in your SQL server in the Azure portal by selecting the SQL databases blade under the Settings menu. The database got created by the imported schema Scale Azure SQL databasesYou can scale-up and scale-out your Azure SQL databases.Scaling-up means to add CPU, memory, and better disk i/o to handle the load. To do that click in the Azure portal on your database on Database size under the monitoring menu and move the slider to the right, or select a higher pricing tier. Scale-up your Azure SQL database Scaling-up will give you more DTUs.Scaling-out means breaking apart a database into smaller pieces. This is called sharding. Methods for sharding can be, for example, by function, by geo-location or by business unit.Another reason for sharding can be that the database is too large to be stored in a single Azure SQL database or that taking a backup takes too long due to the size of the database.To increase the performance, a shard map is used. This is usually a table or database that tells the application where data actually is and where to look for it. A shard map also keeps you from rewriting a big part of your application to handle sharding.Sharding is easily implemented in Azure Table Storage and Azure Cosmos DB but is way more difficult in a relational database. The complexity comes from being transactionally consistent, while having data available and spread throughout several databases.To help developers, Microsoft released a set of tools called Elastic Database Tools that are compatible with Azure SQL database.  This client library can be used in your application to create sharded databases. It has a split-merge tool that will allow you to create new nodes or drop nodes without data loss. It also includes a tool that will keep schema consistent across all the nodes by running scripts on each node individually.Managed elastic pools, including DTUs and eDTUsA single SQL database server can have several databases on it. Those databases can each have their own size and pricing tier. This might work out well if you always know exactly how large each database will and how many DTUs are needed for each one. What happens if you don’t really know that? What if you want all your databases on one server to share their DTUs? The solution for this are Elastic pools.Elastic pools enable you to purchase elastic Database Transaction Units (eDTUs) for a pool of multiple databases. The user adds databases to the pool, sets the minimum and maximum eDTUs for each database, and sets the eDTU limit of the pool based on your budget. This means that within the pool, each database is given the ability to auto-scale in a set range.Create an Elastic poolTo create an Elastic pool, follow these steps: Go to your SQL server in the Azure portal and click on +New pool on the Overview blade. On the Elastic database pool blade, provide a name and select a pricing tier under the Configure pool setting. On the Resource Configuration &amp; Pricing blade, click on Databases and then +Add databases to add your databases. Create a new Elastic pool Click OK to create your Elastic pool. Implement Azure SQL Data SyncSQL Data Sync allows you to bi-directionally replicate data between two Azure SQL databases or between an Azure SQL database and an on-premise SQL server.Azure SQL Data Sync has the following attributes: Attribute Description Sync Group A Sync Group is a group of databases that you want to synchronize using Azure SQL Data Sync. Sync Schema A Sync Schema is the data you want to synchronize. Sync Direction The Sync Direction allows you to synchronize data in either one direction or bi-directionally. Sync Interval Sync Interval controls how often synchronization occurs. Conflict Resolution Policy A Conflict Resolution Policy determines who wins if data conflicts with one another. The following screenshot shows how a data sync infrastructure could look like. Azure Data Sync diagram (Source) The hub database must always be an Azure SQL database. A member database can either be an Azure SQL database or an on-premise SQL server.It is important to note that this is a method of keeping data consistent across multiple databases, it is not an ETL (Extract, Transform and Load) tool. This should not be used to populate a data warehouse or to migrate an on-premise SQL server to the cloud. This can be used to populate a read-only version of the database for reporting, but only if the schema is 100% consistent.Implement graph database functionality in Azure SQL databaseA graph database is a NoSQL solution and introduces two new vocabulary words: nodes and relationships.Nodes are entities in relational database terms and a relationship shows that a connection between nodes exists. The relationship in a graph database is hierarchical, where it is flat in a relational database.A graph is an abstract representation of a set of objects where nodes are linked with relationships in a hierarchy. A graph database is a database with an explicit and enforceable graph structure. Another key difference between a relational database and a graph database is that as the number of nodes increase, the performance cost stays the same. Joining tables will burden the relational database and is a common source of performance issues when scaling. Graph databases don’t suffer from that issue.Relational databases are optimized for aggregation, whereas graph databases are optimized for having plenty of connections between nodes.In Azure SQL database, graph-like capabilities are implemented through T-SQL. You can create graph objects in T-SQL with the following syntax:Create Table Person(ID Integer Primary Key, Name Varchar(100)) As Node; Create Table kids (Birthdate date) As Edge;A query can look like this:SELECT Restaurant.name FROM Person, likes, Restaurant WHERE MATCH (Person-(likes)-&gt;Restaurant) AND Person.name = ‘Wolfgang’;This query will give you every restaurant name which is liked by a person named Wolfgang.ConclusionIn this post, I showed how to create an Azure SQL database and how to export the schema and metadata from your on-premise database to move it into the cloud. After creating the database in the cloud, I talked about restoring a backup and enabling geo-replication to increase the data security. Next, I talked about leveraging Elastic pools to dynamically share the resources between several databases and by doing so keeping your costs low. The last section talked about implementing graph database functionality with your Azure SQL database.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Manage access and monitor storage", "url": "/manage-access-and-monitor-storage/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-04-18 09:54:09 +0200", "snippet": "Azure Storage has a built-in analytics feature called Azure Storage Analytics used for collecting metrics and logging storage request activity. Storage Analytics Metrics are used to collect aggregate transaction and capacity. Storage Analytics Logging is used to capture successful and failed request attempts to your storage account. In this post, I will talk about different methods to manage the access and monitor storage accounts.Generate Shared Access SignaturesBy default, storage resources are protected at the service level. Only authenticated callers can access tables and queues. Blob containers and blobs can optionally be exposed for anonymous access, but you would typically allow anonymous access only to individual blobs. To authenticate to a storage service, a primary or secondary key is used, but this grants the caller access to all actions on the storage account.A shared access signature (SAS) is used to delegate access to specific account resources without enabling access to the entire account. An SAS token lets you control the lifetime by setting the start and expiration time of the signature, the resources you are granting access to, and the permissions being granted.The following operations are supported by SAS: Reading or writing blobs, blob properties, and blob metadata Deleting blobs Listing blobs in a container Adding, updating, or deleting table entities Querying tables Adding and updating queue messages Processing queue messagesThe SAS token should be stored in the Azure Key Vault.Creating an SAS token with C#You can find the following code demo on GitHub. To learn how to create a container, queue and table see Implement Azure Storage blobs and Azure files and Implement Azure Storage Tables, Queues, and Azure Cosmos DB Table API.The following code creates an SAS token for a blob container with a start time and expiration time. Before you start, install the WindowsAzure.Storage NuGet package.Add the following code to your App.config file and replace the placeholder with your account name and Add the connection string to your storage account to the App.config file Use the following code to create an SAS token for your blob with read, write, delete and list rights. The token will expire in one hour. Create an SAS token for a blob With the SAS token, you can access your container. Access the blob container with the SAS token Use the following code to create an SAS token for your queue with read, add, update and processMessages rights. The token will expire in one hour. Create an SAS token for a queue With the following code, you can access your queue and add a new message. Access the queue with the SAS token and add a new message Use the following code to create an SAS token for your table with query, add, update and delete rights. The token will expire in one hour. Create an SAS token for a table With the SAS token, you can access your table. Access the table with the SAS token Creating an SAS token in the Azure PortalAdditionally, to creating the SAS token in your code, you can create it in the Azure Portal. To do that follow these steps: In your storage account select the Shared access signature blade under the Settings menu. Select the following attributes: allowed services, resource types, and permissions start and expiry date and time optionally the allowed IP addresses allowed protocols signing key After you set your desired attributes, click on Generate SAS and connection string. Below the button can you find the SAS token and the URLs to your resources. Create an SAS token in the Azure Portal Renewing an SAS tokenYou can extend access to the same application or user by using new SAS tokens on requests. This should be done with appropriate authentication and authorization in place.Validating dataWhen you give write access to storage resources with SAS, the contents of those resources can be made corrupt or be tampered with by a malicious party. Be sure to validate system use of all resources exposed with SAS keys.Create stored access policiesStored access policies provide greater control over how you grant access to storage resources using SAS tokens. With a stored access policy, you can do the following after releasing an SAS token for resource access: Control permissions for the signature Change the start and end time for a signature’s validity Revoke accessThe stored access policy can be sued to control all issued SAS tokens that are based on the policy. It is best practice to use stored access policies wherever possible, or at least limit the lifetime of SAS tokens to avoid malicious use.Create and test stored access policies programmaticallyYou can create SAS tokens and stored access policies programmatically with C#. I added two C# console applications to my GitHub account. The first application sets everything up and the test client tests the access with the SAS tokens.Note that the test client won’t be able to delete blobs or container because stored access policy has only read and write rights.Before you can start, you have to replace YourAccountName and YourAccessKey in the app.config with the name and access key of your storage account. Then you can run the first application and copy the output for the variables needed in the test client. Then you can run the test client and see the result.You can find the application to create the stored access policies here and the test client here.Regenerate account keysWhen you create a storage account, two 512 bit storage access keys are generated which are used for authentication to the storage account. Since there are two keys, it is possible to regenerate them without impacting the access to storage of your applications.To regenerate your keys without interrupting the access of your applications, follow this strategy: Change your application configurations to use the secondary key. Regenerate the first key. Change your application configurations to use the primary key. Regenerate the second key.To regenerate your storage account keys, follow these steps: In the Azure Portal, select your storage account and click on Access keys under the Settings menu. Click on the Regenerate button of the key you want to regenerate. Click on Yes in the confirmation dialog. Your key is regenerated. Regenerate the primary key The keys should be stored in the Azure Key Vault.Configure and use Cross-Origin Resource SharingCross-Origin Resource Sharing (CORS) enables web applications running in the browser to call web APIs that are hosted by a different domain. By default CORS is disabled, but you can enable it for a specific storage service. To enable CORS for blobs, follow these steps: In the Azure Portal, select your storage account and click on CORS under the Blob Service menu. (There is a CORS blade for every resource type. If you want to add a CORS rule for your queue, select the CORS blade under the Queue Service menu) Click on +Add at the top of the blade. On the Add CORS rule blade, enter your configuration Add a new CORS rule to your blob On the screenshot above, you can see the allow origins are allowed by using *, only HTTP GET requests are allowed, allowed headers and exposed headers are all allowed and no maximum age for the request is defined. After you entered your settings, click on Add. Configure and Monitor metricsStorage Analytics metrics provide insight into transactions and capacity for your storage accounts. By default, the storage metrics are not enabled. You can enable them using the Azure Portal or PowerShell.When you configure storage metrics for a storage account, tables are generated to store the output of the metrics collection. You set the level of metrics collection for transactions and the retention level for each service (blob, table, queue). Additionally, you can set the interval for the metric collection (hourly or by minute).There are two levels of metrics collection: Metric collection level Description Service level The metrics include aggregate statistics for all requests, aggregated at the specified interval. If no requests are made, an aggregate entry is still created for the interval, indicating no requests for that period. API level These metrics record every request to each service and only if a request is made within the interval. By default, Storage Analytics will not delete any metrics data. When the shared 20 TB limit is reached, no new data can be written until space is freed. You can specify a retention period from 0 to 365 days. Metrics data is automatically deleted when the retention period is reached for the entry.Configure storage metrics and retentionTo configure storage metrics and retention for Blob, Table, and Queue services, follow these steps: In the Azure portal, go to your storage account and select Diagnostics under the Monitoring menu. On the Diagnostics blade, click On under the Status property to enable the options for metrics and logging. You can set different settings for each storage type by switching between the tabs beneath the Status slider. Set your desired settings and set a value for retention by moving the slider or entering a number between 0 and 365. Click Save. Configure your storage metrics and retention Analyze storage metricsStorage Analytics metrics are collected. You can access the tables directly in the Azure portal to analyze and review the metrics. In the next sections, I will talk about different ways to access, review and analyze these metrics.Monitor storage metricsThe available metrics in the Azure portal include total requests, total egress, average latency, and availability.To monitor metrics, follow these steps: In the Azure portal, go to your storage account and select Metrics under the Blob, File, Table or Queue Service menu. On the metrics blade, you can see graphs for the previously mentioned metrics. Metric graphs Click on a graph to see additional details and to modify it. View the details of a metric and modify the graph Configure Storage Analytics LoggingStorage Analytics Logging provides details about successful and failed requests to each storage service. By default, storage logging is not enabled, but you can enable it using the management portal, PowerShell or by calling the management API directly.When you configure Storage Analytics Logging for a storage account, a blob named $logs is automatically created to store the output of the logs. You can log any or all of the Blob, Table, or Queue service logs are created only for those services that have activity, so you will not be charged if you enable logging for a service that has no request. The logs are stored as block blobs as requests are logged and are periodically committed so that they are available as blobs.After you enabled Storage Analytics, the log container cannot be deleted. However, the contents of the log container can be deleted.Duplicate log entries may be present within the same hour. You can use the RequestId and operation number to uniquely identify an entry to filter duplicates.Analyze logsLogs are stored as block blobs in delimited text format. You can download logs for review and analysis using any tool compatible with that format. Within the logs, you will find entries for authenticated and anonymous requests.Logs include status messages and operation logs.Finding your logsWhen storage logging is configured, log data is saved to blobs in the $logs container created for your storage account. You can’t see this container by listing containers, but you can navigate directly to the container to access, view, or download the logs.To navigate to the $logs container, use a link following this convention: https://.blob.core.windows.net/$logsView logs with ExcelTo view logs in Excel, follow these steps: Open Excel, and on the Data menu, click From Text. Find the log file and click Import. During the import, select Delimited format, Semicolon as the only delimiter, and Double-Quote(“) as the text qualifier.After you loaded your logs into Excel, you can analyze and gather information such as: Number of requests from a specific IP range Which tables or containers are being accessed and the frequency of those requests Slow requests How many times a particular blob is being accessed with an SAS URL Details to assist in investigating network errorsConclusionIn this post, I talked about creating an SAS token to give access with specific permissions to users and how stored access policies can be created with these tokens. Then, I explained how to regenerate the keys of your storage account and how to enable CORS.In the second part, I talked about how to enable monitoring and logging for your storage account and how to view and analyze these logs in the Azure portal and with Excel.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Implement Azure Storage Tables, Queues, and Azure Cosmos DB Table API", "url": "/implement-azure-storage-tables-queues-and-azure-cosmos-db-table-api/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-04-10 12:40:08 +0200", "snippet": "Azure Storage Tables can store tabular data at petabyte scale. Azure Queue storage is used to provide messaging between application components, as they can be de-coupled and scaled individually.Azure Storage TablesAzure Tables are a key-value database solution with rows and columns. Tables store data as a collection of entities where each entity has a property. Azure Tables can have up to 255 properties (columns in relational databases). The maximum entity size (row size in a relational database) is 1 MB.Azure Tables organize data based on table name. For example, all customers should be stored in the Customers table whereas all products should be stored in the Products table.The tables store the entities based on a partition key and a row key. All entities stored with the same partition key property are grouped into the same partition and are served by the same partition server. The developer has to choose a good partition key. Having a few partitions will improve scalability, as it will increase the number of partition servers handling a request.Azure Storage Tables vs. Azure SQL DatabaseMicrosoft Azure Tables does not enforce any schema for tables. It is the developer’s responsibility to enforce the schema on the client side. Azure Tables do not have stored procedures, triggers, indexes, constraints, default values and many more SQL Database features. The big advantage of Azure Tables is that you are not charged for compute resources for inserting, updating or retrieving data. You are only charged for the total storage you use. which makes it extremely affordable for large datasets. Azure Tables also scale very well without sacrificing performanceUsing basic CRUD operationsThis section will explain how to access table storage programmatically using C#. You can find the code for the following demo on GitHub.Creating a table Create a new C# console application. Add the following code to your app.config file: Setup the connection string to your storage account Replace the placeholder with your storage account name and storage account key Install the WindowsAzure.Storage NuGet package. In the Main method, retrieve the connection string: Retrieve the connection string With the following code can you create a table if it doesn&#8217;t exist: Create the orders table if it does not exist Inserting recordsThe following useful properties for adding entries to a table are provided by the Storage Client Library: Property Description Partition Key The partition key is used to partition data across storage infrastructure Row Key: The row key is a unique identifier in a partition Timestamp Contains the time of the last update ETag The ETag is used internally to provide optimistic concurrency The combination of partition key and row key must be unique within the table. This combination is used for load balancing and scaling, as well as for querying and sorting entities.To insert data into the previously created table, follow these steps: Add the following class: The OrderEntity class will be used to add orders into the table To add an order to the orders table, use the following code: Insert a single record into the table Insert multiple records in a transactionYou can group inserts and other operations into a single batch transaction. You can have up to 100 entities in a batch but the batch size can’t be greater than 4 MB.To insert multiple orders in one transaction, add this code to your application: Insert multiple records into the table Getting records in a partitionYou can select all entities in a partition or a range of entities by partition and row key. Wherever possible, you should try to query with the partition key and row key. Querying entities by other properties does not work well because it launches a scan of the entire table.Within a table, entities are ordered within the partition key. Within a partition, entities are ordered by the row key. The RowKey property is a string, therefore sorting is handled as a string sort. If you are using a date value for your row key, as I did in the previous example, use the order year month day, for example, 20181220.The following code gets all records within a partition using the PartitionKey property: Retrieve all records of the table with the partition key Wolfgang Updating recordsTo update records, you can use the InsertOrReplace() method. It creates a records if it does not exist and updates an existing one, based on the partition key and row key. Use the following code to do that: Update a record in the table Deleting a recordTo delete a record, you have to retrieve it first and then call the Delete() method. Use the following code to do that: Delete a record from the table Designing, managing, and scaling table partitionsAs previously mentioned, tables are partitioned to allow massive scaling. The partition key is the unit of scale for storage tables. The table services will spread your table to multiple servers and key all rows with the same partition key co-located. Therefore, the partition key is an important grouping for querying and availability.There are three types of partition keys: Partition Key Description Single Value There is one partition key for the the entire table. This favors a small number of entities. It also makes batch transactions easier since batch transactions need to share a partition key. It does not scale well for large tables since all rows will be on the same partition server Multiple Values This might place each partition on its own partition server. If the partition size is smaller, it is easier for Azure to load balance partitions. Partitions might get slower as the number of entities increases. This might make further partitioning necessary at some point. Unique values This is for many small partitions. This is highly scalable, but batch transactions are not possible. For query performance, you should use the partition key and row key always together, if possible. This returns an exact row match. The next best thing is to have an exact partition match with a row range. It is best to avoid scanning the entire table.Azure Storage QueuesAzure Storage Queue provides a mechanism for reliable inter-application-messaging to support asynchronous distributed application workflows.Queues are often used in an ordering or booking application. The customer orders or books something, for example, a plane ticket and gets an email confirmation a couple minutes later. The order gets put into a queue at the end of the ordering process and then gets processed by a separate application. This takes sometimes more, sometimes less time, depending on the workload. After the order is processed, the customer gets an email confirmation.Adding messages to a queueYou can find the following demo on GitHub. Create a new C# console application. Add the following code to your app.config file: Setup the connection string to your storage account Replace the placeholder with your storage account name and storage account key. Install the WindowsAzure.Storage NuGet package. In the Main method, retrieve the connection string: Retrieve the connection string You can create a queue with the following code: Create the queue queue if it does not exist After the queue is created, add messages to it: Add three messages to the queue In the Azure Portal, open your storage account and select Queues on the Overview blade. There, you can see your newly created queue and the three added messages. On the screenshot above, you can see that every message in the queue got a unique ID assigned. This ID is used by the Storage Client Library to identify a message.The maximum size for a message in a queue is 64 KB, but it is best practice to keep the messages as small as possible and to store any required data for processing in a durable store, such as Azure SQL or Azure Storage tables. The Azure Storage Queue can store messages up to seven days.Processing messagesMessages are usually published by a different application from the application that listens for new messages to process them. For simplicity, I use the same application for publishing and listening in this demo. To de-queue a message use the following code: Dequeue a single message from the queue which is not older than five minutes The code returns the oldest message, which is not older than five minutes.Retrieving a batch of messagesA queue listener can be implemented as single-threaded (processing one message at a time) or multi-threaded (processing messages in a batch on separate threads). You can retrieve up to 32 messages from a queue using GetMessages() to process multiple messages in parallel. To get more than one message with GetMessages(), specify the number of items which should be de-queued: Dequeue multiple messages from the queue Consider the overhead of message processing before deciding the appropriate number of messages to process in parallel. If significant memory, disk space, or other network resources are used during processing, throttling parallel processing to an acceptable number will be necessary to avoid performance degradation on the compute instance.Scaling queuesEach individual queue has a target of approximately 20,000 messages per second. You can partition your application to use multiple queues to increase this throughput.It is more cost-effective and efficient to pull multiple messages from the queue for processing in parallel on a single compute node. However, this depends on the type of processing and resources required. Scaling out compute nodes to increase processing throughput is usually also required.You can configure VMs or cloud services to auto-scale by queue. You can specify the average number of messages to be processed per instance, and the auto-scale algorithm will queue to run scale actions to increase or decrease available instances accordingly.Choose between Azure Storage Tables and Azure Cosmos DB Table APIAzure Cosmos DB is a cloud-hosted, NoSQL database. A NoSQL database can be key/value stores, table stores, and graph stores. Azure Cosmos DB Table API is a key value store that is very similar to Azure Storage Tables.The main differences are: Azure Table Storage only supports a single region with one optional secondary for high availability. Cosmos DB supports over 30 regions. Azure Table Storage only indexes the partition and the row key. Cosmos DB automatically indexes all properties. Azure Cosmos DB is much faster, with latency lower than 10 ms on reads and 15 ms on writes at any scale. Azure Table Storage only supports strong or eventual consistency. Stronger consistency means less overall throughput and concurrent performance while having more up to date data. Eventual consistency allows for high concurrent throughput but you might see older data. Azure Cosmos DB supports five different consistency models and allows those models to be specified at the session level. This means that on user or feature might have a different consistency level than a different user or feature. Azure Table Storage only charges for the storage you use. This makes it very affordable. Azure Cosmos DB on the other side charges for Request Units (RU) which really is a way for a PaaS product to charge for computer fees. If you need more RUs, you can scale them up. This makes Cosmos DB significantly more expensive than Azure Storage Tables.ConclusionIn this post, I talked about how to use Azure Storage Tables and Azure Storage Queues. I showed how to programmatically access both and how to add, retrieve, and remove data. Then, I talked about how to scale Azure Storage Tables and Azure Storage Queues. The last section was a comparison between Azure Storage Tables and Azure Cosmos DB.You can find the demo code for the Azure Storage Table here and the demo code for the Azure Storage Queues here.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Implement Azure Storage blobs and Azure files", "url": "/implement-azure-storage-blobs-and-azure-files/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-04-08 11:17:44 +0200", "snippet": "Azure provides several methods of storing files, including Azure Storage blobs and Azure Files. In this post, I want to talk about the differences between them and how to use them.Azure Storage blobsAzure Storage blobs should be used when you have files that you are storing using a custom application. Microsoft provides client libraries and REST interfaces for the Azure Storage blobs with which you can store and access data at a massive scale in block blobsCreate a blob storage accountTo create a blob storage account, follow these steps: In the Azure Portal, click on +Create a resource then on Storage and then select Storage account – blob, file, table, queue. On the Create storage account blade provide a name, location, subscription, and resource group. Optionally you can choose between standard (HDD) and premium (SSD) performance and enforce HTTPS by moving the slider to Enabled under the Secure transfer required attribute. Click Create. Create a new Azure Storage account Your data is always replicated three times within the same data center to ensure data security.Read and change dataThe code of the following demo can be downloaded from GitHub You can read and change data by using the Azure SDK for .NET, following these steps: Make sure you have the Azure SDK installed. Create a new C# console application and install the WindowsAzure.Storage NuGet Package. Connect to your storage account in your application using the following code: Connect to your storage account Replace the placeholder for storage account name and storage key with your own. You can find them in the Azure Portal under Access keys in your storage account. Finding the Storage account name, storage key, and connection string Create a container with the following code Create the container myblockcontainer if it does not exist Azure Storage blobs are organized in containers. Each storage account can have an unlimited amount of containers. Note that a container can’t have uppercase letters. Next, set the path to the file you want to upload and upload the file Upload a file to the container After the file is uploaded you can find it in the Azure Portal: Go to your storage account and click on Blobs on the Overview blade On the Blob service blade, click on your container. There you can see all blobs inside this container. The previously uploaded blob Set metadata on a containerMetadata can be used to determine when files have been updated or to set the content types for web artifacts. There are two forms of metadata: System properties metadata give you information about access, file types and more. User-defined metadata is a key-value pair that is specified for your application. It can be the time when a file was processed or a note of the source.A container has only read-only system properties, while blobs have both read-only and read-write properties.Setting user-defined metadataTo set user-defined metadata, expand the code from before with these two lines: Set user-defined metadata Read user-defined metadataTo read the previously added user-defined metadata, add this code: Read user-defined metadata If the metadata key does not exist, an exception is thrown.Read system propertiesTo read system properties, add this code: Read system metadata There are various system metadata. Use the IntelliSense to see all available ones.Store data using block and page blobsAzure Storage Blobs have three different types of blobs: Block blobs are used to upload large files. A blob is divided up into blocks which allow for easy updating large files since you can insert, replace or delete an existing block. After a block is updated, the list of blocks needs to be committed for the file to actually record the update. Page blobs are comprised of 512- byte pages that are optimized for random read and write operations. Page blobs are useful for VHDs and other files which have frequent, random access. Append blobs are optimized for append operations like logging and streaming data.Write data to a page blobFirst, you have to create a page blob: Create a page blob After the page blob is created, you can write data to it: Write to a page blob After some data are added to the blob, you can read it: Read a page blob Stream data using Azure Storage blobsInstead of downloading a whole blob, you can download it to a stream using the DownloadToStream() API. The advantage of this approach is that it avoids loading the whole blob into the memory.Access Azure Storage blobs securelyAzure Storage supports both HTTP and HTTPS. You should always use HTTPS though. You can authenticate in three different ways to your storage account: Shared Key: The shared key is constructed from a set of fields from the request. It is computed with the SHA-256 algorithm and encoded in Base64 Shared Key Lite: The shared key lite is similar to the shared key but it is compatible with previous versions of Azure Storage. Shared Access Signature: The shared access signature grants restricted access rights to containers and blobs. Users with a shared access signature have only specific permissions to a resource for a specified amount of time.Each call to interact with blob storage will be secured, as shown in the following code: Secure access to your Storage Account Implement Async blob copySometimes it is necessary to copy blobs between storage account, for example, before an update or when migrating files from one account to another.The type of the blob can’t be changed during the async copy operation. Any files with the same name on the destination account will be overwritten.When you call the API and get a success message, this means the copy operation has been successfully scheduled. The success message will be returned after checking the permissions on the source and destination account.The copy process can be performed with the Shared Access Signature method,Configure a Content Delivery Network with Azure Storage BlobsA Content Delivery Network (CDN) is used to cache static files to different parts of the world. A CDN would be a perfect solution for serving files close to the users. There are way more CDN nodes than data centers, therefore the files in the CDN can be better distributed in an area and reduce the latency for your customers. As a result, files are loaded faster and the user experience is increased.The CDN cache is perfect for CSS and JavaScript files, documents, images and HTML pages.Once CDN is enabled and files are hosted in an Azure Storage Account, a configured CDN will store and replicate those files without any management.To enable CDN for the storage account, follow these steps: Open your storage account and select Azure CDN under the Blob Service menu. On the Azure CDN blade, create a new CDN by filling out the form. The difference between the Premium and Standard Pricing tier is that the Premium offers advanced real-time analytics. Create a new CDN If a file needs to be replaced or removed, you can delete it from the Azure Storage blob container. Remember that the file is being cached in the CDN. It will be removed or updated when the Time-to-Live (TTL) expires. If no cache expiry period is specified, it will be cached in the CDN for seven days. You set the TTL is the web application by using the clientCache element in the web.confg file. Remember when you place that in the web.confg file it affects all folders and subfolders for that application.Design blob hierarchiesContainers are flat which means that a container can’t have a child container inside it. A hierarchy can be created by naming the files similar to a folder structure. A solution would be to prefix all Azure Storage blobs with pictures with pictures/, for example, the file would be named pictures/house.jpg or pictures/tree.jpg. The path to these images would be: https://wolfgangstorageaccount.blob.core.windows.net/myblob/pictures/house.jpg https://wolfgangstorageaccount.blob.core.windows.net/myblob/pictures/tree.jpgUsing the prefix simulates having folders.Configure custom domainsThe default endpoint for Azure Storage blobs is: StorageAccountName.blob.core.windows.net. Using the default domain can negatively affect SEO. Additionally, it tells that you are hosting your files on Azure. To hide this, you can configure your storage account to use a custom domain. To do that, follow these steps: Go to your storage account and click on Custom Domain under the Blob Service menu. Check the Use indirect CNAME validation checkbox. By checking the checkbox, no downtime will incur for your application. Log on to your DNS provider and add a CName record with the subdomain alias that includes the Asverify domain. On the Custom domain blade, enter the name of your custom domain, but without the Asverify. Click Save. Create another CNAME record that maps your subdomain to your blob service endpoint on your DNS provider’s website Now you can delete the Asverify CNAME since it has been verified by Azure already. Add a custom domain to your storage account Scale blob storageBlob storage can be scaled both in terms of storage capacity and performance. Each Azure subscription can have up to 200 storage account, with 500 TB if capacity each. This means that each Azure subscription can have up to 100 PB of data.A block blob can have 50,000 100 MB blocks with a total size of 4.75 TB. An append blob has a maximum size of 195 GB and a page blob has a maximum size of 8 TB.In order to scale the performance, there are several features available. For example, you could enable geo-caching for the Azure CDN or implement read access geo-redundant storage and copy your data to multiple data center in different locations.Azure Storage only charges for disk space used and network bandwidth.Many small files will perform better in Azure Storage than one large file. Blobs use containers for logical grouping, but each blob can be retrieved by diﬀerent compute resources, even if they are in the same container.Azure filesAzure files are useful for VMs and cloud services as mounted share. Check out my post “Design and Implement ARM VM Azure Storage” for an instruction on how to create an Azure file share.Implement blob leasingYou can create a lock on a blob for write and delete operations. This lick can be between 15 and 60 seconds or it can be infinite. To write to a blob with an active lease, the client must include the active lease ID with the request.When a client requests a lease, a lease ID is returned. The client can then use this lease ID to renew, change, or release the lease. When the lease is active, the lease ID must be included to write to the blob, set any metadata, add to the blob (through append), copy the blob, or delete the blob. Use the following code to get the lease ID: Get the lease ID of your block blob Create connections to files from on-premises or cloud-based Windows or, Linux machinesAzure Files can be used to replace on-premise file servers or NAS devices. You can find an instruction on how to connect to Azure Files in my post “Design and Implement ARM VM Azure Storage”. The instructions are for an Azure VM but you can do also do it with your on-premise machine.Shard large datasetsYou can use containers to group related blobs that have the same security requirements. The partition key of a blob is the account name + container name + blob name. A single blob can only be served by a single server. If sharding is needed, you need to create multiple blobs.Implement Azure File SyncAzure File Sync (AFS) helps you to automatically upload files from a Windows Server 2012 or 2016 server to the cloud.Azure File Sync helps organizations to: Cache data in multiple locations for fast, local performance Centralize file services in Azure storage Eliminate local backupTo enable AFS, follow these steps: Create a Windows server 2012 or 2016 file server and a storage account. In the Azure portal, click on +Create a resource and search for Azure File Sync. Click on it and click on Create. Provide a name, subscription, resource group and location and click Create. Deploy Storage Sync In your storage account, create a new file share. Create a file share in the storage account By now the Storage Sync Service should be deployed. Open it and click on +Sync group on the Overview blade. On the Sync group blade, enter a name and select a subscription, a storage account, and a file share and click Create. Create a Sync group Set up the file server Next, you have to register your server. To do that, connect to your previously created Windows server and download the Azure Storage Sync agent from here. You might have to disable enhanced security. Disable enhanced security After the installation is finished, start the Server Registration if it doesn&#8217;t start automatically. The default path is C:\\Program Files\\Azure\\StorageSyncAgent. If you see a warning that the pre-requisites are missing, you have to install the Azure PowerShell module. Azure File Sync Azure PowerShell Module missing To install the Azure PowerShell module, open PowerShell and enter Install-Module -Name AzureRM -AllowClobber. &lt;/li&gt;&lt;/ol&gt; Install the Azure PowerShell module After the Azure PowerShell module is installed, you can sign in and register your server. Sign in and register this server ### Configure the Sync group After the server is set up, go back to your Storage Sync Service and open the previously created Sync group on the Overview blade. On the Sync group blade, click on Add server endpoint and then select the previously registered server. Next, select a path which should be synchronized and then click Create. Register a server endpoint ### Test the Azure File Sync To test the file sync, copy some files into the sync folder. Copy files into the sync folder To check if the files were synchronized, go to your file share in the storage account. There should be the files from your sync folder. The synchronize process was successful ## ConclusionIn this post, I talked about the different types of Azure Storage blobs and when they should be used. Furthermore, I showed how to implement Azure files as a file share for your cloud-based and on-premise machines.The last section was about implementing Azure File Sync to synchronize files from a server into the cloud.You can find the example code on GitHub. Don&#8217;t forget to replace the account name and access key placeholder with the name of your storage account and it&#8217;s access key.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Design and implement DevTest Labs", "url": "/design-and-implement-devtest-labs/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-04-05 16:45:00 +0200", "snippet": "Azure DevTest Labs is a service designed to help developers and testers quickly spin up virtual machines (VMs) or complete environments in Azure, enabling rapid deployment and testing of applications. This allows you to easily spin up and tear down development and test resources, minimizing waste and providing better cost control. It is also useful to create pre-provisioned environments for demos and training.Create DevTest LabsTo add a new DevTest Lab, follow these steps: In the Azure Portal select All services and search for DevTest Labs. Click on DevTest Labs and select + Add. Enter a Lab name, Subscription, Location and optionally add a Name and Value for tagging. By default, the Auto-shutdown option is enabled. This feature can help you save costs. Click on it, if you want to disable it or if you want to change the time for the shutdown. Create a DevTest Lab Click on Create. The deployment process creates a new resource group with the following resources in it: The DevTest Lab instance A Key vault instance A Storage account A virtual network Deployed resources Add a VM to your labAfter the DevTest Lab is deployed, you can add a VM to it following these steps: In your DevTest Lab, click +Add on the overview blade. On the Choose a base blade, select your desired image for your VM. Selecting an image opens the Virtual machine blade. Provide a machine name, user name, password, the size of your VM and optionally artifacts. Artifacts are third-party tools like Chrome or Git. Create a Windows 2016 Server for your DevTest Lab Click Create to start the deployment process. Claim a VMAfter the VM is created, the ownership is assigned to the creator but it can be made claimable for others. To unclaim the VM,  follow these steps: Click on the VM, you want to unclaim in your DevTest Lab. On the top of the Overview blade click on unclaim. Unclaim a VM Unclaiming a VM removes it from the My virtual machines area in the DevTest Lab and moves it to the Claimable virtual machines section.To claim a VM, open it and select Claim machine on the top of the Overview blade.Create and manage custom images and formulasThe difference between a custom image and a formula is that the custom image is an image based on a VHD, whereas formulas are base on a VHD with additional pre-configurations such VM size, virtual network, and artifacts. These pre-configured settings can be overridden during the deployment process.Creating custom imagesCustom images provide a static, immutable way to create VMs from a configuration.Pros of using custom images: VMs created from the same image are identical The provisioning of the VM is fastCons of using custom images: To update the image, it has to be recreatedYou can use the Azure Portal or PowerShell to create an image from an existing VM or create one from a VHD.Create a custom image from a provisioned VMTo create a custom image from a provisioned VM using the Azure Portal, follow these steps: In your DevTest lab, select All virtual machines under the My Lab menu and then click on the VM, you want to use as a base for your image. All virtual machines in your DevTest Lab After you selected a VM, click on Create custom image under the Operations menu. On the Custom image blade, provide a name and select whether sysprep should be run. Click OK to create the image. Create a custom image Create a custom image from a VHD using the Azure PortalTo create a custom image from a VHD using the Azure Portal follow these steps: In your DevTest Lab instance, select Configuration and policies under the Settings tab. On the Configuration and policies blade, select Custom images under the Virtual Machine Bases menu and click +Add. Provide a name, the operating system type and select the VHD. If you don’t have any, you have to upload one first. Click OK. Delete a custom imageTo delete a custom image, follow these steps: In your DevTest Lab instance, select Configuration and policies under the Settings tab. On the Configuration and policies blade, select Custom images under the Virtual Machine Bases menu and click the three dots next to the image which you want to delete. Click delete and select Yes in the confirmation dialog. Delete a custom image FormulasFormulas provide default property values and therefore offer a fast way to create VMs from a pre-configured state.Pros of using formulas: Formulas can define default setting that custom images can’t provide. The default settings from the formulas can be modified when creating a new VM.Cons of using custom images: Creating a new formula can be more time consuming than creating a VM from a custom image.There are two ways to create a formula: From a base like a custom image or Marketplace image, use a base when you want to define all characteristics of the formula. From an existing lab VM. Use this approach when you want to create a formula which is based on an existing VM.Create a formula from a baseTo create a formula from a base, follow these steps: In your DevTest Labs instance, select Configuration and policies under the Settings menu. On the Configuration and policies blade, select Formulas (reusable bases) under the Virtual Machine Bases menu and click + Add. On the Choose a base blade, select an image to use for the formula, for example, Windows Server 2016 Datacenter. Provide a Formula name and user name. Select the disk type and the size of your VM. Optionally add artifacts. Artifacts are third-party tools like Google Chrome or Docker which will be installed on your VM. On the Advanced blade, you can configure the IP address and the automatic delete options. After you are done with the configuration, click Create to create the formula. Create a new formula Create a formula from a VMTo create a formula from a VM, follow these steps: In your DevTest Labs instance, on the Overview blade, select the VM from which you want to create the new formula. Select a VM for your formula On the VM&#8217;s blade, select Create formula (reusable base) under the Operations menu. On the Create a formula blade, provide a name and optionally a description and click OK. Create a new formula from a VM Modify a formulaTo modify the properties of an existing formula, follow these steps: In your DevTest Labs instance, select Configuration and policies under the Settings menu. On the Configuration and policies blade, select Formulas (reusable bases) under the Virtual Machine Bases menu. Select the formula you wish to modify. Make your changes on the Update formula blade and select Update when you are finished. Modify an existing formula Delete a formulaTo delete an existing formula, follow these steps: In your DevTest Labs instance, select Configuration and policies under the Settings menu. On the Configuration and policies blade, select Formulas (reusable bases) under the Virtual Machine Bases menu. Click on the ellipsis to the right of the formula you want to delete and click Delete in the context menu. Delete an existing formula On the confirmation dialog click Yes. Configure a lab to include policies and proceduresFor each lab you create, you can control cost and minimize your time waste by managing policies.Configure allowed virtual machine sizes policyYou can configure that the creation of only specific VM sizes is allowed. To do that follow these steps: In your DevTest Labs instance, select Configuration and policies under the Settings menu. On the Configuration and policies blade, select Allowed virtual machines under the Settings menu. On the Allowed virtual machine move the slider to On and then check each VM size which you want to be allowed to be created. Click Save on the top of the blade when you are finished. Enable allowed virtual machine sizes policy Configure virtual machines per user policyIt is also possible to limit the number of virtual machines an user can create. Additionally, you can limit the number of VMs using premium OS disks. To do that, follow these steps: In your DevTest Labs instance, select Configuration and policies under the Settings menu. On the Configuration and policies blade, select Virtual machines per user under the Settings menu. Move the slider to Yes to limit the number of virtual machines and/or the number of virtual machines using premium OS disks per user. After you enabled a limitation, provide a number for the limit. The default value is 1. Click Save. Limit the number of virtual machines and premium OS disks per user If a user has reached his maximum amount of VMs but tries to create another one, he will get an error message.Configure virtual machines per lab policyTo specify the maximum number of VMs that can be created in your current lab, follow these steps: In your DevTest Labs instance, select Configuration and policies under the Settings menu. On the Configuration and policies blade, select Virtual machines per lab under the Settings menu. Move the slider to Yes to limit the number of virtual machines and/or the number of virtual machines using premium OS disks per lab. After you enabled a limitation, provide a number for the limit. The default value is 1. Click Save. Limit the number of virtual machines and premium OS disks per lab Configure auto-shutdown policyThe auto-shutdown is the most important policy for helping you to minimize cost control and also helps to prevent costs when the VMs are not in use. I always enable this policy when I do training because there is always at least one student who doesn’t turn off his VMs at the end of the day.To enable auto-shutdown, follow these steps: In your DevTest Labs instance, select Configuration and policies under the Settings menu. On the Configuration and policies blade, select Auto-shutdown under the Schedules menu. Move the slider to On to enable auto-shutdown. Configure the shutdown time and optionally enable sending a notification and provide a webhook URL or an email address. The notification will be sent 15 minutes prior the shutdown. Click Save. Configure auto-shutdown policy Once configured, the auto-shutdown policy will be applied to all VMs in the lab. You also can adjust the auto-shutdown policy for specific VMs. To do that follow these steps: In your DevTest Labs instance, select the VM you want to configure. On the VM blade select Auto-shutdown under the Operations menu. There you can see the same attributes as before but if you change them here, you only change the settings for this VM. After all changes are made, click Save. Configure auto-shutdown policy for an individual VM I like to disable the auto-shutdown policy on my VM, so I can prepare things for the next days training on the evening without getting interrupted by the shutdown.Configure auto-start policyBesides auto-shutdown, you can also configure an auto-start policy. To do that follow these steps: In your DevTest Labs instance, select Configuration and policies under the Settings menu. On the Configuration and policies blade, select Auto-start under the Schedules menu. Move the slider to On and then configure the start time and on which days the VMs should start. Click Save Configure auto-start policy ##Once configured, the auto-start policy will be applied to all VMs in the lab. You can Opt-out the auto-start policy for a specific VM. To do that follow these steps: In your DevTest Labs instance, select the VM you want to configure. On the VM blade select Auto-start under the Operations menu. There you can see move the slider to On or Off to enable or disable auto-start. Click Save. Disable auto-start for a specific VM Set expiration date policyThe expiration date policy ensures that VMs are automatically deleted at a specified date and time. To do that follow these steps: When creating a new VM click on the Advanced settings. On the Advanced blade click the calendar icon under Expiration date and select the date when the VM should be deleted. Next, select the time. Click OK and then Create. Set the expiration date policy Configure cost managementAzure DevTest Labs was designed to manage your resources and costs effectively. Cost Management is a key feature and allows you to track to costs associated to your lab. It also enables you to view trends, set cost targets and configure alerts.To view your Cost trend chart, select Configuration and policies in your DevTest lab and then select Cost trend under the Cost Tracking menu.Cost trendTo view the monthly estimated cost trend chart follow these steps: In your DevTest Labs instance, select Configuration and policies under the Settings menu. On the Configuration and policies blade, select Cost trend under the Cost Tracking menu. The Cost trend chart To modify the Cost trend chart follow these steps: On the Cost trend blade, click on Manage target. On the Manage target blade, you can modify the date interval of the chart and also set a cost target threshold and notifications via webhook when a certain amount is reached. Click OK to save your changes. Modify the cost trend chart Cost by resourceWith cost by resource, you get a breakdown of your costs of each resource. To do that follow these steps: In your DevTest Labs instance, select Configuration and policies under the Settings menu. On the Configuration and policies blade, select Cost by resource under the Cost Tracking menu. All your resources of the lab are listed on the Cost by resource blade. Your costs by resource Secure access to labsThe access to your DevTest Labs is determined by Azure Role-Based Access Control (RBAC). To understand how this works, you have to understand the difference between a permission, a role, and a scope defined by RBAC. Permission: Defines access to a specific action (for example, read-access to the storage account) Role: A set of permissions that can be grouped and assigned to a user. For example, the reader role can read all resources. Scope: A level within the hierarchy of an Azure resource, such as a resource group or a virtual machine.With RBAC, you can segregate duties within your team into roles where you grant only the amount of access necessary to users to perform their job. The three most relevant roles to Azure DevTest Labs are Owner, DevTest Labs User, and Contributor.Add an owner or user at the lab levelOwners and users can be added at the lab level via the Azure Portal. This also includes external users with a valid Microsoft account. To add an owner or user, follow these steps: In your DevTest Labs instance, select Configuration and policies under the Settings menu. On the Configuration and policies blade, select Access control (IAM) under the Manage menu and click +Add. On the Add permissions blade, select Owner or DevTest Labs user as the role. Enter the name or email address and select the user. Click Save. Add a user to the lab Add an external user to a lab using PowerShellAdditionally to the Azure portal, you can add an external user to your Azure DevTest Labs using PowerShell. Before you can do that, you have to add the user as a gust to the Active Directory though. To do that follow these steps: Open the Azure Active Directory and select Users under the Manage menu. On the All users blade, click +New guest user on the top of the blade. Enter the email of the user you want to add as a guest and optionally include a message for the invitation. Add a user as a guest to your Active Directory After the invitation is sent, the user will show up in your Azure Active Directory. The Azure Active Directory with the new guest user  Now, you can add the user to your lab using PowerShell, following these steps: Create the following variables in PowerShell: $subscriptionId = “&#8221; $labResourceGroup = “&lt;Enter lab’s resource name here&gt;” $labName = “&#8221; $userDisplayName = “&lt;Enter user’s display name here&gt;” Select your subscription (you only have to do this step if you have more than one subscription) Select-AzureRmSubscription -SubscriptionId $subscriptionId Get the user object $adObject = Get-AzureRmADUser -SearchString $userDisplayName Create the role assignment $labId = (‘/subscriptions/’ + $subscriptionId + ‘/resourceGroups/’ + $labResourceGroup + ‘/providers/Microsoft.DevTestLab/labs/’ + $labName) Attention: You need a / in front of subscriptions. The Microsoft documentation doesn’t have the slash and therefore it won’t work. Assign the role to the user: New-AzureRmRoleAssignment -ObjectId $adObject.Id -RoleDefinitionName ‘DevTest Labs User’ -Scope $labId Add a user to your lab with PowerShell After you are done, the user will appear in your lab’s Active Directory. To verify it go to your Azure DevTest Labs –&gt; Configuration and policies –&gt; Access control (IAM). There, you can see the previously added user with his assigned role. Confirm that the user has been added to your lab Use lab settings to set access rights to the environmentYou can give your lab users Contributor access rights. This enables them to edit resources such as SQL Server, in the resource group that contains your lab environment. By default, lab users only have the Reader access rights. To modify the user’s rights, follow these steps: In your DevTest Labs instance, select Configuration and policies under the Settings menu. On the Configuration and policies blade, select Lab settings under the Settings menu. Select Contributor to give users the Contributor access rights. Click Save Give users the Contributor or Reader access rights Use environments in a labYou can use Azure Resource Manager (ARM) templates to spin up a complete environment in DevTest labs. These environments can contain multiple VMs, or a SharePoint farm.  Following infrastructure-as-code and configuration-as-code best practices, environment templates are managed in source control. Azure DevTest Labs loads all ARM templates directly from your GitHub or VSTS Git repositories. As a result, Resource Manager templates can be used across the entire release cycle, from the test environment to the production environment.Configure an ARM template repositoryThere are a couple of rules which you have to follow when organizing ARM templates in a repository: The master template file must be named azuredeploy.json. The parameter file must be named azuredeploy.parameters.json. You can use the parameters _artifactsLocation and _artifactsLocationSasToken to construct the parametersLink URI value, allowing DevTest Labs to automatically manage nested templates. Metadata can be defined to specify the template display name and description. This metadata must be in a file named metadata.json. On the following example metadata file, you can see how to specify the display name and description:{“itemDisplayName”: “Your template name”,“description”: “Your description”}To add a repository to your Azure DevTest Labs using the Azure portal, follow these steps: In your DevTest Labs instance, select Configuration and policies under the Settings menu. On the Configuration and policies blade, select Repositories under the External Resources menu and click +Add. Enter a name, the Git clone URI, your access token and optionally a branch. Enter either a folder path that starts with / and is relative to your Git clone URI or your ARM template definition. Click Save. Add a repository with templates to your Azure DevTest Labs After you added the Repository, you can use the templates when you add a new Resource on the Overview blade of your DevTest Labs.ConclusionThis post described the Azure DevTest Labs and showed what you can do with it and how it helps you to minimize the management tasks and also helps to keep your costs low. I talked about creating VMs, custom images, and formulas and how to apply policies like auto-shutdown to the resources within the lab. Furthermore, I showed how to add users to your lab and how to change their access permissions. The last topic was about setting up your own Git repository which helps you to create complete environments with a single deployment process.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Manage ARM VM Availability", "url": "/manage-arm-vm-availability/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-03-18 18:08:10 +0100", "snippet": "To guarantee high availability, your applications should run on multiple identical virtual machines so that your application is not affected when a small subset of these VMs is not available due to updates or power failures. Using several VMs for your application may require you to set up an Azure Load Balancer. This post will cover the section Manage ARM VM Availability of the 70-532 Developing Microsoft Azure Solutions certification exam.Availability setsAvailability sets help you to increase the availability of your VMs by grouping VMs which never should be turned off at the same time.  Additionally, these VMs are physically separated therefore they have different network and power connections. At no point in time should all VMs of your availability set offline. Availability sets consists of update and fault domains.Update domainsAn update domain works similarly to the Availability Set and constraints how Azure updates the host machines which run your VM. By default, Azure uses five update domains in which your VMs are placed in a round-robin process. Azure ensures that only one update domain at a time is affected by updates and restarts. This means if you have two VMs in your update domains, never more than 50% of your VMs are affected by updates to the host machine.Fault domainsFault domains provide isolation regarding power and network. VMs in separate fault domains will not be on the same host machine or even the same server rack as another. By default, Azure places VMs in a round-robin fashion into the fault domains.Availability sets and application tiersIt is best practice for multi-tier applications to place all VMs belonging to a single tier in an unique availability set and have separate availability sets for each application tier.Configure an availability set for a new VMTo create a new VM and put it into an availability group, follow these steps: Go to the Marketplace in the Azure Portal. Select Compute and then click on Windows Server 2016 Datacenter. On the Create virtual machine, provide a name, user name, password, subscription, and location for your new VM. Create a new VM Select the desired size of your VM on the Choose a size blade. On the Settings blade, click on Availability set. Either add a new one by clicking on Create new or select an existing one. Create a new Availability Set After adding the availability set click OK. On the Summary blade, click Create to start the deployment process of your VM. Configure an availability set for an existing VMAn existing VM can’t be added to an availability set. You can add a VM to an availability set only when creating the VM. To move a VM to an availability set, you have to recreate it.Add a Load Balancer to an availability setThe Azure Load Balancer distributes the incoming traffic to all VMs within an availability set in a round robin manner. It also checks the health of the VM and removes unresponsive VMs automatically from the rotation.Configure a Load Balancer with your availability setTo add an Azure Load Balancer, follow these steps: In the Azure Portal, go to the Marketplace and search for Load Balancer. Select the Load Balancer and click Create. On the Create a Load Balancer blade, provide a name, subscription, resource group, and location. Select Public as type, if you want to load balance traffic from the Internet, or internal if you want to load balance traffic from your virtual network Select or add a new Public IP address. Create a Load Balancer Click Create. After the Load Balancer is deployed, select Backend pools under the Settings menu and click + Add. On the Add backend pool blade, provide a name and in the Associated to drop-down list select Availability set. Click + Add a target network IP configuration to add your VM. Repeat this step for each VM you want to add. Add a backend pool Click OK. The created backend pool Select Health probes under the Settings menu and click + Add. On the Add health probe, provide a name. You can leave the remaining options as they are. Optionally you can change the checking interval, and after how many failed tries a VM should be marked as unhealthy. Add health probe to your Load Balancer Click OK. The created health probe Select Load balancing rules under the Settings menu and click + Add. On the Add load balancing rule, provide a name, the previously created pool, and health probe. You can leave the remaining options as they are. Create a load balancing rule Click OK. Conclusion of Manage ARM VM AvailabilityThis post explained what availability sets are and how they use fault and update domains to guarantee that never all your VMs are turned off at the same time. Then I talked about the Azure Load Balancer and how it can be configured to load balance traffic to different VMs within your availability set. These topics covered the part Manage ARM VM Availability of the 70-532 Developing Microsoft Azure Solutions certification exam.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Monitor VMs in Azure", "url": "/monitor-vms-in-azure/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-03-14 15:45:17 +0100", "snippet": "To monitor VMs in Azure means collecting and analyzing different metrics as well as collection log data from system log files and from applications running within the VMs. It is possible to configure an email alert to an administrator which is triggered when certain criteria regarding these metrics are met. Monitoring helps you to gain insight into the status of the VMs.Windows and Linux VMs collect the following metrics out of the box: CPU percentage Disk read and write in KB/s or MB/s Network in and out in KB/s or MB/sOn Windows, the Azure Virtual Machine Agent installs the IaaSDiagnostics extension which monitors and collects diagnostic data. On Linux the Microsoft.Insights.VMDaignsticsSettings extension provides the same function.The metrics collected on a Linux VM are: CPU Disk Memory Network Packets Page SwapThe metrics collected on a Windows VM are: CPU Disk Network Memory ASP.NET SQL ServerThese metrics are stored in Azure Storage Tables. By default, all metrics are collected every minute as a new row in the table.For Windows VMs, metric data is written to the WADPerformanceCountersTable, with aggregates of these performance counter metrics aggregated to the minute or to the hour written to tables that start with the name WADMetricsPT1M for by minute and WADMetricsPT1H for by hour.  In addition to metrics, system logs are also collected. For Linux VM’s, the Syslog is collected into the LinuxsyslogVer2v0 table. For Windows VMs, all event log entries for the three event logs (application, security and system logs) are written to the WADWindowsEventLogsTable.Windows VMs can collect other types of logs. Diagnostic infrastructure logs (events generated by the Azure Diagnostic Agent, such as issues collecting metrics) are written to the WADDiagnosticInfrastructureLogsTable, and application logs (the trace output from your .NET  application running in the VM) are stored in the WADLogsTable. Windows VMs can also collect Event Tracing for Windows Events. These events are collected into the WADETWEventTable. (Source) Configure monitoring and diagnostics for a new VMTo enable monitoring and diagnostics when deploying a new VM follow these steps: Go to the Marketplace, select Compute, then Windows Server, select the desired Version, for example, 2016. After selecting your OS click Create. Provide the following information for your VM: Name Disk type (SSD or HDD) User name Password Subscription An existing or new Resource group Location License (if available) Entering basic data for the new VM On the next blade choose a size for your VM. There are many different configurations with different costs to choose from. After you selected one, click Select. Select a VM size according to your needs On the Settings blade, under the Monitoring settings select enabled for Boot diagnostics and Guest OS diagnostics. Select or create a Storage Account where the logs will be stored. Enable diagnostics when provisioning a VM After entering all your settings, you get an overview of them in the Summary blade. Click Create to start the deployment process for your VM. Summary of the new VM Configure monitoring and diagnostics for an existing VMTo enable monitoring and diagnostics for an existing VM follow these steps: Go to your VM in the Azure Portal. Under the Monitoring section, select Diagnostics settings. For Linux VMs, toggle the Status to On to enable diagnostics. For Windows, you have more options On the Overview tab, select Enable guest-level monitoring. Check the categories you want to log on the Performance counters tab. Additionally, you can set the sample rate in seconds.  On the Logs tab, you can configure what events the different logs (Application, Security, System, and IIS) log. On the Crash dumps tap, you can enable collecting memory dumps during a crash. To enable the collection diagnostic infrastructure logs, tootle Diagnostic infrastructure logs to Enabled on the Agent tab. Then select the desired log level. After all settings are made, select Save.Configure AlertsAzure enables you to configure alert rules based on the collected metrics of your VM. These alerts can be sending an email, invoking a Webhook or run a Logic App. Additionally, it is possible to send an email, an SMS or a Webhook when a specific log event is encountered.Enable alertsTo enable and configure alerts follow these steps: Navigate to your VM in the Azure Portal. Under the Monitoring menu, select Alert rules. Click + Add metric alert or + Add activity log alert to add a new rule. On the Add rule provide a name, select the metric source, specify the condition and then select the action to take when this condition is met. Create a new alert for your VM I checked Email owners, contributors and readers to send an email to them. Additionally, you can also enter emails to notify more people. Monitor metricsYou can assess the status and health of your VM by viewing its metrics in the portal, by querying the table storage for diagnostic logs or by downloading the IIS logs from Azure Storage.To look at the metrics follow these steps: Navigate to your VM in the Azure Portal. Select Metrics under the Monitoring menu. On the Metrics blade, select the metrics you want to view. The selected metrics will be displayed as a graph on the right side. View the desired metrics You can add a title and subtitle on the top of the graph and change the time range on the right side. View event logs, diagnostic infrastructure logs and application logsYou can view event logs, diagnostic infrastructure logs and application logs by querying the respective tables  WADWindowsEventLogsTable, WADDiagnosticInfrastructureLogsTable, WADLogsTable). This can be easily done using Visual Studio: Open Visual Studio On the View menu, click Cloud (or Server) Explorer. Expand the Azure node, then the Storage node. Expand the storage account which contains the logs you want to view. Then expand Tables. Select a log using Visual Studio Right-click on the table you want to query and selectOpen to display its contents. The queried logs in Visual Studio View IIS logsTo view IIS logs using Visual Studio follow these steps: Open Visual Studio On the View menu, click Server Explorer. Expand the Azure node, then the Storage node. If you can’t see your storage account there right-click and select Attach external storage. In the new window, enter the name and key for your storage account. Expand the storage account which contains the logs you want to view. Then expand Blobs. If you don’t see the logs, go to your VM, select Diagnostic settings and on the Logs blade enable IIS logs. Open the IIS log using Visual Studio Right-click on wad-iis-logfiles and select view Blob container. There you can view or download the IIS logs. View Boot diagnosticsTo view the boot diagnostics follow these steps: Open your VM in the Azure Portal. Select Boot diagnostics from the Support + Troubleshooting menu. If you have a Windows VM, you see the login screen. Select Serial log to see and download the log file. If you have a Linux VM, you will see the log by default. View Boot diagnostics Enable Application Insights at runtimeTo enable Application Insights at runtime, you first have to create a new Application Insights instance. You can do that following these steps: Go to the marketplace in the Azure Portal. Search for Application Insights and click Create. Provide a new, Subscription, Resource Group and Location. Leave the Application Type as ASP.NET web application. Create a new Application Insights instance Go to your WebApp, which you want to monitor. Select Application Insights under the Monitoring menu. On the Application Insights blade, click Select existing resource and then click on your previously created Application Insights instance. Enable Application Insights for your WebApp Click on OK. Visit your WebApp and after a couple moments, you will see live information about your WebApp. Get Live Stream information from Application Insights Monitor VM workloads by using Azure Application InsightsTo monitor a VM workload with Azure Application Insights follow these steps: Open the IIS web server on your VM. Download and install the Status Monitor. After the Status Monitor is installed, open IIS and select the web application that you want to monitor. Sign in with your Azure credentials and click on Configure settings to configure to which Application Insights you stream your data. Enable Application Insights Status Monitor on your VM After you configured the settings, the web application will start streaming its information to the selected Application Insights application. To view these information, go to the Azure Portal and open your Application Insights instance. Select Live Metrics under the Investigate menu. You will see a live stream of incoming and outgoing requests, failures and the duration of the requests. Live Metrics in Application Insights from the Web Application running on IIS on the VM Monitor VMs using Azure Log Analytics and OMSWith Azure Log Analytics, you can collect data directly from your Azure VM and other resources in your environments into a single repository for detailed analysis. To set up Azure Log Analytics follow these steps: In the Azure Portal, go to the Marketplace and search for Log Analytics. Select Log Analytics and click Create. Provide a name, Subscription, Resource Group, Location, and Pricing tier, then click OK. Create a new Log Analytics instance Enable the Log Analytics VM ExtensionAfter Log Analytics is created, you have to enable the Log Analytics VM Extension. The extension gets installed automatically and configures the agent to send data to the Log Analytics automatically. To set enable the Log Analytics VM Extension follow these steps: Go to your Log Analytics in the Azure Portal. Select Virtual machines under the Workspace Data Sources menu and select the VM you want to install the agent on. On the Virtual machine blade, click on Connect. This installs the agent and configures it for your Log Analytics workspace. After the installation is completed, the OMS connection status will change to This workspace. Installing the agent on your VM Collect event and performance dataLog Analytics can collect events from the Windows event logs or Linux Syslog and performance and also take action when a particular condition is detected. To set it up, follow these steps: Go to Log Analytics in the Azure Portal. Select Advanced settings under the Settings menu. Select Data and then select Windows Event Logs. Add a new event log by entering a name and then clicking +. After the log is created, select the desired log level. Create a new Log for information from your VM Select Windows Performance Counters and enable your desired performance counters. Select your desired performance counters Click Save. View the collected dataAfter data collection is set up, you can run searches in your log. To do that, follow these steps: Open your Log Analytics in the Azure Portal. Click Log Search under the General menu. The Log Search blade already suggests a couple of queries and the search box also makes suggestions based on your input. For example, you could enter Usage and click the Search icon which will give you various information about the usage of your VM. Search the logs for the usage of your VM Monitor your network topologyAfter you deployed your Network Watcher, you can look at the topology of your network by following these steps: Click on Topology under the Monitoring menu. On the Topology blade select your Subscription, Resource group, and Virtual Network. Your network will be displayed you can download it by clicking on Download topologyMonitor your networkTo monitor and diagnose your network, Azure has a service called Network Watcher. The Network Watcher can visualize your network and therefore can help you to understand, diagnose and gain insights into your network in Azure. Following, I will talk about some features of the Network Watcher.Create a Network WatcherTo enable a Network Watcher follow these steps: In the Azure Portal, click on All services and search for Network Watcher and select the result. On the Overview blade of your Network Watcher, enable the regions you want to monitor in the drop-down list. Enable Network Watchers in your desired location Monitor the topology of your networkAfter your Network Watcher is deployed, you can take a look at the topology of your network. To do that follow these steps: In your Network Watcher, click on Topology under the Monitoring menu. On the Topology blade, select a Subscription, Resource Group, and a Virtual Network. After a couple of seconds, your network will be displayed. You can also download the topology by clicking on Download topology. Topology with virtual machines and network cards Monitor your network limitsYou can only have a certain amount of network resources per subscription. The Network Watcher lists all used resources and makes it easy to monitor them. To do that follow these steps: In your Network Watcher, click on Network subscription limit. Select a Subscription and Location. The Portal will list all resources, the current limit, and your usage. Overview of the Network subscription limit ConclusionIn this post, I showed how to enable monitoring and diagnostics when deploying a new VM as well as for an existing VM. The next section talked about Application Insights and how to monitor WebApps and also application running on IIS on a VM. Next, I showed how to monitor your VMs with Log Analytics and how to filter these logs. The last part was a short overview of the Network Watcher and how to monitor your network with it.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Design and implement ARM VM Azure Storage", "url": "/design-implement-arm-vm-azure-storage/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-03-12 15:59:16 +0100", "snippet": "Azure Storage provides more functionality than just attaching data disks to your VM. In this post, I will talk about creating a file storage account, how to create and access a file share using PowerShell and C# and how to enable caching for your disks.Plan for storage capacityVMs in Azure have a local temp drive (D: on Windows and \\dev\\sdb1 on Linux) and a system disk. The disks are saved as Blob in Azure Storage. This means that this Blob governs the performance and the capacity of the disk. There are two factors when it comes to storage performance and capacity: Is the disk standard (HDD) or premium (SSD)? Is the disk managed or unmanaged?The difference between managed and unmanaged is that unmanaged disks require the creation of an Azure Storage Account in your subscription whereas managed disks manage the Storage Account for you. This means that you only need to specify the size and type of your disk and Azure manages the rest for you. The main advantage of using managed disks is that the Storage Account does not limit the disk. See the following table with the differences between standard and premium managed and unmanaged disks:Comparison of Standard and Premium disks (Source) Feature Standard (unmanaged) Standard (managed) Premium (unmanaged) Premium (managed) Max IOPS for storage account 20k IOPS N/A 60k -127.5k IOPS N/A Max bandwidth for storage account N/A N/A 50 Gbps N/A Max storage capacity per storage account 500 TB N/A 35 TB N/A Max IOPS per VM Depends on VM size Depends on VM size Depends on VM size Depends on VM size Max throughput per VM Depends on VM size Depends on VM size Depends on VM size Depends on VM size Max disk size 4TB 32GB &#8211; 4TB 32GB &#8211; 4TB 32GB &#8211; 4TB Max 8 KB IOPS per disk 300 &#8211; 500 IOPS 500 IOPS 500 &#8211; 7,500 IOPS 120 &#8211; 7,500 IOPS Max throughput per disk 60 MB/s 60 MB/s 100 MB/s &#8211; 250 MB/s 25 MB/s &#8211; 250 MB/s IOPS is a unit of measure which counts the number of input and output operations per second. Usually, Azure VMs allow the number of disks you can attach is twice the number of CPU cores of your VM.Configure Storage PoolsBefore you can configure a storage pool, you have to add disks to your VM.Create new disks for your VMFollow these steps: Open your VM in the Azure Portal. Under the Settings menu click Disks. On the Disks blade, click + Add data disk. In the drop-down menu under Name select Create disk. On the Create managed disk blade provide a Name, Resource group and the Account type (SSD or HDD). As Source type select None (empty disk) and provide your desired size. Create a new disk for your VM Click Create. You can add more disks or click Save on the top of the blade. Adding disks to your VM Create a Storage PoolStorage Pools enable you to group together a set of disks and then create a volume from the available aggregate capacity. To do that follow these steps: Connect to your Windows VM using RDP. Open the Server Manager. Click on File and Storage Services and then Storage Pools. Adding a new Storage Pool to your VM Provide a name for your Storage Pool and click Next. Select all disks which you want to add to the storage pool and click Next. Add the physical disks to the Storage Pool Click Create and then Close to create the storage pool. Create a new Virtual Disk After the storage pool is created, right-click on it and select New Virtual Disk… Create a new virtual disk Select the storage pool you just created and click OK. In the wizard enter a name for the virtual disk and click Next twice. Select Simple as your layout and click Next. You don&#8217;t need mirroring because Azure already replicates your data three times. For the provisioning select and click Next. Select Maximum size, so that the new virtual disk uses the complete capacity of the storage pool and click Next. In the Confirm selections window, click Create. After the new volume is created click Next on the first page of the wizard. Select the disk you just created and click Next. Leave the volume size as it is and click Next. Leave Assign to Drive letter selected and optionally change the drive letter, then click Next. In the last window, click Create and then Close to finish the process. After the wizard is completed, open the Windows Explorer and you can see your new drive. The Windows Explorer with the new mapped disk You can increase the IOPS and total storage capacity if you use multiple blobs for your disks.For Linux, you have to use the Logical Volume Manager to create the volume.Configure disk cachingEach disk you attach to your VM has a local cache which can improve the performance of read and write operations. This cache is outside your VM (it’s on the host of your VM) and uses a combination of memory and disks on the host. There are three caching options available: None: No caching Read-Only: The cache is only used for read operations. If the needed data is not found in the cache, it will be loaded into it form the Azure Storage. Write operations go directly into the Azure Storage. Read/Write: The cache is used for read and write operations. The write operations will be written into Azure Storage later.The default options are Read/Write for the operating system disk and Read-Only for the data disk. Data disks can turn off caching, operating system disk can’t. The reason for this behavior is that Azure Storage can provide better performance for random I/Os than the local disk. The big advantage of caching is obviously the better performance but also minimizes caching your costs because you don’t pay anything if you don’t access your Storage Account.Enable disk cachingTo enable caching for your disk follow these steps: Open your VM in the Azure Portal. Under the Settings menu, select Disks. Select Edit on the Disks blade. Select the Host Caching drop-down and set it to the desired configuration. Enable disk caching Click Save. Enable geo-replicationWith geo-replication, you can copy your data into other data centers, even in other regions all around the world. Additionally to geo-replication, Azure created three copies of your data within the data center where they reside. Keep in mind that geo-replication is not synchronized across blob files. To save money and keep your data safe configure your VM disks to use locally redundant replication.Configure shared storage using Azure File storageAzure File storage enables your VMs to access files using a shared location within the same region your VMs. The VMs don’t even have to be in the same subscription or storage account than your Azure File storage. It only has to be in the same region. It can be compared with a network drive since you can also map it like a normal network drive. Common scenarios are: Support applications which need a file share Centralize storage for logs or crash dumps Provide access to shared application settingsTo create an Azure File storage you need an Azure Storage account. The access is controlled by the storage account name and a key. As long as your VM and the File storage are in the same region, the VM can access the storage using the storage credentials.Each share is an SMB file share and can contain an unlimited number of directories. The maximum file size is one terabyte and the maximum size of a share is five terabytes. A share has a maximum performance of 1,000 IOPS and a throughput of 60 MB/s.Creating a file share using Azure StorageBefore you can create a file share, you need to create a storage account. To do that follow these steps: Click on Storage accounts in the Azure Portal. Click + Add on the top of the blade. On the Create storage account blade provide a name, Subscription, Resource group and Location. Enable Secure transfer required if you want to use https only. Create a new storage account Click Create. With the storage account created, I can use PowerShell to create a file share. To do that I need the storage account name and the storage account key. To get this information open your storage account and click on Access keys under the Settings menu. Keys for the storage account To create a file share using PowerShell use: $context = New-AzureStorageContext -Storage-AccountName “YourStorageAccountName” and then New-AzureStorageShare “YourShareName” -Context $context. The share name must be a valid DNS name, lowercase and between 3 and 63 characters long. Create a file share using PowerShell Mounting the file shareTo access the share follow these steps: Connect to your VM via RDP. Open PowerShell or the command promp. Enter command to add your Azure Storage account credentials to the Windows Credentials Manager: cmdkey /add:.file.core.windows.net /user: /pass:. Replace the values within &lt;&gt; with your credentials. You can find your credentials in the Azure Portal in your Storage Account. Add the Azure Storage account credentials to the Windows Credentials Manager To mount the file share to a drive letter use net use z: \\\\.file.core.windows.net\\. For example net use z: \\\\.file.core.windows.net\\.  Replace the values within &lt;&gt; with your storage account name and share name &lt;/li&gt;&lt;/ol&gt; Map the file to drive letter Z Now you can find the file share in the Windows Explorer. The mapped file share ### Access the file share using PowerShellYou can upload or download file to and from the file share using PowerShell. Before I start, I uploaded a text file to the file share and renamed it to fileshare.txt 1. To work on your storage account, you have to get its context using $variable = New-AzureStorageContext -StorageAccountName -StorageAccountKey . Replace the values within &lt;&gt; with your storage account name and your key. 2. To download a file to your current directory use Get-AzureStorageFileContent -ShareName -Path -Context $variable. Download a file from your file share using PowerShell ### Access the file share using C#For this example, I create a new C# console application. Then follow these steps to access the file share: 1. Install the WindowsAzure.Storage and the WindowsAzure.ConfigurationManager NuGet Packages. 2. Add your storage account credentials to the app.config file. Add the storage connection string to App.config Connect to your storage account and get the reference from the file share. Connect to your storage account and get the reference from the file share Get a reference to your root directory and to the file you want to download. Get a reference to your root directory and to the file you want to download Download the file to your computer. Download the file to your computer You can also upload a file by getting a reference to your directory and then upload the file using UploadText Upload a file to your file share You have to replace my placeholder strings with valid values for a filename or share name. My example project is on GitHub.## **Disk encryption**Before you can encrypt the disk of your VM, you have to do some set up steps.### **Set up**To set up your Azure environment to encrypt the disks of your VMs, you have to do an application registration and create a Key vault.#### Azure Active Directory App RegistrationTo register an app in the AAD follow these steps: 1. In the Azure Portal go to the Azure Active Directory. 2. Select App registrations under the Manage menu and click on + New application registration. 3. On the Create blade, provide a name and Sign-on URL and click on Create. Create an application registration On the App registrations blade, select All apps from the drop-down list on the top and copy the Application Id of your newly created app. This id is the AAD client id which I will need later. Get the AAD client Id Click on your application and then select Settings. Select Key under the Api Access. Enter a description and set the expire that for the key on the Keys blade. Click Save. After the key is created, the hidden key value is display. It is important that you copy the key because after you close the window, it won&#8217;t be displayed again. This key is the client secret for later. Create a client secret #### Create a Key vault 1. The next step is to create a Key vault. To do that click on All services and search for Key vaults. 2. On the Key vaults blade, click on + add. 3. Provide a Name, Subscription, Resource Group and Location on the Create key vault blade. 4. Click on Access policies and the on + Add new. 5. On the Add access policy blade, click on Select principal and search for your previously create application registration. 6. In the Key permissions drop-down list, select Wrap Key. Create a new key vault with the previously created application registration In the Secret permission drop-down list, select Set. Click OK twice and then Create After your Key vault is created, click on Access policies under the Settings menu. On the Access policies blade, click on Click to show advanced access policies and select all three checkboxes. Enable all access policies Still on the Access policies blade, click on your User (mine starts with 789c&#8230; on the screenshot above). In the Key permissions drop-down list, check Select all and click OK. Give your user all key permissions After all these steps, you can encrypt your Windows VM with Powershell, CLI or with a template and your Linux VM with CLI or with a template.### **Windows**To demonstrate how to encrypt a Windows VM, I created a new Windows Server 2016 VM with the name WinServer in the resource group WinRg.#### **Powershell**To encrypt your Windows VM using Powershell follow these steps: 1. Login to your Azure account with Login-AzureRmAccount. 2. Select your Subscription with Select-AzureRmSubscription -SubscriptionName &#8220;YourSubscriptionName&#8221;. 3. $resourceGroupName = &#8220;YourResourceGroup&#8221; 4. $vmName = &#8220;YourVmName&#8221; 5. $clientID = &#8220;YourAadClientId&#8221; (you copied that value during the setup process) 6. $clientSecret = &#8220;YourClientSecret&#8221; (you copied that value during the set up process) 7. $keyVaultName = &#8220;YourKeyVaultName&#8221; 8. $keyVault = Get-AzureRmKeyVault -VaultName $keyVaultName -ResourceGroupName $resourceGroupName 9. $diskEncryptionKeyVaultUrl = $keyVault.VaultUri 10. $keyVaultResourceId = $keyVault.ResourceId 11. Set-AzureRmKeyVaultAccessPolicy -VaultName $keyVaultName -ResourceGroupName $resourceGroupName -EnabledForDiskEncryption 12. Set-AzureRmVMDiskEncryptionExtension -ResourceGroupName $resourceGroupName -VMName $vmName -AadClientID $clientID -AadClientSecret $clientSecret -DiskEncryptionKeyVaultUrl $diskEncryptionKeyVaultUrl -DiskEncryptionKeyVaultId $keyVaultResourceId 13. This starts the Encryption and takes around 10 -15 minutes. After the encryption is done, you can check, if your disks are encrypted with Get-AzureRmVMDiskEncryptionStatus -ResourceGroupName $resourceGroupName -VMName $vmName Encrypt a Windows VM using PowerShell #### **Template**Additionally to the PowerShell encryption can you encrypt your VM with a template. Go to GitHub and then click on deploy to Azure. This opens the template in Azure. Enter the following values: Enable encryption on a running Windows VM template The Aad Client ID and Aad Client Secret are the values you copied during the setup process. After you entered your values, accept the terms and conditions and click on Purchase.### **Linux**To demonstrate how to encrypt a Windows VM, I created a new Kali Linux VM with the name Linux in the resource group WinRg.#### **Template**Go to GitHub and then click on deploy to Azure. This opens the template in Azure. Enter the following values: Create encryption on a running Linux VM with the template The Aad Client ID and Aad Client Secret are the values you copied during the set up process. After you entered your values, accept the terms and conditions and click on Purchase.**CLI**To encrypt your VM disk with Azure CLI see the documentation.## StoreSimpleThe Azure StoreSimple Virtual Array is an integrated storage solution which manages storage tasks between an on-premises virtual array running in a hypervisor and Microsoft Azure cloud storage. It is a cost-effective file server or iSCSI server solution which is well-suited for infrequently accessed archival data. The virtual array supports the SMB and iSCSI protocol. It runs in your existing hypervisor infrastructure and provides tiering to the cloud, cloud backup, fast restore and disaster recovery features.The following table summarizes the most important features (Source): Feature StorSimple Virtual Array Installation requirements Uses virtualization infrastructure (Hyper-V or VMware) Availability Single node Total capacity (including cloud) Up to 64 TB usable capacity per virtual array Local capacity 390 GB to 6.4 TB usable capacity per virtual array (need to provision 500 GB to 8 TB of disk space) Native protocols iSCSI or SMB Recovery time objective (RTO) iSCSI: less than 2 minutes regardless of size Recovery point objective (RPO) Daily backups and on-demand backups Storage tiering Uses heat mapping to determine what data should be tiered in or out Support Virtualization infrastructure supported by the supplier Performance Varies depending on underlying infrastructure Data mobility Can restore to the same device or do item-level recovery (file server) Storage tiers Local hypervisor storage and cloud Share size Tiered: up to 20 TB; locally pinned: up to 2 TB Volume size Tiered: 500 GB to 5 TB; locally pinned: 50 GB to 200 GB, maximum local reservation for tiered volumes is 200 GB. Snapshots Crash consistent Item-level recovery Yes; users can restore from shares ### Why use StorSimpleStorSimple can connect the users and servers to Azure storage in minutes, without making changes to applications. The following table show some benefits of StorSimple Virtual Array (Source): Feature Benefit Transparent integration The virtual array supports the iSCSI or the SMB protocol. The data movement between the local tier and the cloud tier is seamless and transparent to the user. Reduced storage costs With StorSimple, you provision sufficient local storage to meet current demands for the most used hot data. As storage needs grow, StorSimple tiers cold data into cost-effective cloud storage. The data is deduplicated and compressed before sending to the cloud to further reduce storage requirements and expense. Simplified storage management StorSimple provides centralized management in the cloud using StorSimple Device Manager to manage multiple devices. Improved disaster recovery and compliance StorSimple facilitates faster disaster recovery by restoring the metadata immediately and restoring the data as needed. This means normal operations can continue with minimal disruption. Data mobility Data tiered to the cloud can be accessed from other sites for recovery and migration purposes. Note that you can restore data only to the original virtual array. However, you use disaster recovery features to restore the entire virtual array to another virtual array. For more information see the documentation.## ConclusionIn this post, I talked about storage pools on VMs and how virtual disks are created. Then I talked about enabling geo-replication and disk caching. Next, I showed how to create a file share and how to interact with it using your VM, PowerShell or C# code. After the file share, I explained how to set up disk encryption for your Windows and Linux VMs. The last section talks about what StorSimple is and what benefits it can bring.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "Scale ARM Virtual Machines", "url": "/scale-arm-virtual-machines/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-03-07 20:38:46 +0100", "snippet": "In Azure, it is possible to scale Azure Web Apps or Virtual Machines by increasing the size or the count of running instances. Azure also supports auto-scaling on the instance count. In this post, first I will talk about the difference between scale out and scale up. Then I will show how ARM VM Scale Sets are deployed and what they are used for and how to configure them for auto-scale.Scale up vs. scale outScaling a system means changing its available resources and performance. The advantage is that you can easily scale up or scale out a system if you need more performance and scale down or scale in when you don’t need it anymore so you can keep your costs low and save money.Scale up and scale downScaling up and down means changing the size of your VM. You can change the following components of your VM: HDD or SSD size IOPS (Input/output operations per second) The number of CPU cores The amount of RAM Network performanceScaling up and down a VM using the Portal In the Azure Portal go to your VM (Virtual Machines and then click the VM you want to scale). Select Size under the Settings menu. This opens the Choose a size blade which you already know from creating the VM. Select the new size. Click Select to scale your VM.Scaling up and down a VM using PowerShellYou can also scale your VM by using PowerShell. First check which VM sizes are available in a specific region by using Get-AzureRmVmSize -Location “Westeurope” | Sort-Object Name | ft Name, NumberOfCores, MemoryInMB, MaxDataDiskCount -AutoSize. Get all available VM sizes of a region in PowerShell If you want to check another location than westeurope, change westeurope to your desired location.Next put your Resource group, VM name and new VM size into variables. Then use Update-AzureRmVM -ResourceGroupName $ResourceGroupName -VM $VMToScale to update your VM to the desired size. Scale your VM using PowerShell The scaling process takes a couple of minutes. You get the confirmation after it is finished.Scale out and scale inScaling out and scaling in means changing the number of VMs on which your software runs. This means if you need more performance, you start additional VMs and have a Load Balancer distributing the requests to all VMs. Scaling out and scaling in is more common than scaling up because you can scale almost indefinitely. If you need way more performance, you just spin up another 1000 VMs. It is harder (or even impossible) and more expensive to increase the CPU cores to 1000 or ram to 5 TB. Scaling out VMs can be achieved with ARM VM Scale Sets which I will describe in the next section.ARM VM Scale Sets (AVSS)To scale out a VM, Azure deploys a VM Scale Set which contains identical copies of your ARM VM. To scale in, Azure removes these deployed instances. Each VM within the Scale Set has the same size and pricing tier.There are some differences between the VMS in the Scale Set and standalone VMs: Every VM must be identical to the other ones in the VM. Standalone VMs can vary from each other Scale Sets support overprovisioning during the scale out event. This means that Azure deploys more VMs than configured and when the requested amount is running, Azure deletes the additional VM. This feature reduces the deployment time and increases the performance of your application. Furthermore, you are not charged for the additional VMs. Scale Sets can roll out upgrades to all VMs within the Scale Set using upgrade policies. Standalone VMs have to orchestrate the update themselves. The Azure Autoscale feature can be used only with a Scale Set but not with a standalone VM.VM Scale Set capacityThe maximum number of VMs inside a Scale Set is called capacity and depends on three factors: Support for multiple placement groups The use of managed disks If the VM’s use an image from the Marketplace or are created from a custom imagePlacement groups are similar to Availability Sets where a placement group is implicitly an Availability Set with five fault domains and five update domains which support up to 100 VMs. When you deploy a Scale Set you can restrict it to only a single placement group. This will limit the capacity of your Scale Set to 100 VMs.You can allow multiple placement groups, the Scale Set can support up to 1,000 VMs.If you use unmanaged storage, you will be limited to a single placement group which will result in only 100 available VMs for your Scale Set. You can use managed disks to support up to 1,000 VMs.If you used a custom image to create the VMs in your Scale Set, then the Scale sets support 100 VMs. If you used an image from the Marketplace, the capacity of your Scale Set it 1,000  VMs.Deploy a Scale Set using the PortalDeploying a Scale Set is similar to deploying a standalone VM: Go to the Marketplace Click + New and search for scale sets. Then select Virtual machine scale sets and click Create. Provide a name for you Scale Set. Select Windows or Linux as your OS. Select your Subscription, Resource group, and a Location. The Resource group must be either empty or you must create a new one for your Scale Set. Enter a user name and password for Windows or an SSH user name and password or SSH public key for Linux. Under the Instances and Load Balancer menu, enter the desired instance count. Select the size for the VMs in your Scale Set. Enable scaling beyond 100 instances lets you select if you want one or multiple placement groups. If you allow multiple placement groups, you can only select managed disks. If not, you can choose between managed and unmanaged. Configure the public IP address name which you can use to access your VMs via a Load Balancer. Select Dynamic or Static as your public IP allocation mode and provide a label for your domain name. These options are not available if you allowed multiple placement groups. Leave Autoscale disabled. Click Create. Create a VM Scale Set Deploy a Scale Set using a custom imageTo create a Scale set from a custom image perform the following steps: Generalize and capture the image from a standalone VM. Create an ARM template: Creates a managed image based on the generalized unmanaged disk available in Azure Storage. Your template needs to define a resource of type “Microsoft. Compute/images” that references the VHD image by its URI. Configure the Scale Set to use the managed image Deploy the ARM template through the Portal, PowerShell or by using the Azure CLIYou can find an example template on GitHub. This script uses Linux instead of Windows as the operating system.Configure AutoscaleThe Autoscale feature enables you to automatically scale your resources based on various metrics. Not only VMs use the Autoscale feature, it is also possible to scale other services such as the Azure Queue or the Service Bus QueueConfigure Autoscale when provisioning a VM Scale Set using the PortalWhen you configure Autoscale when provisioning a VM Scale Set, you can only scan against its CPU utilization. The only difference to the provisioning without Autoscale is 12. Follow these steps to configure Autoscale: Go to the Marketplace Click + New and search for scale sets. Then select Virtual machine scale sets and click Create. Provide a name for you Scale Set. Select Windows or Linux as your OS. Select your Subscription, Resource group, and a Location. The Resource group must be either empty or you must create a new one for your Scale Set. Enter a user name and password for Windows or an SSH user name and password or SSH public key for Linux. Under the Instances and Load Balancer menu, enter the desired instance count. Select the size for the VMs in your Scale Set. Enable scaling beyond 100 instances lets you select if you want one or multiple placement groups. If you allow multiple placement groups, you can only select managed disks. If not, you can choose between managed and unmanaged. Configure the public IP address name which you can use to access your VMs via a Load Balancer. Select Dynamic or Static as your public IP allocation mode and provide a label for your domain name. These options are not available if you allowed multiple placement groups. In the Autoscale property group, select enable Autoscale. Then provide the desired VM instance count ranges, the scale out or scale in CPU thresholds and instance counts to scale out or scale in by. Click Create. Enable Autoscale during provisioning of a VM Scale Set As already said, these steps are basically the same as when setting up a VM Scale Set without Autoscale. The next section should be more interesting when I configure Autoscale for an existing Scale Set.Configure Autoscale on an existing VM Scale Set using the PortalAfter your Scale Set is deployed, you can add Autoscale and scale against a variety of metrics. To set up Autoscale follow these steps: In the Portal navigate to your VM Scale Set. Under the Settings menu, select Scaling. Add the Default Scale Condition or Add A Scale Condition. The default condition will run when no other scale condition match. For example, the Default Scale Condition checks the CPU Usage and if it’s above a certain threshold, it starts additional instances. The Scale Condition could be a scheduled scaling where you start new instances in the morning on workdays and shut them down on the evening. Specify if you want to scale based on a metric or scale to a specific instance count. Click on +Add a rule to add scaling rule. To scale out select within the rule to increase your instance count and to scale in selected decrease your instance count. Adding a rule should be self-explaining and can be done on a variety of metrics. &lt;div class=\"col-12 col-sm-10 aligncenter\"&gt; Configure Autoscale based on CPU usage for your VM Scale Set &lt;/div&gt;&lt;/li&gt; After that add another Scale Condition by clicking on + Add a scale condition.&lt;/ol&gt; Select Scale to a specific instance count and set the instance count to 5. Then select Repeat specific days and select Monday until Friday. If necessary change the Timezone and the Start time or End time to meet your requirements.&lt;div class=\"col-12 col-sm-10 aligncenter\"&gt; &lt;a href=\"/assets/img/posts/2018/03/Configure-scheduled-scaling-for-your-VM-Scale-Set.jpg\"&gt;&lt;img loading=\"lazy\" src=\"/assets/img/posts/2018/03/Configure-scheduled-scaling-for-your-VM-Scale-Set.jpg\" alt=\"Configure scheduled scaling for your VM Scale Set\" /&gt;&lt;/a&gt; &lt;p&gt; Configure scheduled scaling for your VM Scale Set &lt;/p&gt;&lt;/div&gt;## ConclusionThis post explained the difference between scaling out and scaling up. Then, I showed how to scale up in the Azure Portal and with PowerShell. Afterward, I talked about Scale Sets and how they can be used to scale out VMs and how they can be configured to auto-scale. The Autoscale feature can be used on existing Scale Sets but can also be configured during the provisioning process.For more information about the 70-532 exam get the &lt;a href=\"http://amzn.to/2EWNWMF\" target=\"_blank\" rel=\"noopener noreferrer\"&gt;Exam Ref book from Microsoft&lt;/a&gt; and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam &lt;a href=\"/prepared-for-the-70-532-exam/\" target=\"_blank\" rel=\"noopener noreferrer\"&gt;here&lt;/a&gt;." }, { "title": "Perform Configuration Management", "url": "/perform-configuration-management/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-03-06 12:21:11 +0100", "snippet": "In this post, I want to show how to use Windows PowerShell Desired State Configuration (DSC) and the VM Agent to perform various configuration management tasks like remote debugging and at the end, I will show how to use variables in a deployment template.Automate configuration management by using PowerShell Desired State Configuration (DSC) or VM Agent (custom script extensions)The VM Agent gets installed automatically when creating a new VM. It is a lightweight process which can install and configure VM extensions. These extensions can be added through the portal, with PowerShell cmdlets or through the Azure Cross Platform Command Line Interface (Azure CLI)Popular VM extensions are: Octopus Deploy Tentacle Agent Docker extension Chef extensionConfigure VMs with Custom Script ExtensionCustom Script Extensions allow you to automatically download files from Azure store and run PowerShell or Shell scripts to copy files or configure VMs. Open the blade of your Windows Server Under the Settings section, select Extensions The Extensions option under Settings &lt;/li&gt; On the Extensions blade, click + Add on the top This opens a new blade with all available extensions. Select Custom Script Extension and press Create On the install extension blade upload a PowerShell script file which you want to run when the VM starts. The PowerShell script contains only one line to copy a file from C:\\ to the desktop. The entire command is: Copy-Item &#8220;FilePath&#8221; -Destination &#8220;DestinationPath&#8221;. Before I upload the script, I create the file under C:\\. Adding the PowerShell script which will run when the VM starts &lt;/li&gt; Click OK and the script will be downloaded on the Server. When you start your VM, the VM executes the script which opens a Windows Explorer window. &lt;/ol&gt; &lt;/li&gt; &lt;/ol&gt; Copied the file on the desktop using the uploaded script Using PowerShell Desired State Configuration (DSC) DSC is a management platform which was introduced with PowerShell 4.0. It has an easy, declarative syntax which simplifies configuration and management tasks. With DSC it is possible to describe what application resources you want to add or remove based on the current state of a server node. A DSC describes the state of one or more resources. These resources can be the Registry or the filesystem. Tasks you can achieve with a DSC script are: Manage server roles and Windows features Copy files and folders Deploy software Run PowerShell scripts DSC extends PowerShell (4.0 and upwards) with a Configuration keyword which is used to express the desired state of one or more target nodes. Example of a DSC script Custom Resources Many resources are already defined and exposed to DSC. Additionally, it is possible to implement custom resources by creating a PowerShell module. This module includes a MOF (Managed Object Format) file, a script module, and a module manifest. Local Configuration Manager The Local Configuration Manager runs on all target notes and is the engine of DSC. It enables you to do the following tasks: Pushing configurations to bootstrap a target node Pulling configuration from a specified location to bootstrap or update a target node The documentation for the Local Configuration Manager can be found here. Configure VMs with DSC To configure a VM using DSC, you have to create a PowerShell script which describes the desired configuration state of this VM. This script selects the resources which need to be configured and provides the desired settings. After the script is created, there are several methods to run the script when the VM starts. Creating a configuration script To create a configuration script, create a PowerShell file and include all nodes which you want to configure. If you want to copy files, make sure that the file exists at the source and that the destination also exists. Deploy a DSC script Before you can deploy your DSC script, you have to pack it in a zip file. To do that open PowerShell and enter Publish-AzureVMDscConfiguration Filepath -ConfigurationArchivePath DestinationPath. Publish-AzureVMDscConfiguration .\\IIS.ps1 -ConfigurationArchivePath .\\IIS.ps1.zip After the script is prepared, follow these steps to deploy it to your VM: In the Azure Portal open your VM Click on Extensions under the settings menu. On the Extensions blade click + Add, then select PowerShell Desired State Configuration and click Create. On the Install extension blade, upload your zip File, enter the full name for your script and enter the desired version of the DSC extension. Click OK to deploy your script. Deploy the DSC script to your VM Enable remote debugging You can use Visual Studio 2015 or 2017 to debug applications which run on your Windows VM. To do that following these steps: Open the Cloud Explorer in Visual Studio. Expand the node of your subscription and then the Virtual Machines node. Right-click on the VM you want to debug and select Enable Debugging. This installs the debugging extension on the VM and takes a couple minutes. After the extension is installed, right-click on the VM and select Attach Debugger. This opens a new window with a list of all available processes. Select the process you want to debug and click Attach. The probably most useful process is w3.wp.exe. This process only appears if you have a web application running on your server though. Enable remote debugging on your VM in Visual Studio Implement VM template variables to configure VMs When you create a new resource, for example, a VM from the marketplace, it creates a JSON template file. You can write this template by yourself or you can download the template from an existing VM and modify it. Variables can be used to simplify your template which makes it easier to read and easier to change. They also help you with your configuration management since you only have to change one value to change the configuration for your whole template. In this section, I will edit an existing template to show, how variables can be used to simplify a template. Edit an existing template from the Azure Marketplace In the Azure Portal go to the Marketplace. Enter template in the search, then select Template deployment and click Create. On the custom deployment blade select Create a Windows virtual machine. This opens the Deploy a simple Windows VM blade. Click Edit template to open the template. When you scroll down a bit, you will find the &#8220;variables&#8221; section. All variables are configured in this section. Change any of the values or add your own variables. To use the variable use &#8220;[variables(&#8216;yourVariableName&#8217;)]&#8221;. Variables in the deployment template Conclusion In this post, I talked about what Desired State Configuration is and what you can do with it. Then, I showed how to enable remote debugging using Visual Studio 2015 or 2017 and in the last section, I explained how variables in templates work and how they can be used to simplify the template. For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here. " }, { "title": "Deploy workload on Azure ARM virtual machines", "url": "/deploy-workload-on-azure-arm-virtual-machines/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-03-05 14:18:57 +0100", "snippet": "Azure ARM virtual machines can run a lot of different operating systems. In this posts, I will give an overview of these different types and I will also show how to create them in the Azure Portal. This post is part of the Microsoft 70-532 certification exam and contains the information necessary for the Deploy workload on Azure ARM virtual machines part of the exam.Identify workloads that can be deployed on your Azure ARM virtual machinesAzure offers a wide variety of different VMs which you can deploy. These include: Base VMs which run different operating systems like Windows Server, Windows or different versions of Linux (Suse, Ubuntu or Red Hat) Web servers like Apache Tomcat Data science, database and big-data workloads like MS SQL Server, Couchbase or Hadoop Workloads that provide security and protection like intrusion detection systems, firewalls, data encryption or key management Workloads that help developers, for example, Visual Studio or the Java Development KitTo see all available workloads go to the Azure Marketplace. There are besides workloads provided by Microsoft also workloads provided and maintained by the community. The topology that deploys the VM and any supporting infrastructure is described in an Azure Resource Manager (ARM) template.Before deploying a new VM you have to consider the following requirements: CPU RAM Disk storage capacity Disk Performance (IOPS) Networking Availability SecurityCreate a Windows Server VM using the MarketplaceFollowing, I will explain how to create a new Windows Server VM from a pre-built image from the marketplace. Go to the Marketplace, select Compute, then Windows Server, select the desired Version, for example, 2016. After selecting your OS click Create. Provide the following information for your VM: &lt;li style=\"list-style-type: none;\"&gt; Name Disk type (SSD or HDD) User name Password Subscription An existing or new Resource group Location License (if available) &lt;/li&gt; Entering basic data for the new VM On the next blade choose a size for your VM. There are many different configurations with different costs to choose from. After you selected one click Select. Select a VM size according to your needs Leave the optional Settings as they are. After entering all your settings, you get an overview of them in the Summary blade. Click Create to start the deployment process for your VM. Summary of the new VM Creating a Linux or SQL Server VM works the same way. The only difference is that you can provide a public SSH key instead of the password for the Linux VM.Create and provision VMs, including custom VM imagesYou can create a new VM from an existing custom image. The advantage of this approach is that you could create the image and install all your programs on-premise and then upload the image to the cloud. Then you can use it to create as many VMs as you want in whatever data center you want.The VMs can be created in the Azure Portal or using PowerShell.Creating a custom image VM using the Azure PortalTo provision a VM using a custom image, I first have to create a custom image. To do that, I create a new Windows VM from the Marketplace. I repeat the same steps as in the previous chapter Create a Windows Server VM using the Marketplace except that I name the VM Source.After the VM is deployed, I connect myself to the VM using a remote connection. You can download the remote connection when you click on your VM and then click Connect on the Overview blade. Download the remote connection to the VM Connecting to the VM for the first time might take a while. Usually, it is pretty fast but it took me two hours until the remote connection was established once. After I am connected to the VM, I create a new txt file on the desktop. I don’t do anything with this file. This file is to demonstrate that the current image was used to create the new VM after I am finished.The next step is to open the command line as administrator and change the path to %windir%\\system32\\sysprep and then start sysprep.exe. In the System Preparation Tool select Enter System Out-of-Box Experience (OOBE) and check the Generalize check box. In the Shutdown Options select Shutdown and then click OK. This will prepare the VM for cloning and remove all personal account information.Create the imageAfter the image is generalized go back to the Azure Portal. Click on your VM and select Capture from the top menu on the Overview blade. Capture the new image On the Create image blade provide a name for your new image and a Resource group. Then click Create to create your new image. Create the image from your VM Create a new VM from your custom imageThe last step is to create your new VM from the previously created image. To do that click on All resources and select the image you just created. The type is Image and the symbol looks similar to the symbol of a virtual machine. After you selected your image, click on + Create VM on the top of the Overview blade. This opens the Create virtual machine blade where you can enter the configuration of your VM. Create the VM from your image Creating a custom VM using PowerShellTo create a VM using a custom image using PowerShell see the official documentation from Microsoft.Deploy workloads using TerraformTerraform is a templating language which allows the creation of human-readable templates to deploy complete infrastructures in Azure.  To see how you can use it take a look at Microsoft’s documentation.ConclusionI talked about the requirements you should check before you create your Azure ARM virtual machines and then I showed how to create your own Windows Server VM with the Azure Marketplace. After that, I generalized the VM image to create a new VM with my custom image. At the end of this post, I linked to the documentation which explains how custom VMs can be created with PowerShell and how Terraform works.For more information about the 70-532 exam get the Exam Ref book from Microsoft and continue reading my blog posts. I am covering all topics needed to pass the exam. You can find an overview of all posts related to the 70-532 exam here." }, { "title": "How I prepared for the 70-532 Exam", "url": "/prepared-for-the-70-532-exam/", "categories": "Cloud", "tags": "70-532, Azure, Certification, Exam, Learning", "date": "2018-02-23 14:33:48 +0100", "snippet": "Recently I started to prepare for the Microsoft certification exam 70-532 “Developing Microsoft Azure Solutions” using the official Exam Ref book. This post contains a comprehensive list of blog posts with summaries of all needed abilities. I used the Exam Ref book, the official documentation and my personal experience to write these posts. I will constantly update the list until all topics are discussed.How I prepared for the 70-532 examPrior starting to prepare for the exam, I had some knowledge about cloud technologies and the different features of Azure. Most of my skills came from simple trying out features in the Azure portal or from watching Youtube videos.To study more in depth for the exam, I got the Exam Ref from Microsoft. If you get this book make sure that it’s the second edition. The first edition is outdated. For a comprehensive post about every topic see the following list (the list will be updated constantly until it is complete):Create and Manage Azure Resource Manager Virtual Machines Deploy workloads on Azure ARM virtual machines Perform Configuration Management Scale ARM Virtual Machines Design and implement ARM VM Azure Storage Monitor VMs in Azure Manage ARM VM Availability Design and implement Azure DevTest LabsDesign and Implement a Storage and Data Strategy Implement Azure Storage blobs and Azure files Implement Azure Storage Tables, Queues, and Azure Cosmos DB Table API Manage access and monitor storage Implement Azure SQL database Implement Azure Cosmos DB DocumentDB Implement Redis Cache Implement Azure SearchDesign and Implement Azure Compute, Web and Mobile Services Design Azure App Service Web App Design Azure App Service API Apps Develop an Azure App Service Logic App Develop Azure App Service Mobile App Implement API Management Implement Azure Functions and WebJobs Design and Implement Azure Service Fabric apps Design and implement DevOps" }, { "title": "Templated Helper Methods", "url": "/templated-helper-methods/", "categories": "ASP.NET", "tags": "Attribute, Scaffolding, Template", "date": "2018-02-08 23:36:06 +0100", "snippet": "I talked about HTML Helper Methods in my last post. Although it was a pretty lengthy post, I only talked about the basic helper methods. The problem with this approach was that I had to decide which input field I want to render. I want to tell the framework which property I want to be displayed and let the framework decide which HTML element gets rendered. Therefore, I want to talk about templated helper methods today and show how they can be used to easily create views with even less programming to do.Setting up the projectI use the same project as last time. The only changes I made was adding an address and the birthday to the customer class. You can download the code here. Implementation of the Address class Using Templated Helper MethodsThe simplest templated helper method is Html.Editor and the strongly typed version Html.EditorFor. I like the strongly typed version due to it’s IntelliSense support and pass the property name as the parameter. The MVC framework renders the element which it thinks fits best. Using Html.EditorFor to create HTML elements The output can vary depending on what browser you use.  When you go to the devTools and inspect the rendered elements, you will see that the type for CustomerId is number and the type of Birthday is datetime whereas all the others have the type text. This type tells the browser which element it should render.The MVC framework has the following built-in templated helper methods: Helper Example Description Display Html.Display(&#8220;PropertyName&#8221;) Renders a read-only view of the specified model property, choosing an HTML element according to the property’s type and metadata DisplayFor Html.DisplayFor(x =&gt; x.PropertyName) Strongly typed version of the Display helper Editor Html.Editor(&#8220;PropertyName&#8221;) Renders an editor for the specified model property, choosing an HTML element according to the property’s type and metadata EditorFor Html.EditorFor(x =&gt; x.PropertyName) Strongly typed version of the Editor helper Label Html.Label(&#8220;PropertyName&#8221;) Renders an HTML label element referring to the specified model property LabelFor Html.LabelFor(x =&gt; x.PropertyName) Strongly typed version of the Label helper Displaying the Model with Templated Helper MethodsSo far I have used templated helper methods for every property. The MVC framework also offers methods to process an entire model. This process is called scaffolding.The following templated helper methods are available for the scaffolding process: Helper Example Description DisplayForModel Html.DisplayForModel() Renders a read-only view of the entire model object EditorForModel Html.EditorForModel() Displays editor elements for the entire model object LabelForModel Html.LabelForModel() Renders an HTML label element referring to the entire model object With these templated helper methods it is possible to render forms with just a couple lines of code. This makes them easy to read and easy to change. Creating a form using templated helper methods with the model When you look at the result you can see that there are some problems with the output:The Html.LabelForModel method didn’t render anything in the headline, and the role is a textbox instead of a drop-down list. (this time it is not the fault of IE) Additionally, the Address is not rendered and probably I don’t want to display the Id to the user.The solution to this problem is to add attributes to the model class to tell the framework what and how it should render the properties.Using Model AttributesThe MVC framework renders the HTML fields which it thinks fit best. As you saw in the last example, this is not always what you expect or want. I like the templated helper methods because they make the view so simple but I also have to ensure that they render what I want. I can add some metadata to my model to tell the MVC framework how to handle the property. ASP.NET MVC has a variety of built-in attributes which help me to display what I want with the scaffolding process.Using Metadata for Data ValuesAttributes offer a convenient way to tell the rendering engine which datatype it renders. For example, I can tell it that I only want to display the date part of the birthday or that I want to render a password field which masks the user’s input.The ASP.NET MVC framework offers the following datatype attributes: DataType Value Description Date Displays the date part of a DateTime DateTime Displays a date and time (this is the default behavior for System.DateTime values) EmailAddress Displays the data as an e-mail address (using a link (a) element with a mailto href) MultilineText Renders the value in a textarea element Password Displays the data so that individual characters are masked from view PhoneNumber Displays a phone number Text Displays a single line of text Time Displays the time part of a DateTime Url Displays the data as a URL (using an HTML link (a) element) Hide or Display Elements using Metadata Earlier I said that I don&#8217;t want to display the CustomerId to the user. It is common not to display all information, for example, the id or primary key of an element is usually not relevant to the user. To hide a property, I decorate it with the HiddenInput attribute and set the DisplayValue to false. Applying the HiddenInput attribute to the CustomerId I have to set the DisplayValue property to false because otherwise, the MVC framework would render a read-only field. Another approach to hide a property is to exclude it from scaffolding with [ScaffoldColumn(false)]. The problem with the excluding is that it doesn&#8217;t get sent to the view. Therefore if the user returns the view for example after editing some information, the id is not included and so I don&#8217;t know which user was sent back. Using Attributes to display Property Names The scaffolding process displays the name of the property as the label. The problem is that property names are rarely useful to the user. No user wants to read FirstName. A user expects First name. Therefore, I can decorate properties with the Display attribute and a class with the DisplayName attribute. As the parameter, I pass the name which I want to be displayed. Decorating the model properties with the Display attribute When I start the application now, nice names are displayed and also Person in the header will be rendered. Output of the form with the Display attribute for naming the properties Applying Metadata to automatically created Classes It is not always possible to apply attributes to classes because they are automatically generated by tools like the Entity Framework. (Actually, you can add attributes but they will be overridden the next time the class gets updated and therefore generated again). The solution to this problem is to add a partial class of the class you want to extend with the same properties and add the attributes there. To be able to do that the original class has to be partial as well. Fortunately Entity Framework creates partial classes. Then you have to add the MetadataType attribute to the class with typeof(your class) as the parameter. Implementation of the partial customer class with attributes On the screenshot above, I created a partial class and called it CustomerWithAttributes. Then I added all attributes from the Customer class which has attributes. Additionally to these changes, I made the Customer class partial and added the MetadataType attribute to it with the CustomerWithAttributes class as the parameter. Adding the MetaData attribute to the customer class I also removed all attributes from this class since I want to simulate an automatically generated class. If you run your application now, it will look as before. Note that the partial classes have to be in the same namespace. Displaying Complex Type Properties with Templated Helper Methods The next part I want to fix is the display of the Address and its properties. The Address properties weren&#8217;t rendered yet because the EditorFor helper can only operate on simple types such int or string. Therefore it does not work recursively and as a result complex data types get ignored. The reason why the MVC framework is not rendering properties recursively is that it could easily trigger lazy-loading which will result in rendering all elements from the database. To render complex datatype, I have to explicitly tell the MVC framework how to do that by creating a separate call to the templated helper method: Adding a render method for the Address property The only change I had to make was adding the line with EditorFor and all properties of the Address class will be rendered after the previously rendered properties. Conclusion In this post, I refactored the application of my last post to use templated helper methods which enables you to create a view with only a couple of codes. To help the render engine, I showed how to add additional information on what you want to be rendered by applying attributes to the model class. The result is that the view is simple to understand and can be easily extended or changed. For more details about model validation, I highly recommend the books Pro ASP.NET MVC 5 and Pro ASP.NET MVC 5 Plattform. You can find the source code with all examples on GitHub. " }, { "title": "Helper Methods in ASP.NET MVC", "url": "/helper-methods-in-asp-net-mvc/", "categories": "ASP.NET", "tags": "Form, Helper, HTML", "date": "2018-02-07 18:57:46 +0100", "snippet": "Writing HTML forms can be a tedious task. To help the developers, the ASP.NET MVC framework offers a wide range of helper methods which make creating an HTML form way easier.ASP.NET MVC offers for every feature the option to use your own implementation. In this post, I will only talk about the built-in helper methods and not about custom helper methods.Setting up the project for Model ValidationI created a new ASP.NET MVC project with the empty template and add folders and core references for MVC. Setting up the ASP.NET MVC project In the next step, I create a model classes, Customer and a Role enum. Implementation of the Customer class Implementation of the Role enum In the Home controller, I create two actions, both with the name CreateCustomer. One sends a new Customer object to the view and the other action takes an HTTP post request with the customer. This is the standard approach to deal with HTML forms. Actions in the Home controller Using built-in Helper MethodsThe MVC framework offers many helper methods out of the box to manage your HTML code for elements. First, I want to show you how to build a form without any helper methods and then extend this form piece by piece. View with a simple HTML form This is a very simple view. I added Bootstrap and some simple CSS. Usually, I would use a layout file but since the focus of this post is on helper methods, I think this approach is fine. Note that I have to set the name property of every input field. This name corresponds to a property of the model. If I don’t set the name, the model binder wouldn’t be able to bind the data.Creating a formThe most used helper method is Html.BeginForm. This helper creates the form tag and has 13 different versions. You can use either @{Html.BeginForm();} to create an open form tag and @{Html.EndForm();} to create a closing form tag or you can use the simpler version @using(Html.BeginForm()){ and add the closing bracket } at the end of the form. I think I never saw anything else than the approach with the using since it’s so much simpler. Creating a form using Html.BeginForm() Here are the different overloaded versions of the BefginForm helper method: Overload Description BeginForm() Creates a form which posts back to the action method it originated from BeginForm(action, controller) Creates a form which posts back to the action method and controller, specified as strings BeginForm(action, controller, method) As for the previous overload, but allows you to specify the value for the method attribute using a value from the System.Web.Mvc.FormMethod enumeration BeginForm(action, controller, method, attributes) As for the previous overload, but allows you to specify attributes for the form element an object whose properties are used as the attribute names BeginForm(action, controller, routeValues, method, attributes) Like the previous overload, but allows you to specify values for the variable route segments in your application routing configuration as an object whose properties correspond to the routing variables Source 1, Source 2 Specifying the route for a form When you use the BeginForm helper method, you leave the routing to the MVC framework. It takes the first route in the routing configuration. If you want to use a specific route for your form, use BeginRouteForm instead. To demonstrate the BeginRouteForm helper method, I added a new route in the RouteConfig. I added the new route before the default one and named it MyFormRoute. New route for BeginRouteForm helper method In the BeginRouteForm method, I pass the route name as the parameter. The MVC framework now finds the first route and routes to the CreateCustomer action in the Home controller. (I know this behavior isn&#8217;t very clever but it shows how BeginRouteForm works) Using BeginRouteForm with the route name as parameter The BeginRouteForm helper method also has several overloaded versions which enable you to specify the form element in more detail. Using HTML Input Helper Methods In the previous example, I create the HTML code by hand and assigned the model data to the value attribute. The ASP.NET MVC offers several HTML Helper methods which make creating forms easier. The following input helper methods are available out of the box: HTML Element Example Check box Html.CheckBox(&#8220;myCheckbox&#8221;, false) Hidden field Html.Hidden(&#8220;myHidden&#8221;, &#8220;val&#8221;) Radio button Html.RadioButton(&#8220;myRadiobutton&#8221;, &#8220;val&#8221;, true) Password Html.Password(&#8220;myPassword&#8221;, &#8220;val&#8221;) Text area Html.TextArea(&#8220;myTextarea&#8221;, &#8220;val&#8221;, 5, 20, null) Text box Html.TextBox(&#8220;myTextbox&#8221;, &#8220;val&#8221;) I replace the HTML input elements with Html.TextBox. The syntax might feel strange in the beginning but once you got used to it, you won&#8217;t go back to writing plain HTML forms. Replacing the HTML input fields with MVC HTML input helper methods Generating HTML elements from a view model Previously, I showed you how to create input elements using HTML helper methods. The problem with this approach is that I had to specify the name of the element and had to make sure that this name fits the view model property which I want to bind. If I had misspelled a name, the MVC framework wouldn&#8217;t be able to bind the property to the element. The solution to this problem is another overloaded version of the helper method which takes only a string as a parameter. The string is the property name and the MVC framework automatically creates the name attribute for you. Generating input elements from a property Note that I had to use null as the second parameter to add a CSS class. If you don&#8217;t need CSS you only need the string parameter. This string parameter is used to search the ViewBag and then the Model for a corresponding property. For example for the first input element with personId the MVC framework searches for ViewBag.personId and @Model.personId. It finds the property in the model and therefore is able to bind it. As always, the first value that is found is used. Using strongly typed Input Helper Methods Each input helper method has also a strongly typed version which works with lambda expressions. The view model is passed to the helper method and you can select the needed property. The strongly typed input helper methods can only be used in a strongly typed view. Creating input element using strongly typed input helper methods I prefer this approach because it is less likely that you mistype a property name and with the lambda expressions you also have IntelliSense support. Creating Select elements So far, I only talked about input elements. The MVC framework also offers helper methods for drop-down lists. HTML helper methods for select elements The difference between a normal drop-down list and multiple-select is that the drop-down list allows only one element to be selected. Creating a drop-down list with HTML helper methods The drop-down list displays all available roles. the DropDownList and ListBox HTML helper methods work with IEnumerable. Therefore, I have to use GetNames to get all the names of the roles. Conclusion Today I talked about different approaches on how to create input fields with HTML helper methods which are built-in into the ASP.NET MVC framework. With all this new knowledge, it&#8217;s time to go out and write some code 😉 For more details about model validation, I highly recommend the books Pro ASP.NET MVC 5 and Pro ASP.NET MVC 5 Plattform. You can find the source code with all examples on GitHub. " }, { "title": "Model Validation in ASP.NET MVC", "url": "/model-validation-in-asp-net-mvc/", "categories": "ASP.NET", "tags": "Ajax, Attributes, Javascript, Model Validation", "date": "2018-02-03 18:33:31 +0100", "snippet": "Model validation is the process of checking whether the user input is suitable for model binding and if not it should provide useful error messages to the user. The first part is to ensure that only valid entries are made. This should filter inputs which don’t make any sense. This could be a birth date in the future or an appointment date in the past. As important as checking for valid data is to inform the user about the wrong input and help him to enter the information in the expected form. Without any help, it can be frustrating and annoying for the user which might end up in losing a potential customer.To help the developer, ASP.NET MVC provides several possibilities for model validation.Setting up the project for Model ValidationI created a new ASP.NET MVC project with the empty template and add folders and core references for MVC. Setting up the ASP.NET MVC project for model validation Then I create a simple Home controller with two actions. The first action returns a view to the user to enter customer information and the second action processes the user input. In a real-world solution, the customer information would be stored probably in a database. In this example, I don’t do anything with it except passing to a second view to display the information. Implementation of the Home controller To work with customers, I need to implement a customer model. I want to keep the customer as simple as possible and therefore the class only has basic properties. Implementation of the customer class I keep the views simple too. The RegisterCustomer view takes a name, birthday and has a checkbox to accept the terms and conditions. Register new customer view Right now the user can enter whatever he wants and the application accepts it. To create a valid customer the user have to provide a name, a birth date (in the dd/mm/yyyy format) in the past and he must accept the terms and condition.To enforce these requirements, I use model validation.Explicitly validating a ModelOne possible approach of checking if the model is valid is checking directly in the action. Validating the model directly in the action I check if every field contains the expected value. If not, I add a model error to the Modelstate. After I checked every attribute, I return the RegisterComplete view if the Modelstate is valid and if not I return the input view again.I added some CSS to the HTML which colors the border of the element with the model error in red. CSS to highlight model error in the view (Source) Highlighted errors in the view Displaying error messagesHighlighting input fields which contain wrong values is a nice beginning but it doesn’t tell the user what is wrong. The MVC framework provides several helper methods to display useful error messages to the user. The simplest one is Html.ValidationSumary(). Displaying error messages with Html.ValidationSummar This helper method adds all error messages above the form. If there are none, nothing will be rendered before the form. Displaying error messages to the user The Html.ValidationSummary helper has several overloaded implementations. See the following tables for the different versions: Overloaded Method Description Html.ValidationSummary() Generates a summary for all validation errors Html.ValidationSummary(bool) If the bool parameter is true, then only model-level errors are displayed. If the parameter is false, then all errors are shown. Html.ValidationSummary(string) Displays a message (contained in the string parameter) before a summary of all the validation errors. Html.ValidationSummary(bool, string) Displays a message before the validation errors. If the bool parameter is true, only model-level errors will be shown. Model level validation messagesAnother approach to validate a model is model level validation. This is used when you have to ensure that two or more properties interact correctly. I know it’s a stupid example but for whatever reason, you don’t allow a customers name to be Wolfgang with his birthday yesterday. Adding a model level error to the ModelState is also achieved by using the AddModelError with the difference that the first parameter is an empty string. Additionally, you have to pass true as the parameter for the ValidationSummary in the view. Model level error implementation Displaying the model level error message Property level validation messagesDisplaying all error messages on the top of the form might be ok if the form is as short as mine but if the form has 20 rows, the user will be confused with all the error messages. The solution for this is the ValidationMessageFor HTML helper. This method takes a lambda expression with the name of the property of which it should display the error message. Implementation of the ValidationMessageFor helper for every input field Displaying an error message for every input field Validation using the Model BinderThe default model binder performs validation during the binding process. If it can’t bind a property, it will display an error message. For example, if you leave the birthday empty, the model binder will display a message prompting the user to enter a birthday. Error message by model binder because of empty birthday field The model binder also displays an error message if you try to enter an invalid value, for example, a string as birthday. Error message by model binder because of the wrong value in the birthday field Modern browsers like Chrome or Edge don’t even let the user enter a string. They offer a date picker to help the user entering the right value. The Internet Explorer, on the other hand, doesn’t offer this feature (surprise, surprise IE sucks).Validation using MetadataThe MVC framework enables you to add attributes to properties of a model. The advantage of this approach is that the attributes and therefore the model validation are always enforced when the model is used. Using built-in validation attributes in the model class I applied the Required attribute with and without a specific error message and the Range attribute to ensure that the checkbox is checked. If there is no custom error, a default error will be displayed. It feels unnatural to use the Range attribute for a checkbox but the framework doesn’t provide an attribute for bool operations. Therefore, I use the Range attribute and set the range from true to true. ASP.NET MVC provides five useful built-in attributes for validation: Attribute Example Description Compare [Compare(&#8220;Password&#8221;)] Two properties must have the same value. This is useful when you ask the user to provide the same information twice, such as an e-mail address or a password. Range [Range(5, 10)] A numeric value must lie between the specified minimum and maximum values. To specify a boundary on only one side, use a MinValue or MaxValue constant—for example, [Range(int.MinValue, 10)]. RegularExpression [RegularExpression(&#8220;regex&#8221;)] A string value must match the specified regular expression pattern. Note that the pattern has to match the entire user-supplied value, not just a substring within it. By default, it matches case sensitively, but you can make it case insensitive by applying the (?i) modifier—that is, [RegularExpression(&#8220;(?i)regex&#8221;)]. Required [Required] The value must not be empty or be a string consisting only of spaces. If you want to treat whitespace as valid, use [Required(AllowEmptyStrings = true)]. StringLength [StringLength(12)] A string value must not be longer than the specified maximum length. You can also specify a minimum length: [StringLength(12, MinimumLength=8)]. Validation using a custom property attribute The ASP.NET MVC framework is highly custom able and therefore it is easy to implement your own attribute for validating a property. I don&#8217;t like the Range attribute for a checkbox and therefore I will create my own called MustBeTrue. As I already said, implementing your own attribute is pretty simple. You only have to create a new class which derives from ValidationAttribute and then overrides the IsValid method. Implementation of a custom attribute for property validation Now I can replace the Range attribute with the MustBeTrue attribute on the property. Applying my custom attribute to a property As you can see, I can still use the ErrorMessage property to display the same error message as before. Implementing a custom property by deriving from a built-in property You can also derive from a built-in validation property, for example, Required, and extend its functionality. I create a new validation property which derives from RequiredAttribute and override the IsValid method again. Then I return IsValid from the base class and my additional check. In this case, I want to make sure that the date is in the past. Extending the Required attribute with my custom validation Performing client-side validation All previous validation techniques are for server-side validation. This means that the user sends the data to the server, then it gets validated and sent back. Sending data back and forth uses a lot of bandwidth which can lead to problems if you have a very popular application or if the user has a slow connection, for example on his mobile phone. Therefore it would be better to do some validation on the client even before sending the data to the server. This reduces the amount of data sent back and forth and also increases the user experience. Setting up client-side validation To enable client-side validation change the values of ClientValidationEnabled and UnobtrusiveJavaScriptEnabled in the web.config to true. Configuring web.config for client-side validation The next step is to install the following NuGet packages: jQuery jQuery.Validation Microsoft.jQuery.Unobtrusive.Validation The last step is to add these three Javascript files to your layout or view. Using client-side validation Using client-side validation is pretty simple. Just add validation properties to your model, for example, Required or StringLength. Adding validation attributes to the model for client-side validation Error message from client-side validation Thanks to client-side validation the user gets informed that the name has to be at least two characters long even before the form is sent to the server. Client-side validation is a great feature. However, you still have to do server-side validation to ensure the integrity of the data.Performing remote validationRemote validation is a mix of server-side and client-side validation. It is often used to check whether an entered username or email is already taken. The check is performed in the background using ajax without the user doing anything. It is a pretty lightweight request but it still should be used for only some input fields since every input is sent back to the server which leads to a lot of requests.Implementing remote validationImplementing remote validation consists of two steps. The first step is to implement an action in your controller which returns a JsonResult and takes one string parameter. Implementation of the remote validation in the controller In the action, I check if the customer name is Wolfgang. If it is Wolfgang, I return an error message. Note that the parameter is Name with an upper case N. I have to do this because the parameter has to match the name of the input field you want to check. I also have to allow Json GET requests because they are disallowed by the MVC framework by default.The second step is to add the Remote attribute to the Name property of the Customer class. Additionally, I add the validation method name and the controller. Adding the remote attribute to the property of the model class That’s it. When you enter a name, you will get an error message as soon as you have finished entering Wolfgang. If you change one letter, the error message will disappear. Remote validation of the customer name ConclusionIn this post, I showed different approaches on how to perform model validation. Model validation is necessary to ensure data integrity and check whether the user entered the valid data.For more details about model validation, I highly recommend the books Pro ASP.NET MVC 5 and Pro ASP.NET MVC 5 Plattform.You can find the source code on GitHub." }, { "title": "Model Binding in ASP.NET MVC", "url": "/model-binding-in-asp-net-mvc/", "categories": "ASP.NET", "tags": "Model Binding, MVC", "date": "2018-01-22 22:53:33 +0100", "snippet": "ASP.NET MVC creates objects using the model binding process with the data which is sent by the browser in an HTTP request. The action method parameters are created through model binding from the data in the request.Setting up the projectI created a new ASP.NET MVC project with the empty template and add folders and core references for MVC. &lt;img loading=\"lazy\"title=\"Setting up the Model Binding ASP.NET MVC project\" src=\"/assets/img/posts/2018/01/Set-up-project.jpg\" alt=\"Setting up the Model Binding ASP.NET MVC project\" /&gt; Setting up the ASP.NET MVC project Then I create a simple Home controller and a view to display some information about customers. Home controller with two actions for working with customers View to display a customer I also installed the Bootstrap NuGet to make the forms look a bit nicer.How Model Binding worksModel binding is a simple way to connect C# code with an HTTP request. Most MVC frameworks use some form of model binding to get the data from the request for an action. If you used MVC before, it is very likely that you already used model binding, even if you didn’t realize it.Action invokers rely on model binders to bind the data from the request to the data of the C# code. When you use a parameter for an action, model binders generate these parameters before an action is invoked. This process starts after the request is received and is processed by the routing engine. Model binders are defined by the IModelBinder interface.There can be several model binder in an MVC application and you can also create your own. In this post, I will only talk about the built-in binder, DefaultModelBinder. Displaying customer with id 1 Searching parameter for Model BindingOn the screenshot above, you can see that I passed a parameter in the URL. After the MVC framework received the process and did the routing, the action invoker examines the Index method and finds an int parameter. Then the model binder for int calls its BindModel method to bind the value from the URL to the method parameter. The MVC framework searches four locations for a suiting parameter. If one is found, the search is finished and the value is processed. The locations for the search are: Name Description Request.Form Values provided by the user in HTML form elements RouteData.Values The values obtained using the application routes Request.QueryString Data included in the query string portion of the request URL Request.Files Files that have been uploaded as part of the request In the example from above the model binder searches Request.Form[“id”] and then RouteData.Values[“id”]. The needed data can be found in the route information and therefore the search is finished.Note that I am using the default route. It is also important that the variable name in the route and the parameter name are the same. If I named the parameter customerId, the model binder wouldn’t be able to match the id with the customerId.Binding primitive data typesWhen the model binder encounters a primitive data type, it tries to convert it into the needed type. If the conversion fails, an exception message is displayed because int is not nullable (except you handle the exception as I described in Built-in Filter in ASP.NET MVC) Converting exception leading to yellow screen of death You can prevent this from happening but you have to make sure that your code can handle the id if it is null. Nullable int parameter An even simpler alternative is to use default parameter. Default value for the action parameter If the model binder can’t bind the user input to the parameter, the default value is used. Displaying default customer after casting failed Default parameters prevent the application from crashing if the model binding process fails but don’t forget if the value supplied by the user is valid for your application. For example, there is probably no customer with the id -1.Binding complex data typesIf the DefaultModelBinder class encounters a complex type in the parameter, it uses reflections to get the public properties and then binds to each of them in turn. Complex data type Customer as action parameter If you use HTML Helper methods, the helper sets the name attributes of the element to match the format that the model binder uses. Name set by HTML helper Using custom prefixesSometimes you don’t want to bind the data to the type the HTML generates for you. This means that the prefixes containing the view won’t correspond to the structure that the model binder is expecting and therefore your data won’t be processed properly. For example, I have some address information in a form and pass the form to an action. This action only needs some properties and therefore I create a new class called AddressShort. New action for address details If you pass the Customer class with the Address property to the action, it can’t be passed and the AddressShort object will only contain null objects. The values are null because the name attributes have the prefix Address in the HTML form and the model binder is looking for this type when trying to bind the AddressShort type. You can fix this by telling the model binder which prefix it should look for with the Bind attribute in the action. The Bind attribute in the action I am not a big fan of this syntax because I think it makes the code messy but it is an easy way to achieve the desired behavior. If you call the DisplayAddressShort action from the CreateCustomer action, the AddressShort object will contain the country and city of the new customer.Binding only selected propertiesSometimes you don’t want the user to see sensitive data. You could hide the information in the HTML or create a new view model and only send the information you want to display. A simpler solution is to tell the model binder not to bind the properties which you don’t want to display. You can tell the model binder to not bind a property by using the Exclude attribute in the action. Action excluding the city from the binding process On the screenshot above, you can see that I excluded the city from being bound. Another approach would be to include the properties I want to bind with the Include attribute. You can also use the Include and Exclude property in a class. On the following screenshot, I show how to use the Include property in the model class. Applying the Include property in a class If you have the Bind attribute in the class and in the action, it only binds if neither the class nor the action excludes a property.Binding to Arrays and CollectionsModel binding arrays and collections are supported by the model binder and can be achieved very easily.Binding to ArraysTo demonstrate how binding to an array works, I create a new action which takes a string array as parameter: Action with an array parameter The model binder searches for all items with the name attribute countries and then create an array containing these values. Since it is not possible to assign a default value to an array, you have to check in the code whether the array is null. In this example, I let the user enter three countries, pass them as an array into the action and then return the array to display it in the view. It is important that all text boxes which take countries have the name set to countries which is the same name as the parameter. If these two don’t match, the model binder can’t bind the values to the parameter.Binding to collectionsBinding to collections works as binding to arrays. I create a new action and let the user enter cities this time. The only difference is that the parameter is of type IList: Action with a list parameter Again, I check whether the list is null and then pass the values to the view to display them. I also changed the name of the text boxes to cities to match the list parameter.Binding collections of complex data typesBinding to a collection of a complex data type is not different than binding to a normal collection except the naming of the items is a bit different. The names start with the index in square brackets followed by a period and the name of the property. For example [0].City Names starting with [0] to bind to a collection After the values are sent to the action, the model binder binds the objects to the list. Starting with all items starting with [0] are added as the first object, all items starting with [1] as the second item and so on.ConclusionIn this post, I showed how the default model binder works and how you can bind primitive and complex data types. I also explained how to bind to arrays and collection.For more details about model binding, I highly recommend the books Pro ASP.NET MVC 5 and Pro ASP.NET MVC 5 Plattform.You can find the source code on GitHub." }, { "title": "ASP.NET MVC Bundles", "url": "/asp-net-mvc-bundles/", "categories": "ASP.NET", "tags": "CSS, Javascript, Minification, MVC, Optimization", "date": "2018-01-21 00:19:37 +0100", "snippet": "The Bundles feature is built-in into the MVC framework and helps to organize and optimize CSS and Javascript files. In this post, I will show what bundling is, how to use it and what effects it has on the performance of your application.Setting up the projectTo show how bundles word, I create a new empty ASP.NET MVC project with the MVC folders. Then I install the following NuGet packages: Bootstrap jQuery jQuery.Validation Microsoft.jQuery.Unobtrusive.Validation Microsoft.jQuery.Unobtrusive.AjaxThese CSS and Javascript libraries are often used, so they represent a real-world example. Then I create a simple view which uses at least one of the features of each package so that there are several files which need to be loaded. Adding CSS and Javascript files Analyzing the network loadYou can analyze the network load with every browser by pressing the F12 key. This opens the dev tools which shows some useful information for development and debugging. Switch to the Network tab and disable Caching. Then reload the website. This lists all files requested and also displays a summary of the number of files transferred and the number of bytes. Network tab in the Chrome DevTools   Summary of the requests and transferred data As you can see there were quite some files transferred for such a simple page. In total nine requests were sent and 649 KB of data. This is a lot of data for such a simple page and could be a problem when the page grows and hosts more content. The solution for too many requests and too many sent data is bundling.What is Bundling?In the bundling process, the ASP.NET MVC framework minifies Javascript and CSS files to reduce the file size and therefore the amount of data which needs to be sent to the browser. The reduction of the file size is achieved by removing white spaces and shortening variable and method names. Often these minified files are already downloaded when installing a NuGet package, for example, while installing jQuery, the Packet Manager also installed the minified version. You can detect such a file at the ending .min.js. Due to the minification, these files are hardly human readable.Setting up BundlesThe first step to use Bundles is installing the Microsoft.AspNet.Web.Optimization packages from NuGet. Then add the BundleConfig class to the App_Start folder. You don’t have to do this if you use the MVC template when creating a new project. Then Visual Studio automatically creates the class and installs the NuGet package for you.The BundleConfig class has one static method,  RegisterBundles which takes a BundleCollection as parameter. Add your CSS files by adding a new StyleBundle to the BundleCollection and the Javascript files by adding a new ScriptBundle to the BundleCollection. Adding CSS and Javascript Bundles to the BundleConfig class You can add files with the full name or you could wildcards if you want to use all files from a folder, for example, you could add all Javascript files from the Scripts folder with “~/Scripts/*.js”. Note that I used the version variable for jQuery. This means that every jQuery version in the Scripts folder gets included. The advantage is that I can update jQuery and don’t have to think about adding the version number in the Bundle. The downside is that if I have several jQuery files in the folder, all get loaded.If one file depends on the other one, make sure to add them in the right order. For example jQuery.validate needs jQuery, therefore the jQuery file needs to be added first.Using Bundles in ASP.NET MVCAfter creating the Bundles, I have to add the RegisterBundle method in the Global.asx file to make sure it is called at the start of the application. To register the RegisterBunle method in the Global.asx file add the following line of code: BundleConfig.RegisterBundles(BundleTable.Bundles); Registering the Bundles in the Global.asax file The next step is to add the System.Web.Optimization namespace to the web.config viel in the Views folder. Adding the System.Web.Optimization namespace to the web.config file The last step is adding the Bundles to your layout view and enabling CSS and Javascript optimization in the web.config file. To do that use Styles.Render and Scripts.Render with the name of the Bundle you provided while creating it. Adding the Bundles to the layout view Then set the value of the debug attribute in the compilation tag in the web.config to false. If this is set to true, the browser requests all the files with separate requests and doesn’t use bundling. Setting debug to false in the web.config In theory, fewer files get requested and the transferred data size is smaller too now. To test this I start the application and open the DevTools again. Then I reload the page. Network tab in the Chrome DevTools after bundling As you can see only 3 requests and only 272 KB were sent. The request was finished after 53 ms instead of 127 ms without bundling.ConclusionIn this post, I showed how to optimize your CSS and Javascript file by minimizing them using the ASP.NET MVS Bundling feature. With this feature, the number of requests, transferred data and time to finish got significantly decreased.For more details about how bundles work, I highly recommend the books Pro ASP.NET MVC 5 and Pro ASP.NET MVC 5 Plattform.You can find the source code on GitHub." }, { "title": "Built-in Filter in ASP.NET MVC", "url": "/built-filter-asp-net-mvc/", "categories": "ASP.NET", "tags": "Action, Attribute, Controller, filter, MVC", "date": "2018-01-20 16:53:11 +0100", "snippet": "Filters provide a simple and elegant way to implement cross-cutting concerns in your ASP.NET MVC application. Filter achieve this by injecting code into the request processing. Examples of cross-cutting concerns are logging and authorization.Five Types of FilterASP.NET MVC has five types of filter built-in which allow you to implement additional logic during the request processing. Filter Type Interface Default Implementation Description Action IActionFilter ActionFilterAttribute Runs before and after the action method Authentication IAuthenticationFilter None Runs first, before any other filters or the action method and can run again after the authorization filters Authorization IAuthorizationFilter AuthorizeAttribute Runs second, after authentication, but before any other filters or the action method Exception IExceptionFilter HandleErrorAttribute Runs only if another filter, the action method, or the action result throws an exception Result IResultFilter ActionFilterAttribute Runs before and after the action method Before the framework invokes an action, it inspects the method definition to see if it has attributes. If the framework finds one, the methods of this attribute are invoked. As you can see, the ActionFilterAttribute is implemented by IActionFilter and IResultFilter. This class is abstract and enforces you to implement it. The AuthorizeAttribute and HandleErrorAttribute classes already contain useful features therefore you don’t have to derive from them.Applying a Filter to an Action or ControllerIt is possible to apply one or more filters to an action or controller. A filter which is often used is the Authorize attribute. If the Authorize attribute is applied to an action, only users which are logged in can invoke this action. If the attribute is applied to the controller, it applies to all actions. Apply filter to action and controller On the example above, I applied the Authorize attribute to the controller which means that every action of this controller can be invoked only by logged in users. Additionally, I applied the attribute to the DoAdminStuff and set the Roles to Admin which means that only users which have the admin role are allowed to invoke this action. This example also shows that some attributes can have a parameter to specify the action.Applying Authorization FiltersIn the last section, I already used one of the two attributes of the Authorize attribute. The available properties are:  Name Description Users Comma-separated usernames which are allowed Roles Comma-separated role names If several roles are declared, then the user must have at least one of these roles.Applying Authentication FiltersAuthentication filters run before any other filter and can also run after an action has been executed but before the ActionResult is processed.The IAuthenticationFilter InterfaceThe Authentication filter implements IAuthenticationFilter which implements two methods: void OnAuthentication(AuthenticationContext context); void OnAuthenticationChallenge(AuthenticationChallengeContext context);The OnAuthenticationChallenge method is invoked whenever a request fails the authentication or authorization process. The parameter is an AuthenticationChallengeContext object which is derived from ControllerContext. The AuthenticationChallengeContext class has two useful properties: Name Description ActionDescriptor Returns an ActionDescriptor that describes the action method to which the filter has been applied Result Sets an ActionResult that expresses the result of the authentication challenge Applying Exception FiltersException filters are only executed if an unhandled exception has been thrown when invoking an action. An exception can be thrown by: Another filter The action itself During the action result executionMicrosoft provides the HandleErrorAttribute for exception handling. This class implements the IExceptionFilter interface. The HandleErrorAttribute class has three properties: Name Type Description ExceptionType Type The exception type handled by this filter. It will also handle exception types that inherit from the specified value but will ignore all others. The default value is System.Exception, which means that, by default, it will handle all standard exceptions. View string The name of the view template that this filter renders. If you do not specify a value, it takes  a default value of Error, so by default, it renders /Views// Error.cshtml or /Views/Shared/Error.cshtml. &lt;/td&gt; &lt;/tr&gt; Master string The name of the layout used when rendering this filter’s view. If you do not specify a value, the view uses its default layout page. &lt;/table&gt;&lt;/div&gt;As summary: When an unhandled exception of the type specified by ExceptionType is encountered, this filter will render the view specified by the View propertyIf an exception occurs in your development environment, you will get the famous yellow screen of death. To prevent this from happening, you have to set the customsError attribute to the web.config file. CustommError in web.config The customError attribute redirects the request to the location specified at the defaultRedirect attribute. The default value for the mode attribute is RemoteOnly, which means that all local requests see the yellow screen of death.### Applying the HandleError attributeApplying the HandleError attribute to an action enables you to prevent the application from crashing and also enables you to provide some useful error message and help to the user. You can specify an ExceptionType and then send the request to a specific view and provide some information about what went wrong. Apply HandleError on an action On the screenshot above, you can see two different versions of how to use the HandleError attribute. The first line with the attribute only would redirect to /Error/ErrorMessage because I defined this destination in the defaultRedirect attribute in the web.config. This page can only show a generic error message which might not help the user at all.Therefore you can also specify which exception might happen and then redirect the request to a specific view. In the example above, I redirect all ArgumentOutOfRangeException to the ErrorMessage view. This view can be either in the view folder of the controller or in the shared view folder.### Displaying information with the HandleErrorInfo view modelThe HandleError attribute passes the HandleErrorInfo view model to your view. The properties of the HandleErrorInfo view model are: Name Type Description ActionName string Returns the name of the action method that generated the exception ControllerName string Returns the name of the controller that generated the exception Exception Exception Returns the exception In the ErrorMessage view, I can use this view model to give the user a proper error description and provide some help to solve the problem. Helpful error message in the view I know that my error message is not the best and I bet that you can come up with a better one. I also provided a link, so the user can easily return to the page where he came from.## Applying Action FiltersAction filters can be used for anything. I use them for performance checks to find problems with the execution of actions.### Using the IActionFilter interfaceThe IActionFilter interface has two methods: * void OnActionExecuting(ActionExecutingContext filterContext) * void OnActionExecuted(ActionExecutedContext filterContext)The OnActionExecuting method is called before an action method is invoked. The parameter, ActionExecutingContext has two properties: Name Type Description ActionDescriptor ActionDescriptor Provides details of the action method Result ActionResult The result for the action method. A filter can cancel the request by setting this property to a non-null value The OnActionExecuted method is called after the action is completed and has 5 properties: Name Type Description ActionDescriptor ActionDescriptor Provides details of the action method Canceled bool Returns true if the action has been canceled by another filter Exception Exception Returns an exception thrown by another filter or by the action method ExceptionHandled bool Returns true if the exception has been handled Result ActionResult The result for the action method; a filter can cancel the request by setting this property to a non-null value ### Implementing an Action FilterI created a new class called MyActionAttribute which derives from FilterAttribute and IActionFilter. In the OnActionExecuting method, I check if the browser is Chrome. If the request comes from Chrome, I return an HttpUnauthorizedResult. Again, this is not the most useful implementation but I think you get what you can do with this method. For now, I don&#8217;t implement anything in the OnActionExecuted method. This is no problem. Just be careful because Visual Studio throws a NotImplementedException if you let Visual Studio create the method. The action filter class implementation To use my new filter, I apply it to the CheckFilter action. Note that you don&#8217;t have to write MyActionAttribute when applying it as attribute. MyAction is enough. Applying the action filter attribute to an action When you call this action from Chrome, you can see that the access is denied for the user. Result of the action filter &nbsp; Unauthorized result in Chrome ### Implementing the OnActionExecutedMethodAs I already mentioned, I often use action filter to measure the execution time of an action. To do that, I start a Stopwatch in the OnActionExecuting method and stop it in the OnActionExecuted method. Afterwards, I print the execution time. Implementation of the PerformanceAction attribute To see different results, I added Thread.Sleep with a random number, so that the execution will take somewhere between some milliseconds and two seconds. Applying the PerformanceAction attribute The print of the execution time is printed before the content of the action because the action filter is executed before the result of the action is processed. Result of performance test with the action filter ## Applying Result FiltersThe IResultFilter which is implemented by result filters has two methods: * void OnResultExecuting(ResultExecutingContext filterContext) * void OnResultExecuted(ResultExecutedContext filterContext)The OnResultExecuting method is called after an action has returned an action result but before this result is executed. The OnResultExecuted method is called after the action result is executed. The ResultExecutingContext and ResultExecutedContext parameter has the same properties as the action filter parameter and also have the same effects.### Implementing a Result FilterTo demonstrate how the result filter works, I repeat the performance test but this time I measure the time between the start and the end of the execution of the ActionResult. To do that, I implement the IResultFilter and work with the OnResultExecuting and OnResultExecuted method. Implementation of the ResultAction attribute The next step is applying the filter to an action and call the action to see the result. Applying the result filter attribute to an action When you look at the output, you see a difference in the result of the action filter output. The print of the measured time is beneath the output of the action. This behavior is caused by the fact that the result filter is processed after the ActionResult. Therefore the result is printed and then afterward the measured time is added. Result of performance test with the action filter ## Filtering without FilterIn the previous examples, I showed how to create implementations of filters by implementing the desired interface. The Controller class already implements the IAuthenticationFilter, IAuthorizationFilter, IActionFilter, IResultFilter, and IExceptionFilter interfaces. It also provides empty virtual implementations for each of the methods. Knowing this, you can override the desired methods directly in the controller and achieve the same outcome as the filter classes did. Applying filtering in a controller without filter attributes If you call the Index action of the FilteringWithoutFilter controller, you will see the same result as previously with the attributes.&nbsp; Result of performance test with the action and result filter I have to admit that I am not a big fan of this approach. The big advantage of ASP.NET MVC is Separation of Concerns. When using filtering within a controller, you lose this advantage. The only time when it might make sense to use filtering directly in a controller is when this class is a base class for other controllers.I recommend using the attribute approach.### Simply combining Action and Result FilterIf you look closely, you can see that I combine the action filter method OnActionExecuting and the result filter method OnResultExecuted. You can do this yourself by implementing the IActionFilter and IResultFilter interface or by deriving from ActionFilterAttribute which implements both interfaces. The advantage of deriving from ActionFilterAttribute is that you don&#8217;t have empty methods which are not implemented.## Applying Global FiltersIf you want to apply filter to all your actions, you can use global filters. If you are using the MVC template, the framework already creates the FilterConfig.cs file under the App_Start folder. Since I didn&#8217;t use the template, I create the file myself. This class has one static method, RegisterGlobalFilters. The FilterConfig.cs is pretty similar to the RouteConfig.cs. To add a new filter, add it to the filters collection with add. Register a filter globally in FilterConfig.cs Note that the namespace is Filter, not Filter.App_Start.The HandleErrorAttribute will always be defined as global filter in ASP.NET MVC. It is not mandatory to define the HandleErrorAttribute as global filter but it is the default exception handling and will render the /Views/Shared/Error.cshtml view if an exception happens.To register a filter a, you have to pass an instance of the filter class. The name contains the Attribute prefix. If you use the class as attribute, you can omit the Attribute prefix. Additionally, you have to add the RegisterGlobalFilters call in the Global.asx file. Content of the Global.asx ## ConclusionIn this post, I showed all five types of built-in filters in the ASP.NET MVC framework and discussed how to use them to address cross-cutting concerns effectively. These filters help you to extend the logic of your controller and actions while a request is processed.For more details about how built-in filters work, I highly recommend the books Pro ASP.NET MVC 5 and Pro ASP.NET MVC 5 Plattform.You can find the source code on GitHub." }, { "title": "Unit Testing Controllers and Actions", "url": "/unit-testing-controller-and-actions/", "categories": "ASP.NET", "tags": "FluentAssertions, MVC, TDD, xUnit", "date": "2018-01-15 20:10:13 +0100", "snippet": "In my last post, I showed how to work with controllers and actions. This included passing data, return types of actions and redirects. In this post, I want to check if my implemented features work as I expect them to by unit testing controllers and actions.Setting up the projectI use the project which I created the last time. For the testing, I add a class library project called ControllersAndActions.Test. In this project, I add one class called ActionTests. Usually, I would use several classes for every class I test. To make things simple, I only use this one class this time. After creating the class, I install xUnit and FluentAssertions. You can choose the testing framework of your liking. FluentAssertions is a great tool to make the asserts more readable.Testing the name of the viewThe first tests I write will test what view is returned by the action. To do that, I call an action and then the view name of the returned value should be the name of the view I expect to be called. Testing returned view name of action If your action returns an ActionResult instead of a ViewResult, you have to cast the object first before you can access the ViewName property.Testing ViewBag valuesMore interesting than testing if the right view, is called is testing the values of the view bag. You can access the view bag with the returned object of the action and compare it with the value you expect it to be. Testing ViewBag value Testing redirectsWhen testing the redirect to a controller, the return object of the action has two interesting properties when testing redirects. The first property is Permanent which is a bool indicating whether the redirect was permanent. The second property is URL which you can compare to the URL you expect. Testing redirect to a controller Testing the redirection to a route is a bit different. You have to compare the route values for the controller and action to the values you expect them to be. Testing redirect to a route Testing the returned status codeThe next interesting property in the return object is the status code. With this property, you can check if the returned HTTP status code is what you expect. Testing the status code Testing the ViewModelThe last test for today tests if the view model has the expected data type. To do that use the ViewData.Model property of the return value. Testing ViewData data type ConclusionIn this post, I presented several test cases for unit testing controller and actions. For these tests, I accessed various properties of the return object of the action call.For more details about how controller and actions work, I highly recommend the books Pro ASP.NET MVC 5 and Pro ASP.NET MVC 5 Plattform.You can find the source code on GitHub." }, { "title": "Controllers and Actions", "url": "/controllers-and-actions/", "categories": "ASP.NET", "tags": "Action, ActionResult, Controller, MVC, Redirect", "date": "2018-01-15 16:52:28 +0100", "snippet": "In this post, I want to talk about how controllers and actions interact with each other and present several built-in functions.  The MVC Framework is endlessly customizable and extensible. As a result, it is possible to implement your own controller which I will shortly talk about and show how to do that.Setting up the projectI created a new ASP.NET MVC project with the empty template and add folders and core references for MVC. &lt;img loading=\"lazy\"title=\"Set up the ASP.NET MVC project for Actions and Controller\" src=\"/assets/img/posts/2018/01/Set-up-project.jpg\" alt=\"Set up the ASP.NET MVC project for Actions and Controller\" /&gt; Set up the ASP.NET MVC project Throughout this post, I will only use the default route. This post describes the basics of controllers and actions but I will not explain every single simple step. Therefore it is expected that you know at least how to create a controller and how to call its Index action.Passing data to an actionFollowing I will show several options, how to pass data to the action. First I start with the simplest way to call an action though. The easiest way to call an action is by entering an URL which is mapped to a route and redirected to the fitting action. Simple Action The Index action does nothing except returning a view to the user’s browser. Therefore it does not take or process any data.Passing data to the parameterA simple way to pass data to an action is by using a parameter for the action. The parameter will be added at the end of the URL. Action with int parameter As you can see, I used int as the data type for the parameter. If the user enters a parameter which can not be converted to an int, ASP.NET MVC throws an exception and displays the yellow screen of death. To prevent this exception, it is possible to set a default parameter. If the framework can’t convert the parameter provided by the user, the default parameter is used.Using a parameter to pass data to a method makes it also way easier to unit test.It is also interesting to note that parameter can’t have out or ref parameter. It wouldn’t make any sense though and therefore ASP.NET MVC would throw an exception.Passing multiple parameters to the actionThere are two options on how to pass multiple parameters to an action. The first option is to use the catchall variable in the route. To do that, you have to configure the route first, as shown in the following screenshot. Catchall route After setting up the route, you can use the catchall variable as the parameter and add as many variables as you want to it. The variables are all passed as one string and have to be parsed. The following screenshot shows one id and three variables in the catchall are passed to the method. Catchall action The second way is to use multiple parameters. This method is used to pass data from a form to the action. Action with multiple parameter Retrieving data within an actionBesides parameter, there are several other sources from where you can receive data. These data sources can be: Request User Server HttpContext RouteDataI would suggest to type these classes into your action and look through the options provided by the intellisense. On the following screenshot, I show some of these options. Action with different sources to get some data ActionResult typesAction results help to make the code easier and cleaner and also make unit testing easier. The ActionResult class is an abstract class which can be used. But there are also a whole bunch of derived classes which can be used to increase the readability of the action. If you are familiar with the command patter, you will recognize it here. ActionResults pass around objects that describe the operations which are performed.As I just mentioned, there is a whole list of action results. Following I will present some of them.Returning a viewReturning a view is the easiest way to present an HTML page to the user. Using ActionResult as return type works and compiles. Though I prefer using ViewResult for views because it tells me faster what the actual return type is. There are different overloaded versions of View().View without parameterThe simplest is without a parameter. When using this version, the MVC framework searches for a view with the same name as the action. View without parameter The framework searches in the following locations: /Areas/&lt;/span&gt;/Views/&lt;/span&gt;/&lt;/span&gt;.aspx /Views//.aspx /Areas/&lt;/span&gt;/Views/&lt;/span&gt;/&lt;/span&gt;.ascx /Views//.ascx&lt;/span&gt; /Areas/&lt;/span&gt;/Views/Shared/&lt;/span&gt;.aspx /Views/Shared/&lt;/span&gt;.aspx&lt;/span&gt; /Areas/&lt;/span&gt;/Views/Shared/&lt;/span&gt;.ascx /Views/Shared/&lt;/span&gt;.ascx&lt;/span&gt; /Areas/&lt;/span&gt;/Views/&lt;/span&gt;/&lt;/span&gt;.cshtml /Views/&lt;/span&gt;/&lt;/span&gt;.cshtml&lt;/span&gt; /Areas/&lt;/span&gt;/Views/&lt;/span&gt;/&lt;/span&gt;.vbhtml /Views/&lt;/span&gt;/&lt;/span&gt;.vbhtml&lt;/span&gt; /Areas/&lt;/span&gt;/Views/Shared/.cshtml&lt;/span&gt; /Views/Shared/&lt;/span&gt;.cshtml&lt;/span&gt; /Areas/&lt;/span&gt;/Views/Shared/&lt;/span&gt;.vbhtm /Views/Shared/&lt;/span&gt;.vbhtml&lt;/span&gt;The MVC framework also searches for ASPX views, C# and Visual Basic .NET Razor templates. This behavior guarantees backward compatibility with earlier versions of the MVC frameworkIf the file is found in one of the locations, it is returned and the search stops. If no file is found, a yellow screen of death is shown with some information which locations where searched.View with name parameterBy default, the framework searches for a view with the same name as the action. If you want to return a view from the same controller but with a different name, you have to pass the name as the parameter in the view. View with view name View with pathSometimes the view you want to return is in a different directory. Then you can pass the path to the view as the parameter. Note: Don’t forget the tilde at the beginning of the path. Otherwise, you won’t get the expected view. View with path It is very uncommon to use this feature and it is a hint that your design might not be optimal.View with a view modelAll versions of the view which I showed above also have the option to pass a view model. To pass the view model to the view, simply add it as a parameter. On the following screenshot, I pass the path to the view and the view model (which in this case is just a string). View with path and view model When you use only a view model which is a string, the ASP.NET MVC framework thinks that it is the name of the view you want to return. To tell the framework that it is a model, you have to prefix the model with the model: keyword. View with model keyword Passing data with a view modelPreviously I showed how to pass a model to the view. Now there are two different ways to use this view model in the view.  Either you cast the model to its data type and then use it inline, for example, @(((DateTime)Model).ToLocalTime()) or you can declare the model and its data type and then use it like a normal C# variable. View with view model Casting the object works but it makes the view pretty messy. Therefore I recommend using strongly typed views. It is important to note that when declaring the model it starts with a lower case and when using it, it starts with an upper case.Passing data with view bagAnother way to pass data to a view is with the view bag. The difference between the view bag and the data model is that the view bag is a dynamic object. The view bag doesn’t have to be passed to the view but can be used in the view like the view model. Adding data into the view bag Due to its dynamic behavior, I don’t have to declare the view bag variable before using it. I can simply assign my data to it. View with view bag Passing data with temp dataTemp data works like the view bag with one difference. After reading the data from temp data, the values get marked for deletion. The usage is similar to the usage of a session. Assigning values to temp data If the value is a primitive data type, it can be used like the session. If it is an object, you have to cast it. Using temp data in a view Keeping dataThere are three different ways to store the values from temp data: Assign it to the view bag Assign it to a variable Use Peek() or Keep()Peek returns the value without marking it for deletion. Keep marks the value to keep it after it was accessed. When the value is accessed a second time, it gets marked for removal if you don’t use Keep again. Ways to store data from temp data Returning an HTTP status codeASP.NET MVC hat to built-in classes for returning an HTTP status code HttpNotFoundResult HttpUnauthorizedResultAdditionally, you can return every HTTP status code you want with HttpStatusCodeResult. Pass the status code and a message as parameter. HttpNotFoundResult has a short form, HttpNotFound. The return value for a status code is HttpStatusCodeResult. Working with HttpStatusCode More resultsAdditionally, to the action results, I just presented there are some more like: PartialViewResult FileResult JsonResult JavaScriptResult EmptyResultThe names are speaking for themselves. Therefore I won’t go into more detail about them.Performing redirectsA common performance of an action is to redirect the user to another URL. Often this URL is another action which generates an output for the user.The Post / Redirect / Get patternThe Post / Redirect / Get pattern helps to avoid the problem of resubmitting a form for the second time which could cause unexpected results. This problem can occur when you return HTML after processing a POST and the user clicks the reload button of his browser which results in resubmitting his form.With the Post / Redirect / get pattern the POST request is processed and then the browser gets redirected to another URL. The redirect generates a GET request. Since GET requests don’t (shouldn’t) modify the state of the application, any resubmission of the request won’t cause any problems.Temporary and permanent redirectsThere are two different HTTP status codes for redirects: 301 for permanent redirection 302 for temporary redirectionThe HTTP code 302 is often used, especially when using the Post / Redirect / Get pattern. Be careful when using this cause because it instructs the recipient to never again use the requested URL and use the new URL instead. If you don’t know which code you should use, use temporary redirects.ASP.NET MVC offers for every redirect a temporary and permanent implementation. For example, you can use Redirect() for a temporary redirect or RedirectPermanent() for a permanent redirection.Redirect to a viewIf you want to redirect to a view, use the Redirect or RedirectPermanent method with the folder and index name as a string parameter. You don’t have to provide the file ending of the view. The return value of this is RedirectResult. Redirect to view Redirect to a routeTo redirect from an action to a route use RedirectToRoute or RedirectToRoutePermanent. The methods take an anonymous type with the route information. The return value is RedirectToRouteResult. Redirect to route Redirect to an actionMore elegantly than redirecting to a route is redirecting to an action. RedirectToAction and RedirectToActionPermanent are wrapper for RedirectToRoute and RedirectToRoutePermanent but make the code cleaner in my opinion. The values provided as a parameter for the action and controller are not verified at compile time. This means that you are responsible for making sure that your target exists.  Redirect to action ConclusionIn this post, I showed how to pass data into an action and how to retrieve data within an action from different sources. Then I presented some of the ActionResult types which are built-in into the MVC framework and also how to pass data between actions using ViewBag and TempData.. In the last part, I talked about the different ways how to perform redirects.For more details about how controller and actions work, I highly recommend the books Pro ASP.NET MVC 5 and Pro ASP.NET MVC 5 Plattform.I uploaded the source code to GitHub." }, { "title": "Routing in ASP.NET MVC", "url": "/routing-in-asp-net-mvc/", "categories": "ASP.NET", "tags": "MVC, Routing", "date": "2018-01-13 15:34:13 +0100", "snippet": "Routing in ASP.NET MVC can be a really complex topic. In this post, I want to present the most used URL Patterns and explain how they work.Routing with RoutesRoutes can be created by adding them to the RouteCollection or by decorating actions or controller with attributes. In this section, I will show different approaches how to build a route and adding it to the RouteCollection. Adding a route happens in the RouteConfig class in the App_Start folder at the start of the application.Default routeThe ASP.NET MVC framework comes out of the box with a default route. The template also displays the property names of the route attributes, so it is easier for a beginner to understand what’s going on. Let’s have a look at the default route:Every route has a unique name. The name of the default route is Default. The url attribute describes the pattern of the url. The default pattern is Controller/Action/Id. The defaults property sets default properties for the controller, action and sets the id as optional. The default values are used when no values for the attribute is passed. Valid URLs for this route are for example: / /Home /Admin /Home/Index /Home/Index/123abc /Home/Index/abcThe names of the properties (name, url and defaults) are grayed out because they are not needed. They are there only for an easier understanding, especially for beginners.Additionally to the default route, the ASP.NET template implements routes.IgnoreRoute(“{resource}.axd/{*pathInfo}”);. You should keep this rule to prevent ASP.NET MVC from trying to handle .axd files. These files don’t exist physically and therefore are handled by an HttpHandler.Simple routeNow it’s time to implement our own routes. The simplest route takes a controller and an action with no defaults or additional parameters.If the user types into his browser myurl.com/Home/Index the Index action in the Home controller is called. If the user only enters /Home the route won’t find a suiting action because no default action is defined.Static route segmentsASP.NET MVC also offers the possibility of static route segments. This means that if the route contains a certain word that a specific controller and/or action are called.The screenshot above shows three different variations of a static segment in the route. The first route calls the ShowArchievePosts action in the Posts controller when the user enters /Blog/Archive. The second route calls an action entered in the Posts controller when the user enters/Blog/ActionName.The third route is selected when the user input starts with /InternalBlog. If the user doesn’t enter anything else the default controller and action are called. The user can also enter a controller or a controller and an action.Another possibility to add a static part to a route is to prefix the controller or action as part of its name.This route is matched when the controller begins with External. This means that the URL /ExternalHome/Index would call the Index action in the Home controller.Route orderingNow it gets a bit trickier. The routes are added to the RouteCollection as they appear in the RegisterRoutes method. After the user entered a URL, ASP.NET searches through the RouteCollection until it finds a fitting route. A fitting route does not mean that it leads to the result which the user expects.Let’s take a look at the two roots from above. The first route is the default route with a default controller and action and the second route has the static segment InternalBlog in front of the controller. What happens if the user enters “/InternalBlog/Posts/Display”? You might think that the second route is selected. But that’s not the case. The entered URL fits the first route where InternalBlog = controller, Posts = action, Display = id. If the application doesn’t have an InternalBlog controller with the Posts action, an error message is displayed.Custom segment variablesThe default route already showed that it is possible to add a variable after the action. For example, the route /Home/Index/123 call the Index action from the HomeController with the parameter 123. The parameter name of the action must match the variable name in the route. Otherwise, it will be null.The default route sets the id as UrlParameter.Optional which means that this parameter is optional (what a surprise).Variable amount of segment variablesLike params in C#, the routing in ASP.NET MVC offers a feature to take a variable amount of variables. To achieve that use the *catchall keyword.This allows the user to enable any amount of variables into the URL. A fitting URL would be for example /Home/Index/User/Detail/123. User/Detail/123 would be passed as catchall parameter to the Index action in the HomeController.The catchall string contains User/Detail/123.Default values for attributesI already showed that it is possible to set default values for controller, actions and attributes in the route. It is also possible to set default values for attributes in the action.  This is done as in normal C# with variable = defaultValue, for example string id = “1”.Variable constraintsThe routing in ASP.NET MVC enables you to restrict the data type and the range of the entered attributes. To restrict a variable to int for example use, variable = new IntRouteConstraint(). There are several classes like FloatRouteConstraint() or AlphaRouteConstraint().To restrict a variable to a certain range, use the RangeRouteConstraint class, for example variable = new RangeRouteConstraint(min, max).Restrict HTTP methodNot only controller, actions and variables can be restricted, also the HTTP method can be restricted. To restrict a route to a certain HTTP method use httpMethod = new HttPMethodConstraint(“GET”). Instead of get, you could use any HTTP verb.Attribute Routing in ASP.NET MVCAdditionally, to creating routes it is possible to decorate controller and actions with route attributes.Enabling attribute routingTo enable attribute routing you have to add routes.MapMvcAttributeRoutes(); to the RegisterRoutes method. Next, you have to set the route attribute on an action and the desired route, for example [Route(“MyRoute”]). Now you can call your action with /MyRouteIt is also possible to decorate an action with several route attributes.Attribute routing with variablesWith attribute routes, it is also possible to add variables which can be processed in the action as parameters. To declare a variable wrap it in curly brackets. The name in the route must match the name of the parameter, otherwise, the parameter will be null.Constraints for route attributesThe variables in the route attribute can be restricted to a certain data type. This would be useful for the id. Ids are usually int, so it makes sense to expect an int id. To do that you only have to add the data type after the variable name within the brackets, separated by a colon. Don’t forget to change the data type of the parameter, otherwise ASP.NET won’t match the variable with the parameter.Routing Static FilesIf the user enters a path to a static file, for example, an image or a pdf file, the routing in ASP.NET MVC forwards the user to this file, if it exists.The mechanisms for routing in ASP.NET MVC searches first the path of the file. Only then it evaluates the routes. To prevent this behavior use set RouteExistingFiles to true.It is also possible to ignore the routing for a certain file type with routes.IgnoreRoute(path).The example above shows that routing is ignored for all HTML files in the StaticContent folder. You can use {filename} as a variable for all file names in this directory.ConclusionI showed several approaches for routing in ASP.NET MVC using the routing method and using attributes on actions and controllers. Routing is a pretty complex topic and there is way more to about it than what I presented. For more details about routing, I highly recommend the books Pro ASP.NET MVC 5 and Pro ASP.NET MVC 5 Plattform.I uploaded the source code to GitHub if you want to download it and play a bit around with different routes." }, { "title": "Repository and Unit of Work Pattern", "url": "/repository-and-unit-of-work-pattern/", "categories": "Design Pattern", "tags": "C#, Entity Framework, Software Architecture", "date": "2018-01-09 19:32:51 +0100", "snippet": "The Repository pattern and Unit of Work pattern are used together most of the time. Therefore I will combine them in this post and show how to implement them both.Definition RepositoryThe Repository mediates between the domain and data mapping layers, acting like an in-memory collection of domain objects. (“Patterns of Enterprise Application Architecture” by Martin Fowler)Repository Pattern Goals Decouple Business code from data Access. As a result, the persistence Framework can be changed without a great effort Separation of Concerns Minimize duplicate query logic TestabilityIntroductionThe Repository pattern is often used when an application performs data access operations. These operations can be on a database, Web Service or file storage. The repository encapsulates These operations so that it doesn’t matter to the business logic where the operations are performed. For example, the business logic performs the method GetAllCustomers() and expects to get all available customers. The application doesn’t care whether they are loaded from a database or web service.The repository should look like an in-memory collection and should have generic methods like Add, Remove or FindById. With such generic methods, the repository can be easily reused in different applications.Additionally to the generic repository, one or more specific repositories, which inherit from the generic repository, are implemented. These specialized repositories have methods which are needed by the application. For example, if the application is working with customers the CustomerRepository might have a method GetCustomersWithHighestRevenue.With the data access set up, I need a way to keep track of the changes. Therefore I use the Unit of Work pattern.Definition Unit of WorkMaintains a list of objects affected by a business transaction and coordinates the writing out of changes.  (“Patterns of Enterprise Application Architecture” by Martin Fowler)Consequences of the Unit of Work Pattern Increases the level of abstraction and keep business logic free of data access code Increased maintainability, flexibility and testability More classes and interfaces but less duplicated code The business logic is further away from the data because the repository abstracts the infrastructure. This has the effect that it might be harder to optimize certain operations which are performed against the data source.Does Entity Framework implement the Repository and Unit of Work Pattern?Entity Framework has a DbSet class which has Add and Remove method and therefore looks like a repository.  the DbContext class has the method SaveChanges and so looks like the unit of work. Therefore I thought that it is possible to use entity framework and have all the Advantages of the Repository and Unit of Work pattern out of the box. After taking a deeper look, I realized that that’s not the case.The problem with DbSet is that its Linq statements are often big queries which are repeated all over the code. This makes to code harder to read and harder to change. Therefore it does not replace a repository.The problem when using DbContext is that the code is tightly coupled to entity framework and therefore is really hard, if not impossible to replace it if needed.Repository Pattern and Unit of Work Pattern UML Diagram Repository pattern UML diagram The Repository pattern consists of one IRepository which contains all generic operations like Add or Remove. It is implemented by the Repository and by all IConcreteRepository interfaces. Every IConcreteRepository interface is implemented by one ConcreteRepository class which also derives from the Repository class. With this implementation, the ConcreteRepositoy has all generic methods and also the methods for the specific class. As an example: the CustomerRepository could implement a method which is called GetAllSeniorCustomers or GetBestCustomersByRevenue.The unit of work provides the ability to save the changes to the storage (whatever this storage is). The IUnitOfWork interface has a method for saving which is often called Complete and every concrete repository as property. For example, if I have the repository ICustomerRepository then the IUnitOfWork has an ICustomerRepositry property with a getter only. Additionally, IUnitOfWork inherits from IDisposable.The UnitOfWork class implements the Complete method, in which the data get saved to the data storage. The advantage of this implementation is that wherever you want to save something you only have to call the Complete method from UnitOfWork and don’t care about where it gets saved.Implementation of the Repository and Unit of Work PatternFor this example, I created a console project (RepositoryAndUnitOfWork) and a class library (RepositoryAndUnitOfWork.DataAccess). In the class library, I generate a database with a customer table.Next, I let Entity Framework generate the data model from the database. If you don’t know how to do that, check the documentation for a step by step walkthrough.Implementing RepositoriesAfter setting up the database, it’s time to implement the repository. To do that, I create a new folder, Repositories, in the class library project and add a new interface IRepositry. In this Interface, I add all generic methods I want to use later in my applications. These methods are, GetById, Add, AddRange, Remove or Find. To make the Interface usable for all classes I use the generic type parameter T, where T is a class.After the generic repository, I also implement a specific repository for the customer. The ICustomerRepository inherits from IRepository and only implements one method.After implementing all interfaces it’s time to implement concrete repository classes. First, I create a class Repository which inherits from IRepository. In this class, I implement all methods from the interface. Additionally to the methods, I have a constructor which takes a DbContext as Parameter. This DbContext instantiates a DbSet which will be used to get or add data.The implementations of the methods are pretty straight Forward. The only interesting one might be the Find method which takes an expression as parameter. In the implementation, I use Where to find all entries which fit the Expression of the parameter.The final step for the Repository pattern is to implement the CustomerReposiotry. This class derives from Repository and ICustomerRepository and implements the method from the interface. The constructor takes a CustomerDbEntities object as Parameter which is derived from DbContext and generated by Entity Framework.Implementing Unit of WorkAll repositories are created now, but I need a class which writes my data to the database, the unit of work. To implement this class, I first implement the IUnitOfWork interface in the repositories folder in the library project. This interface derives from IDisposable and has an ICustomerRepository property and the method Complete. This method is responsible for saving changes. The Name of the method could be Save, Finish or whatever you like best.Like before, I add the concrete implementation of IUnitOfWork to the repositories folder in the console application project. The constructor takes a CustomerDbEnties object as parameter and also initializes the ICustomerRepository. The Complete Method saves the context with SaveChanges and the Dispose method disposes changes.Using the Repository and Unit of WorkThe usage of the unit of work differs between a web application and a console application. In an MVC application, the unit of work gets injected into the constructor. In the console application, I have to use a using statement. I can use with unitOfWork.Customer.Method(), for example unitOfWork.GetBestCustomers(3). To save the changes use unitOfWork.Complete().You can find the source code on GitHub. If you want to try out the example, you have to change to connection string in the App.config to the location of the database on your computerNote: In this example, I always talked about writing and reading data from the database. The storage location could also be a web service or file drive. If you want to try my example, you have to change the connection string for the database in the App.config file to the Location of the database on your computer.ConclusionIn this post, I showed how to implement the Repository and Unit of Work pattern. Implementing both patterns results in more classes but the advantages of abstraction increased testability and increased maintainability outweigh the disadvantages. I also talked about entity framework and that although it looks like an out of the box Repository and Unit of Work pattern, it comes at the cost of tight coupling to the framework and should not be used to replace the patterns." }, { "title": "Template Method Pattern", "url": "/template-method-pattern/", "categories": "Design Pattern", "tags": "C#, Software Architecture", "date": "2018-01-04 21:01:01 +0100", "snippet": "The Template Method pattern helps to create the skeleton of an algorithm. This skeleton provides one or many methods which can be altered by subclasses but which don’t change the algorithm’s structure.UML DiagramThe UML diagram for the Template Method is pretty simple. It has one abstract class with the TemplateMethod and one or many sub methods which can be overridden by subclasses. These sub methods can be either abstract or virtual. If they are abstract, they have to be implemented. If they are virtual, they can be overridden to alter the behavior of this step.Implementation of the Template Method patternTo be honest, the Template Method was pretty confusing to me in the beginning but after I tried to implement it, it became clear how it works. My implementation is pretty simple but it helped me to understand the pattern and I hope it helps you too.I want to do some calculation and then save the result somewhere. The algorithm has three steps whereas step two and three can be overridden by a subclass. The class Calculator offers the TemplateMethod. This method contains the three steps.The first step, BeforeCalculation can’t be overridden by the subclasses and therefore will always be executed. The second step of the algorithm, CalculateSomething can be overridden. The classes CalculatorOracle and CalculatorSqlAzure override this method and do their own calculations. The CalculatorSqlAzure also overrides the property Result. Implementation of CalculateSomething in the CalculatorSqlAzure class The CalculatorOracle class only overrides the CalculateSomething method. Implementation of CalculateSomething in the CalculatorOracle class The third and last step, SaveResult is only overridden by the CalculatorSqlAzure class. This means that the CalculatorOracle class uses the method provided by the Calculator. Overridden SaveResult method in the CalculatorSqlAzure class I know that this example is not really what you will see in a real-world project but I hope that it helped you to understand the Template Method pattern. You can find the source code on GitHub.ConsequencesThe Template Method pattern to achieve a clean design. The algorithm provided by the base class is closed for modification but is open for extension by subclasses. As a result, your design satisfies the open-closed principle. The pattern also helps to implement the Hollywood principle (“Don’t call us, we call you”).One downside is that the steps of the algorithm must be already known when the pattern is applied. Therefore the Template Method pattern is great for reuse but it is not as flexible as for example the Strategy pattern.Related patternsStrategy: Inject a complete algorithm implementation into another moduleDecorator: Compose an algorithm or behavior from several sub-partsFactory: Define a common interface for creating new instances of types with many implementationsConclusionI showed how the Template Method pattern can be used to provide a skeleton for an algorithm. One or many parts of this algorithm can be overridden by subclasses. This behavior helps achieving the open-closed principle and therefore a clean overall design. I also implemented a simple example to show how the pattern works." }, { "title": "Strategy Pattern", "url": "/strategy-pattern/", "categories": "Design Pattern", "tags": "C#, Software Architecture", "date": "2018-01-02 15:17:42 +0100", "snippet": "The Strategy pattern is one of the simpler design patterns and probably a good one to get started with design patterns. Additionally, it is also very practical and can help to clean up the code.Goals encapsulate related algorithms so they are callable through a common interface let the algorithm vary from the class using it allow a class to maintain a single purposeWhen to use itThe most obvious sign that you might want to use the strategy pattern are switch statements. In my example further down, I will also show how a switch statement can be replaced by the Strategy pattern.Another hint that you should use the Strategy pattern is when you want to add a new calculation but in doing so you have to modify your class which violates the open-closed principle.UML DiagramThe context class does the work. It takes the desired strategy in the constructor (the strategy can also be passed as parameter in the method as you will see later). The strategy interface declares a method which is called by the context class to perform the calculation I want on a concrete strategy.Implementation without the Strategy patternThere are many examples of implementing the Strategy pattern on the internet. Popular ones are implementing different search algorithm or different calculations for products or orders. In my example, I will implement a cost calculator for products. The costs depend on the country in which the product is produced.My products have some basics properties like price or name and also the production country. The ProductionCostCalculatorService class implements a calculate Methode in which it has a switch Statement. Depending on the production Country, the production costs are calculated differently. Calculating production costs using a switch statement The production countries are China, Australia and the USA. If a different production Country is passed in the product, a UnknownProductionCountryException is thrown.This approach has some flaws. The biggest problem is that it is not easy to add a new country. To achieve that, a new case has to be added to the switch. This doesn’t sound like a problem but it violates the open-closed principle and is also a problem if you have to add a new country every week. You will end up with a huge switch Statement. Another design flaw is that the product doesn’t have to know where it is produced. Therefore the algorithm shouldn’t rely on the information from the product.You can find the source code on GitHub. The solution to the problems is the Strategy pattern.Implementation of the Strategy patternTo implement the strategy pattern, I have to implement a new class for every strategy. As a result, I will have the classes ChinaProductionCostStrategy, AustraliaProductionCostStrategy and UsaProductionCostStrategy. These classes implement the new Interface IProductionCostCalculatorService which has one method. The method is Calculate and takes a product as parameter.After these changes, I can modify the ProductionCostCalculatorService class. First, I inject the IProdctionCostCalculatorService in the constructor. Then, I change the CalculateProductionCost method to return the return value of the Calculate method from the Interface. ProductionCostCalculatorService with the Strategy pattern The last step is to modify the call of the calculation. For every strategy, I need an object, which I inject into the constructor of the ProductionCostCalculatorService. Calculating production costs using strategies and print out I have to admit that These calls look a bit messy and I am not a big fan of them. Therefore it is possible to change the ProductionCostCalculatorService, so it takes the strategy as parameter in the Calculate method instead of the constructor. Method call with strategy as parameter This change also tidies up the ProductionCostCalculatorService, which makes it even easier to read. With this changes implemented, I can now change the call of the calculation. It is not necessary anymore to create a new ProductionCostCalculatorService object for every different production country. Instead, I pass the country as parameter in the CalculateProductionCost method. Calculating production costs using the strategy as parameter You can find the source code on GitHub.ConclusionIn this example, I showed that the Strategy pattern is a simple pattern which helps to tidy up our code. While programming, look out for switch statements since these often indicate that the Strategy pattern could be used. As a result of the Strategy pattern, decoupling between the algorithm and other classes can be achieved.Additionally, I showed a second variation on how to implement the pattern which leads to an even cleaner implementation." }, { "title": "Facade Pattern", "url": "/facade-pattern/", "categories": "Design Pattern", "tags": "C#, Software Architecture", "date": "2017-12-29 11:41:38 +0100", "snippet": "The Facade pattern is often used without the programmer knowing that he uses it. In this post, I want to give the thing a developer often does automatically a name.Goals Simplify complex code to make it easier to consume Expose a simple interface which is built on complex code or several interfaces Expose only needed methods of a complex API or library Hide poorly designed APIs or legacy code behind well designed facadesDownsides Only selected interfaces of an API will be exposed The facade needs to be updated to offer more functionality from the underlying systemUML DiagramThe UML diagram for the facade pattern is pretty empty. It only shows the facade which is called by a client and that it calls methods of subsystems. These subsystems can be classes within your own system but also third party libraries or calls to web services.Implementation without the Facade PatternIn this example, I am implementing a fake API which provides information for books. The API has three methods: LookUpAuthor, LookUpPublisher and LookUpTitle. All three methods take the ISBN as string Parameter and also return a string. To get the information for a specific ISBN, every method has to be called. Implementation of API calls without the Facade Pattern This is ok when you have three methods but what if you also want to get information about the year of publication, related books from the author or an excerpt of the book? This would increase the methods very quickly and as a result, would bloat the code. Imagine this code with 10 services for every attribute of a book. This would be nasty to use. The solution to this problem is to implement a facade.You can find the source code of this example on GitHub.Implementation of the Facade PatternTo hide all the service calls, I implement a facade which takes the ISBN as Parameter and returns all values from the service calls. Additionally to the facade, I implement a Book class, which holds all the information about a book and which is returned by the facade. It is quite common that facades have helper classes which contain all the information from different method calls.Implementing a facade is pretty simple. I move all the service calls into a separate class called Book Service. This class exposes only one method, LookUpBookInformation which takes the ISBN as a parameter and returns a book object. Inside the LookUpBookInformation method, I implement all the service calls which were in the main method before. The return value of every service call is mapped to a property of the book object. Implementation of the BookService class With this implementation, i can tidy up the main method and end up with only one method call. Invoking the BookService You can find the source code of this example on GitHub.ConclusionIn this post, I showed the Advantages of using the Facade pattern. I also presented an example without the pattern and how to refactor the code to tidy it up." }, { "title": "Chain of Responsibility Pattern", "url": "/chain-responsibility-pattern/", "categories": "Design Pattern", "tags": "C#, Software Architecture", "date": "2017-12-26 16:58:08 +0100", "snippet": "I think that the chain of responsibility pattern is pretty easy to learn. It is not used too often but it is very useful when sending messages to a receiver where the sender doesn’t care too much about which receiver handles the message.DefinitionAvoid coupling the sender of a request to its receiver by giving more than one object a chance to handle the request. Chain the receiving objects and pass the request along the chain until an object handles it. (Gang of Four)Real world exampleA real world example for the chain of responsibility is the chain of command in a company. For example if an employee needs an approval for a task, he gives the report to his manager. If the manager can’t approve the report, for example because the costs surpass his authority, he gives the report to his manager until a manger with enough authority is found or until the report is rejected.The normal employee does neither care nor know who gets the report. He only knows his manager who knows his manager and so on.BenefitsThe goals of the chain of responsibility pattern are: reduction of the coupling dynamically manage message handlers end of chain behavior can be defined as neededUML DiagramThe client and concrete handler link together to form the chain of responsibility. The client could be the employee who needs his report approved and the handler are different managers like team leader, area manager and CEO.Implementation without the Chain of Responsibility PatternI will now implement the real world example which I mentioned before. You can find the source code on GitHub.The user can enter an amount and then different managers are asked for their approval. If the amount is even too high for the CEO, the approval is denied. The problem with this approach is that all the business logic happens in the main method. Lets say the employee goes to his manager and he can’t approve the amount. He then sends the report back and tells the employee to go to his manager instead of passing the report directly. Finding a manager to approve the report All the managers are stored in the employees list and the normal employee has to ask the first manager for the approval. If the approval can’t be given, the next manager has to be ask, and so on. Next I want to implement the chain of responsibility which cleans up the code and moves to business logic closer to the manager.Implementation of the Chain of Responsibility PatternTo implement the chain of responsibility pattern I reuse the code from the previous example but I will clean it up a bit.First I implement the ExpenseHandler which handles the approving process. If the current manager is not able to approve the amount, the costs are given to the next manager in line to approve it. To be able to do that, the managers of the chain have to be registered. The register process replaces the adding of the managers to the list of the previous example. Create chain of responsibility To get an approval of the report, the only thing Tom has to do is call the approve method with the expenses as parameter. The ExpenseHandler will take care of sending the message to the right manager. As a result of this, the code looks cleaner and is easier to read. Get approval of the report Improving the codeWith this changes, the code already works. But the code produces a null reference exception if the costs can’t be approved by anyone. The reason for this exception is that the handler calls the next manager until the costs are approved. If the handler doesn’t get an approval by the last manager, it calls the approve method on the next manager which doesn’t exist (and therefore is null). There are two solutions to this problem. Either I could implement a null check or I could implement another handler which handles the end of chain operation.I implemented another handler called EndOfChainExpenseHandler which handles the end of the chain for me. If this handler is called to approve the report, it denies the report, because no manager could approve the report. Furthermore it is not impossible to add a manager as next because the EndOfChainExpenseHandler is always last in the chain. To add the EndOfChainExpenseHandler to the chain I only have to add it as next when I register another manager. So a new manager has the EndOfChainExpenseHandler is next which prevents the program from running into a null reference exception and also enables me to do whatever I want when the end of the chain is reached.When to use the Chain of Responsibility PatternYou should use this pattern if: You have more than one handler for a message The appropriate handler is not known to the sender The set of handlers can be dynamically definedConclusionIn this example I showed how to use the chain of responsibility pattern to decouple the business logic from the main method and how to send the message to different receiver, which the sender doesn’t even know. Additionally I implemented another handler, which takes care of the case that no receiver could handle the message.The source code to the example can be found on GitHub. " }, { "title": "Adapter Pattern", "url": "/adapter-pattern/", "categories": "Design Pattern", "tags": "C#, Software Architecture", "date": "2017-12-25 16:31:34 +0100", "snippet": "The adapter pattern is one of the most useful patterns in my eyes. Therefore I want to explain what the adapter is and present a simple real life example on how it can be used.The Adapter Pattern in real lifeIt works the same way as a real-life adapter, for example for a power outlet. If you travel from the European Union to Switzerland or the US your normal power plug won’t fit into the power outlet. Hence you have to get an adapter which can link the power outlet with the power plug. This example perfectly describes how the adapter pattern works.The Adapter Pattern in software developmentThe adapter pattern is often used in combination with third-party libraries. For example, your client needs to utilize a specific interface and a library which you have to use doesn’t use this Interface or the library expects a different Interface. This is where the adapter comes to use. The adapter sits between the client and the library and connects them together. Like in the real world an adapter helps two classes which are not compatible to work together.On the other hand, if you develop the library, you can provide support for future adapters so your users can implement it easily into their application.UML DiagramThe UML diagram for the adapter pattern is pretty simple. The client wants to call an operation on the Adaptee. To do that the client implements the Adapter interface and calls the Operation method. The ConcreteAdapter implements the method and calls the AdaptedOperation of the adaptee. After the execution, the ConcreteAdapter returns the information needed to the client.Let’s look at a real time exampleImplementation of the Adapter PatternAs an example, I import a list of my employees and then use a third party tool to a fancy header and footer to the list while printing it. Let’s say that my internal SAP system returns my employees as string array. The third-party tool expects a list of strings though. Therefore I need an adapter which to enable me to use the library with my string array. Internal employee management system The third-party tool called FancyReportingTool is pretty simple too. It is instantiated with an interface which will provide the data for printing in the ShowEmployeeList method. The interface I provide during the instantiating of the FancyReportingTool is the Adapter interface, which is implemented by the concrete adapter.The constructor of the library asks for an interface. Because of this interface, it is possible for me to use the adapter. Remember this when creating libraries yourself. It’s good practice to provide support for future adapters. FancyReportingTool implementation The last missing piece is the ConcreteAdapter which is called EmployeeAdapter. This class inherits from the interface and form the SAPSystem. The adapter gets the employees from the SAPSystem class and converts the array into a list and returns it. The FancyReportingTool can print this list now. EmplyeeAdapter implementation The outputWith everything set up, I can create an instance of the interface, an instance of the FancyReportingTool with this interface and then call the ShowEmplyeeList on the FancyReportingTool object. This will get the employee data from my internal SAPSystem, convert it and use the third party library to print the data to the console. Output with data from the adapter In addition to the employee adapter, I could implement another adapter. The additional adapter could work as a data provider for my inventory and then send it to the third-party library for a nice print out.Related PatternsIn my eyes there are three related patterns which are worth to take a look at when learning the adapter pattern. Repository Strategy FacadeThe repository and strategy pattern often implement an adapter whereas the facade is pretty similar to the adapter pattern. The difference between facade and adapter is that the facade usually wraps many classes to simplify interfaces. On the contrary the adapter mostly only wraps a single class.ConclusionIn this example, I showed how to use the adapter pattern to combine two incompatible classes. This pattern is often used in combination with third-party libraries which have to be used.You can find the source code of the example on GitHub." }, { "title": "Visitor Pattern", "url": "/visitor-pattern/", "categories": "Design Pattern", "tags": "C#, Software Architecture", "date": "2017-12-17 18:55:46 +0100", "snippet": "Today I want to talk about the visitor pattern. It is a powerful pattern and I think it is used too little. Maybe because it looks complex but once you got how it works, it is pretty easy and powerful. The visitor pattern belongs to the behavioral patterns.DefinitionThe Gang of Four defines the visitor pattern as followed: “Represent an operation to be performed on the elements of an object structure. Visitor lets you define a new operation without changing the classes of the elements on which it operates.”Goals Remove duplication of code Separates an algorithm from an object structure by moving the hierarchy of methods into one object. Helps to ensure the SRP Ensures that we can add new operations to an object without modifying itUML DigramAs already mentioned, the visitor pattern might look complex when you only look at the UML diagram. So bear with me. At the end of this post, you will be able to understand and to implement it. On the UML diagram, you can see the elements of the visitor pattern. The important parts are the Visitor interface which is implemented by the ConcreteVisitor. This visitor has a Visit Method which takes a ConcreteElement as parameter. You need a Visit method for every ConcreteElement you want to work on.On the other side, you have an interface which implements the Accept method with IVisitor as parameter.Now the client can make a list of IElement and call Accept and each element. The Accept method then calls the Visit method of the ConcreteVisitor. The ConcreteVisitor does the calculations and stores the result in a public property. After the calculation is done, the client can access the result by using the property of the ConcreteVisitor.If you want to add a new ConcreteClass, you only have to add a new Visit method to the IVisitor and implement it in the ConcreteVisitor. So you don’t have to modify any other classes.If this sounds too complicated don’t worry. Following I will show how an implementation works without the visitor pattern and then I will refactor it to use the pattern. This will point out the advantages.Implementation without the Visitor PatternI want to create an application which calculates the pay of a person. The pay consists of a salary class which has the salary before and after tax as properties. Also added is a bonus which will be calculated in the Bonus class depending on the revenue. Lastly, I subtract the costs of the marketing from the pay. The person class has a list for the bonus, marketing and salary.After all the classes are set up, I can add elements to the lists of the person. Set up person To calculate the pay of the person, I have to create a foreach loop for Salary, Bonus and Marketing. Calculate pay of the person The program calculates the pay and then prints it with the name of the person to the console. This works fine. But what if I want to add another income source, for example, business expenses. In this case, I have to add a new class BusinessExpenses, add a new list for the BusinessExpenses to the person class and I also have to add a new foreach loop to the calculation of the total pay. Basically, all classes have to change to implement the business expenses. This violates the Single Responsible Principle.That’s where the visitor comes into play. You can find the solution on GitHub.Implementation of the Visitor PatternThe first step is to implement the IVisitor interface, containing three Visit methods with Salary, Marketing and Bonus as parameter. IVisitor interface If I want to expand the functionality of my program, I only have to add a new Visit method to the interface. Next, I implement another interface. I call this interface IAsset. The interface has only one method, Accept with IVisitor as parameter. IAsset interface The Salary, Bonus, Marketing and Person class implement the IAsset interface. All classes implement the Accept method the same way. The visitor calls visit with this as parameter.  There is a slight difference of the implementation in the Person class which I will talk in a second. Implementation of Accept With this change, the lists for the salary, bonus and marketing are not needed any longer in the Person class. I replace these three lists with a list of the type IAsset called Assets. This list contains all assets which are needed to calculate the pay of a person. The Accept method iterates through the Assets list and calls visit on every item of the list. Implementation of Accept in Person class The last step is to implement a class which contains the logic for the calculation of the salary. This class is the ConcreteVisitor from the UML diagram. I call it TotalSalaryVisitor. The TotalSalaryVisitor implements the IVisitor and therefore also implements all the Visit methods. In these methods the actual calculation takes place. The result will be stored in a public property called TotalSalary. This means that the Visit methods for Salary and Bonus add the SalaryAfterTax and BonusAfterTax to the TotalSalary. The Visit method for the Marketing subtracts the MarketingCosts from the TotalSalary. TotalSalaryVisitor implementation Executing the calculationWith everything set up, I can remove the logic from the Main method. I also add the values for Salary, Bonus and Marketing to the Assets list. To calculate the salary of a person I call the Accept method of the person with the TotalSalaryVisitor as parameter. Lastly, I print the total pay by accessing the TotalSalary property of the visitor. Executing the calculation of the pay of a person Adding new AssetsIf I want to add a new asset, let’s say business expenses, I only have to add the new class BusinessExpenses. This class then implements IAsset. In the Visitor, I add a new Visit method which adds or subtracts the business expenses from the TotalSalary property. With the visitor pattern, I was able to extend the application without changing the call of the calculation.Adding new calculationsWith the visitor pattern, it is also possible to add new calculations without changing the program. For example, I want to add a calculation to see the amount of taxes a person pays. To achieve this I only have to add a new Visitor, called TaxVisitor and call the Accept method of the person with this new Visitor.  Added the TaxVisitor You can find the source code of the implementation on GitHub.ConclusionI showed that the visitor pattern is great for decoupling data and the calculation. This decoupling helps to extend the functionality of the application without changing the existing code." }, { "title": "Whats new in C# 7.0", "url": "/whats-new-c-sharp-7/", "categories": "Programming", "tags": ".NET, C#", "date": "2017-10-31 16:11:26 +0100", "snippet": "Recently I made a post, about the new features in C# 6.0 and in this post I will show you whats new in C# 7.0.Whats new in C# 7.0To use all features of C# 7.0 you have to use Visual Studio 2017. In theory, you can also use Visual Studio 2015 but it’s a pain to get it running. I highly recommend using Visual Studio 2017out ParameterBefore C# 7.0 the variable used as out parameter needed to be declared. With C# 7.0 you don’t have to do that anymore which makes the code shorter and more readable. The new out parameter It’s not a big change but it’s a nice simplification of the code.TuplesTuples give you the possibility to return two values at a time. These two values can be different data types. On the screenshots below you can see the call of a simple method which returns two strings. Call of the method which returns a tuple value   Method returns a tuple value To get access the returned values use variable.returnName. The returnName is the name you defined in the signature of the method.To use Tuples you need to target the .NET Framework 4.7. If you target a lower .NET Framework, you have to install the System.ValueTuple NuGet package which can be found here.Pattern matchingWith the new feature pattern matching it is now possible to have switch cases for data types. For example case int: do something, case string: do something else. On the screenshot below I show how to calculate the sum of all int and double values of a list of objects. If the element is a string the program writes the string to the console. List of objects containing different data types   Switch statement for pattern matching calculating the sum of int and double values Literal improvementsThe last new feature I want to talk about is the literal improvement. With C# 7.0 it is now possible to separate const numbers with an underscore to improve the readability. Better readability of long numbers These numbers can be used as normal numbers without an underscore which means when printed the underscore won’t be printed.Code and further readingYou can find the code examples on GitHub. A more extensive post about whats new in C# 7.0 can be found in the official documentation." }, { "title": "Web sites cannot be started unless WAS and the World Wide Web Publishing Service are running", "url": "/cannot-started-unless-world-wide-web-publishing-service-running/", "categories": "Miscellaneous", "tags": "Continous Deployment, IIS, PowerShell", "date": "2017-10-31 12:31:00 +0100", "snippet": "Today I was working on a new Continuous Development task for one of my Projects. I was testing if it works and got an error message. After Fixing the error I rerun the deployment and got an error in an earlier step where I use a PowerShell script to stop the Default WebAppPool. It was strange because the script worked fine before. I found out that the problem was that the World Wide Web Publishing Service wasn’t running. In this post, I will tell you how to fix this problem.The errorDuring the CD I got the following error while executing the PowerShell script: [error]Command execution stopped because the preference variable “ErrorActionPreference” or common parameter is set to Stop: Access is denied. (Exception from HRESULT: 0x80070005 (E_ACCESSDENIED)). The script only contains two lines of code:Import-Module WebAdministrationStop-WebSite ‘Default Web Site’After I encountered this error I connected to the Server and saw that the WebAppPool (IIS –&gt; Sites –&gt; Default Web Site) wasn’t running. I clicked on Start and got the following error message:Starting the World Wide Web Publishing Service (W3SVC)After that, I went to the Services (Computer–&gt; right click on manage –&gt; Configuration –&gt; Services or directly Services in the Start menu) and saw that the World Wide Web Publishing Service wasn’t running. Right-click the service and select Start. Then the service should be running. You can see that on the left column where you should have the Options Stop and Restart the service.After I started the services manually, the PowerShell script could stop the WebAppPool automatically.Further readingFor more information see Technet." }, { "title": "Whats new in C# 6.0", "url": "/whats-new-c-sharp-6/", "categories": "Programming", "tags": ".NET, C#", "date": "2017-10-29 17:26:58 +0100", "snippet": "C# 6.0 is around for a while but in the last couple weeks, I spoke with many programmers who don’t know anything about the new features. Therefore I want to present some of the new features in this post.To use C# 6.0 you need at least Visual Studio 2015 which has the new Roslyn compiler which is needed for C# 6.0. Theoretically, you could install the compiler for older versions but I would recommend using the new Visual Studio versions if possible.Whats new in C# 6.0I will only present a few features of C# 6.0 in this post. For all new features please see the official documentation.Read-only propertiesthe new read-only properties enable real read only-behavior. To achieve this, just remove the setter as shown below. String interpolationString interpolation is my favorite new feature of C# 6.0. This new feature replaces the string.format and makes it easier and faster to combine strings and variables. To use this place a $ in front of the quotation mark of the string. Now you can write a variable directly into the string. You only have to put the variable in curly braces. Expression-bodied functionExpression-bodied functions can help to reduce unnecessary lines of code. You can use this new feature only when the method has only a single statement. Below you can see an example on how to use expression-bodied functions.I don’t use this function too often because I like the method block. This makes it easier to read the code for me. Using staticUsing static brings some syntactic sugar to C# 6.0. You can declare a namespace static as shown below.After you did this, it’s not necessary to qualify the class when using a method. For example, it is not necessary to use Math.PI. When you only have to use PI, the code gets easier to read. Null-conditional operatorEvery programmer who uses objects has encountered a null reference exception. To prevent this C# 6.0 introduces the null-conditional operator. To prevent the throwing of a null reference exception, place the Elvis operator (?) after the element which might be null. The example below shows that if the person is null, “unknown” will be returned.Without the ?? “unknown” part, null would have been returned. If you don’t return a value in case of null, you have to make sure that the left side of the = is a nullable value. NameofNameof enables the developer to get the name of the variable. I used this feature for logging. With nameof I logged the class in which something happened. In the example below, you can see how you can achieve that. Code and further readingYou can find the code examples on GitHub. A more extensive post about whats new in C# 6.0 can be found in the official documentation. " }, { "title": "How I learned programming - Part 5", "url": "/learned-programming-part-5/", "categories": "Programming", "tags": "Books, Learning, Pluralsight, Youtube", "date": "2017-10-29 13:00:49 +0100", "snippet": "Welcome to part 5 of how I learned programming. In my last post, I wrote about how I got into web development and what problems I encountered while learning. You can find this post here.In this post I will talk about how I teach myself new technologies and programming related topics at home using Pluralsight, Youtube and books.YoutubeYoutube is great. There are videos on every imaginable topic and it’s free. The only downside I see with Youtube is that the quality of the videos is very different. It might take some time to find channels which produce high quality videos and which you like.PluralsightPluralsight is an online video learning platform with thousands of high quality videos and in my eyes the best source for studying. The advantage over Youtube is that all the videos are reviewed and only high quality content is uploaded. You also have the chance to ask questions to the videos and can download the slides and source code, used in the videos.The only downside is that it’s not free. The costs are 29$ monthly or 299$ yearly. In my eyes, it is totally worth it and every developer should have an account there. If you have a MSDN subscription you get some free access to Pluralsight, depending on your subscription. Visual Studio Professional grants you access up to 30 courses per year. The Enterprise edition gives you unlimited access to all courses for a year.BooksNot everyone likes reading books but I highly recommend you doing it. There are some must have read books in my eyes like “Clean Code” from Uncle Bob or “Test-Driven Development by Example” from Kent Beck. I like reading books on the train or during cardio in the gym. Sometimes I also like eBooks because it’s so easy to copy code examples and try them myself.I know that books are not cheap but it’s really important to invest in yourself and stay up to date with new technologies.Try it outSometimes it’s also nice to sit down and just play around with something new. This way I learned my way around the Azure Portal. Sometimes you don’t know anything about a new technology and therefore it’s nice to learn the basics before diving into it. I have to admit that I barely just dive into something without any tutorial or videos.That’s the story of how I learned programming. If you have any questions, feel free to contact me or leave a comment below." }, { "title": "How I learned programming - Part 4", "url": "/how-i-learned-programming-part-4/", "categories": "Programming", "tags": "ASP.NET MVC, CSS, HTML, Javascript, Learning, PHP", "date": "2017-10-17 21:37:49 +0200", "snippet": "Welcome to part 4 of how I learned programming. In my last post, I wrote about how I learned Java and C++ in Canada and how the teacher there impressed me. You can find this post here.In this post, I will talk about how I got into web development and what problems I encountered while learning.HTML, CSS and Javascript… UghBack in 2013 web development wasn’t fun at all. Especially when you are a beginner. HTML5 wasn’t standardized yet and there was a big problem with the browser compatibility. Especially with IE6. You could design a website, test it with every browser and it looks the same. Then you try Internet Explorer and it looked like something completely different and totally broken.Anyways, I started learning HTML, CSS and Javascript first in theory in class and then with an assignment.Creating a company homepageThe first assignment was to create a homepage for a company. The following requirements needed to be implemented: Use a 3 column grid layout with Bootstrap Use the HTML5 semantic tags (aside, nav, header, footer,…) Header: Logo + Headline with Web fonts  (https://www.google.com/fonts) Nav: navigation elements with CSS transition (highlighting) Aside: Welcome box Section: 2x article incl. Bootstrap icons and placeholder text Footer Implement a register form (doesn’t have to send any data to the server) Show the geolocation in the welcome box Use rounded corners, shadows and other CSS attributes Use Bootstrap, jQuery and jQueryUIDoing this assignment wasn’t too hard. When learning HTML the w3school homepage is your best friend. Your first couple projects will look like websites from the 90’s. Here you can find my 90’s company website.Next step: PHPNext, I learned PHP. PHP is a scripting language and contrary to Javascript, the code is executed on the server. For the assignment, I had to implement an URL shortener, like bitly.com. The following tasks had to be implemented: Non logged in users can open the start page and can call shortened URLs can log in or register shortened URL is valid for 24 hours A logged in user can show / edit own URLs show / edit user profile logout shortened URL is valid indefinitely The starting page offers the following functions Displays information about the service Offers to shorten entered URLs Shortened URLs consists of the server address and an 8 character long alpha numeric unique sequence Shortening URLs redirect with .htacess / web.config the page contains the key for the shortened URL as a query string redirect with correct HTTP response code Signup / Login user signs up with Email, username and password optionally user can enter the first and last name, address and birthday login works with username or email and password Show / edit own URLS List contains creation date and number of redirects URLs can be activated / deactivated or deleted (with an AJAX request, which I have not implemented though) Show / edit user profile all field from the sing up are displayed and can be edited for critical changes, the user has to re-enter his password (changing email, username or password) Misc user Separation of Concern use useful HTML tags use different architectural layers (UI, business logic, database,…) implement OWASP best practices use jQuery and Bootstrap My implementationThis assignment was fun to do but it also showed that PHP can be a pain if it’s mixed too much with HTML. Also setting up the database took my some time because we had to use PHP_pdo to connect the database. I can’t remember the details but for whatever reason, I couldn’t get it working with PHP 5.5 and so had to use 5.4. I had to use Visual Studio and installing the PHP tools didn’t work at the first time either.For the assignment, I implemented most of the features. I didn’t implement the deletion of the URLs with Ajax and also didn’t apply all security guidelines. You can find my solution here.Learning ASP.NET MVCThe last part of learning web development was ASP.NET MVC. In theory, it’s pretty simple. You have Separation of Concern and the Controller takes the user input, modifies the Model and then sends the View to the user’s browser. ASP.NET is great because you have the full Visual Studio support and intelli sense and it also has already most of the security features built in. I understood all theoretical parts but I just didn’t get how it works in code. I sat in front of my Visual Studio and had no idea what was going on. Therefore I didn’t do the assignment. Around a year later I tried again to understand it. I made some progress but gave up again. A bit later I convinced myself that it can’t be that hard and tried again. This time I understood everything.Now I can’t even explain why I didn’t understand it back then. But it shows that even if you don’t understand something the first or second time, don’t give up. After I started understanding it, I really liked it.If you are wondering about the assignment. Here are the tasks of the assignment:Create a portal for blogging using ASP.NET MVC anonymous users: see an overview of the blogs go through the archive (/archive/month/year) read blogs filter tags search for tags or blogs registered / logged in users create a blog edit a blog display / edit user profile Blog entry title friendly URL content author tag(s) timestamp Misc user Separation of Concern use useful HTML tags use different architectural layers (UI, business logic, database,…) implement OWASP best practices use WYSIWYG-Editor, jQuery and Bootstrap  This is the story how I got into web development. In the last part of this series, I will tell you how I teach myself new stuff at home.Next: Part 5Previous: Part 3" }, { "title": "How I learned programming - Part 3", "url": "/how-i-learned-programming-part-3/", "categories": "Programming", "tags": "C#, Learning", "date": "2017-10-16 20:48:28 +0200", "snippet": "Welcome to part 3 of how I learned programming. In my last post, I wrote about how I learned C# in university in Austria. You can find this post here.In this post, I will talk about studying in Canada and the differences to Austria.Moving to CanadaI went to Canada for two semesters. There I had Java, C++, Assembly Language and Algorithm and data structures classes. Assembly was nice to see but honestly, I didn’t need to have it the whole semester. On the other side, you learn to value the simplicity of high languages like C# or Java. In the algodat class, I had to implement different kinds of sorting algorithm and also build my own stack, queue and list.The interesting classes were Java and C#. It was interesting to see how other programming languages feel like and how pointers in C++ work. The most mind-blowing part about studying in Canada was how teachers and tutors supported the students. I didn’t expect that at all and taught me a great deal on how to teach others.Mind-blowing teaching skillsA huge difference between my Java class and the previous C# class was the length of the class. In my last post, I told you that my C# class was for 7 hours straight. The Java class was twice a week and only 1.5 hours long. It makes such a big difference when learning new stuff. After the seven hour-class, sometimes I couldn’t remember anything about my way home, although I drove home. With this short classes, you still learn a lot but not too much which would overwhelm you.Additionally, to the theoretical class, there was a 1.5-hour lab once a week. We were between 10 and 12 students with one professor and one tutor. We got a small assignment which we had to solve during the lab. The big difference to my lab in Austria was that the teacher did actually help you. If you had a question he came over and explained it to you and also gave hints on how to solve the problem if you got stuck. This was just mind-blowing for me. The tasks were pretty easy for me but it showed me that teachers actually can be useful.Every week we got some assignments. Usually around five small problems to solve. After handing them in, I had to see the tutor and explain my programs to him. Then he asked me some questions about my implementation. In this conversation, I learned probably as much as while doing the assignment. This was another wow effect for me. I think that these experiences impressed me so much that I still follow them when I teach someone or even when I help colleagues with a problem.Another awesome classThe C++ class was similar to the Java one. The class was twice a week for 1.5 hours. There was no lab for this class and therefore the assignments were a bit longer. I also had not to show my assignments to a tutor. A big (and nice) difference to Austria was the exam. The exam was on the computer and we had to solve some small assignments. We were allowed to use every source of information we wanted. Such an environment makes it way more realistic to a working situation. If you don’t know anything at work you just google it.That’s it with my story about studying in Canada. If you have the chance, I highly recommend you to study abroad. In the next chapter, I will talk about how I got into web development and how I taught my self new stuff.Next: Part 4Previous: Part 2" }, { "title": "How I learned programming - Part 2", "url": "/how-i-learned-programming-part-2/", "categories": "Programming", "tags": "C#, Learning", "date": "2017-10-16 13:40:17 +0200", "snippet": "Welcome to part 2 of how I learned programming. In my last post, I wrote about how I started learning C# in university in Austria. You can find this post here.In this post, I will continue my story about how I learned C#.Learning inheritance and more OOP conceptsAfter my second assignment, I had another two theory blocks. There I heard the first time about inheritance and abstract classes. With this new information, I still didn’t understand the concept to its fullest but it started to make more and more sense.The next assignment was designed to understand classes and inheritance more. The task was to create a console application in which the user can create different objects. These objects were a circle, rectangle and diamond. Every object had a border color, body color, name, starting coordinates and specific attributes according to the object type. The circle had a radius, the rectangle length and width and the diamond had rows.After the user entered his objects, these objects were painted. Additionally, the user could select one and move it around in the console. The user could switch between the objects and also change their Z- level. The Z-level decided which object was in front and covered the other ones.The program also could display the entered attributes of the object plus calculated attributes like the area or diameter.This assignment was my favorite of this semester and the source can be found on GitHub.Learning about interfaces and improving the previous assignmentBefore my fourth assignment, I had one theory class where I learned about interfaces. To be honest it took me way longer than this semester to understand interfaces but during the fourth assignment, I could at least see why they are useful.The fourth assignment was an extension of the third one. The task was to implement interfaces to enable the user to use a color or monochrome renderer. Additionally, the user should be able to sort the objects by their area using the IComparer interface.The source code to my solution can be found on GitHub.The last assignment was something about creating a file structure in the console using event.Network programming and source controlAfter the programming class, I had a two-week intense class called project week. This class was Monday to Friday from 8 to 6. The idea of this class was to see how it feels to work in a team on a “real” project. It started with gathering the requirements, designing the ideas, refining them with the stakeholders and then implementing it.The project goal was to implement Pacman which can be played over LAN. The player started the application on his computer and other clients could join his game. After the game start, the map was split between all logged in clients. The player could always see the Pacman on his screen but the Pacman went from one computer to another when moving around the map. We set up a whole computer room and at the end of the project, we could watch the Pacman move from one screen to the next one. This was pretty cool to see what we achieved in such a short time.Before we started this project we had a short introduction to network programming with TCP and UDP and also about source control. We only learned that TFS is a source control and with that, we can work as a team together. We have to check out a file if we want to work on it and then check it in after we are done. This was all we knew and all we needed at that time. You can find the source code here.Teach yourselfIf you are interested in network programming you can start with reading the documentation to TCP here. Start with implementing a simple chat tool with one server and clients which can connect to the server. If one client sends a message, all connected clients get this message. As next step try to implement the same program using UDP.After the second assignment, I implemented my own Snake game in the console. It was actually easier than expected and only took me around two hours (obviously there is much to improve). It was a nice confidence boost to see that I could make my own game and also fun playing it afterwards. You can find the source code here.I highly recommend to sit down at least once a week and implement something. It doesn’t even matter what it is, as long as it’s a challenge.In the next post, I will talk about new experiences on how to get feedback from a tutor and what I learned in Canada and later how I got into web development.Next: Part 3Previous: Part 1" }, { "title": "How I learned programming", "url": "/how-i-learned-programming/", "categories": "Programming", "tags": ".NET, C#, Learning", "date": "2017-10-15 23:07:59 +0200", "snippet": "I learned programming in university in Austria. Before going to university, I only knew that there is the for loop and the if condition. This was all I knew. Unsurprisingly you have many theoretical classes like requirements engineering, software design and introduction to computer systems in your first semesters. This classes also helped to understand the bigger picture of programming but in this post, I will focus on the practical parts.The journey beginsIn my first semester, I had a class called logic and logic programming. The logic programming was programming with Prolog. If you have never heard about this programming language, be happy. In Prolog, everything works with lists and recursion. This is probably the worst language to start learning programming since recursion is not that easy to understand in the beginning and so you can’t understand what’s going on during debugging either. For me, there was a lot of magic happening and I couldn’t tell why either.Also, our assignments were not beginner friendly at all. In class we learned that with a logical conclusion you get to the result. For example, a BMW Z4 is faster than a Ford Focus and the Starship Enterprise is faster than the BMW. Therefore, the Enterprise is faster than the Ford. So far so good. The assignment then was that user enters the plan of a building and how they are connected and a starting point. The program should calculate all possible exits and store them in a list. Every exit could be selected only once. Maybe the assignment was a bit different but it was something like that.This class is at least in the top 3 of the biggest bullshit classes of 5 years studying. I think it’s a terrible way to start learning programming and it made me almost quit. Honestly no idea how I passed. In the second semester I had C# instead and from this moment on I loved programming.C#: The start of a love storyIn this chapter and in the next part I will describe what I learned in theory in class and what assignments I had. I will also show my implementations of these assignments. Bear in mind that I would do them, with the knowledge of today, completely different. I will link my GitHub repository.As already mentioned, in my second semester I had programming in C#. I had one class a week for seven hours (from 10 to 5). During this class, we had a big theory block and then lab. In the first theory block, I learned that different kinds of loops and the if and switch condition exists. I also learned that it’s possible to read user input in the console and print out something in different colors to the console. The last thing I learned was that a data storage called array exists and that I can save different values there.After these around 150 slides, I had my first lab. The assignment for this lab was to program a console program which paints the sinus curve and lets the user move to the left and right and always displaying the cursor at the current location of the curve. I had no idea how to do that at all and the only answer from our instructor was “You have to figure that out by yourself”. Since this lab didn’t give any grades I didn’t worry too much and looked forward to the first of five assignments.My first assignment: The Matrix CalculatorThe first assignment was to program a console application which lets the user enter two-dimensional matrices. Each matrix had a name which had to be unique. The user could enter up to 10 different matrices. Every matrix started with opening square brackets and ended with closed square brackets. The values were separated by a comma and a row was ended by a semicolon. An example for a valid matrix is A=[1, 2, 3; 7, 8, 9; ]. An invalid matrix would be A = [1, 2, 3; 7, 8;] (The name is already taken and the second row has fewer values than the first one).After the user finished his input, he could perform different calculations with the entered matrices. For example, A + B should add the previously entered matrices A and B and then display the result. The program must not crash at any point when the user entered invalid inputs. This was fastidiously tested which in my eyes is not useful at all because it’s way more important to understand what you are doing than testing every single input (at least when you start learning how to program).The implementationThe program had to be handed in within two weeks and it took me around 60 hours to finish it. Within this two weeks, I spent a couple nights in front of my computer and tried to figure out how to solve this problem. For the data input, I used a three-dimensional array. Up to today, this is the only time I used one. At that time I didn’t know better than using the first dimension for the name and the second and third dimension for the data. I also wasted a lot of time with testing every single input for all the possible wrong user inputs. Programming the calculations was done quickly because in my first semester I already did this in Matlab. So I could basically copy that. Otherwise, the assignment would have been even a couple hours longer.Another useless guideline from my school was that we had to use Style Cop with all comment settings activated. Handing in an assignment with a single Style Cop warning would lead to zero points. Comments can be good and necessary but in that excess, we had to do it and was counterproductive and lowered the readability and was a huge pain in the ass.Today I know that objective programming and lists exist and therefore my solution today would look completely different than back in the days. If you are interested in my solution you can find it on GitHub.The second assignment: Flight Route CalculationPrior to the next assignment, I had two theory classes. In this classes I learned that it’s possible to overload methods, to catch exceptions with a try catch block and to start a program with command line arguments. I also heard the first time about object-oriented programming which allows you to create classes in your program. To be honest, I didn’t understand the concept fully at this time (which you can see in my assignment because I only had one class which had over 450 lines of code). Another great feature I learned was List. Finally, I could store data dynamically at run time.Equipped with this new knowledge I got my second assignment. The program should take flight routes from the user and then calculate all possible flight routes between two destinations. This program was similar to the flight plan calculator in Prolog which I described earlier.My implementationAfter figuring out how recursion works, this assignment wasn’t too hard anymore. It was again a lot of input testing and printing the menus. Additionally, to the user input, it is possible to start the program with some command line arguments. I can’t remember the syntax of the arguments but in the program, it was again just input testing.An example for the user input is: Paris-London Paris-Vienna Vienna-Zurich London-ZurichAfter the routes are entered the user can enter his query, for example, Paris-Zurich. Now the program searches all possible routes from Paris to Zurich and Prints: Paris-Vienna-Zurich Paris-London-ZurichThe tricky part here is to know if the route was already found and when to stop. Otherwise, you will end in an infinite loop.You can find my original solution on GitHub. Again, today my solution would look completely different. Back then I didn’t understand object-oriented programming and had never heard of clean code or TDD.This was the first part of how I learned programming. In the next part, you can read about the second part of the semester.Next: Part 2" }, { "title": "Garbage Collection in C#", "url": "/garbage-collection-c/", "categories": "Programming", "tags": "C#", "date": "2017-10-15 17:03:22 +0200", "snippet": "The garbage collection is a great feature which makes the life of a programmer was easier. It releases unused variables and doing so frees memory on the heap.  If you know C++ or C, you know what a pain it can be to release all unused object and variables and how easily you can forget one which will lead to a memory leak. A memory leak is caused by a variable which has a value but no reference. This means that the variable can’t be deleted and remains in the memory until the computer is restarted.In C# the garbage collection works fully automatically and the programmer doesn’t have to worry about it. But how does it work?Garbage Collection and variable generationsWhen a new variable is generated it will be placed in generation 0. The garbage collector is invoked after the generation 0 filled up. The unused variable will be deleted and the memory will be released from the heap. Variables which survive this process will be pushed into generation 1. If generation 1 is full the process will be repeated. C# knows the generations 0, 1, 2. Lower generations will be checked more frequently.The programmer can’t start the garbage collection process. Going through all variables is an expensive task and comes with the cost of lower performance.Further readingIn this short post, I only covered the basics on how the garbage collection works. For more detailed information see the provided links.https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/http://aspalliance.com/828https://www.codeproject.com/Articles/1060/Garbage-Collection-in-NET" }, { "title": "Scrum Part 6 - Rules", "url": "/scrum-part-6-rules/", "categories": "Miscellaneous", "tags": "Agile, Scrum", "date": "2017-10-12 22:18:36 +0200", "snippet": "Scrum has a set of predefined rules. In this last post about the Scrum fundamentals, I will give you an overview of the different rules.Product Backlog RulesThe Product Backlog must be visible and available at all time. The Product Backlog is usually digital, for example in TFS or Jira. The top Backlog Items should be as detailed as necessary so everyone on the team can understand it. One feature should not take more than one day of work. If the feature takes too much time, it needs to be broken down into separate, smaller features.Development Team RulesDo the Daily Stand-up every morning. The team is also responsible to manage its own process. This means that the team is responsible for updating the progress at least once a day.The team should never change within the sprint. It can be a big disruption if the team is changing which can lead to not reaching the set goals.The development team has to make sure that the Definition of Done adheres. The last rule is that the team is responsible to track the velocity of the sprint. This can also be done with tools like TFS or Jira. If you already use TFS as your source control, I highly recommend you using it as your sprint planning tool too.Product Owner RulesThe Product Owner is responsible for the Product Backlog. This includes ordering the items according to their priority and making sure that they are well enough described so that the team can understand them. If the Product Owner makes a decision, the organization must respect this decision.Scrum Master RulesThe Scrum Master is responsible for facilitating all scrum meetings. He is a coach or moderator who helps the team to solve problems and improve the quality of the work. The Scrum Master does not decide which work will be done. Often someone with management experience works in this role. It’s also possible that the Scrum Master is someone from within the team.Sources and further readinghttps://app.pluralsight.com/library/courses/scrum-fundamentals/table-of-contentshttp://scrumguides.org/https://www.scrum.org/ Previous: Scrum Part 5 – Meetings" }, { "title": "Scrum Part 5 - Meetings", "url": "/scrum-part-5-meetings/", "categories": "Miscellaneous", "tags": "Agile, Scrum", "date": "2017-10-12 22:16:42 +0200", "snippet": "Every Scrum meeting is time-boxed. But what does time-boxed mean? I will explain this term and afterwards, I will present all Scrum meetings.Time-boxingEvery meeting is a time-boxed event. Time-boxed means that there is a maximum duration. This means if a meeting is scheduled to last two hours, then the meeting is over after latest two hours. Time-boxed doesn’t allow to extend the meeting, but it’s possible to finish the meeting early.Due to the time boxing, the participants focus better on getting the best results in the given time.SprintThe Sprint is a time-boxed event which consists of different events. A sprint usually lasts between 2 – 4 weeks. An advantage of a short sprint is that the risk of failing is limited. It’s unrealistic to plan 5 or 6 weeks ahead. This long timespan results more in guessing than planning. Planning only 2 – 4 weeks ahead mitigates the risk of not being able to produce a shippable product at the end of the sprint.Limited time helps to create and keep the focus on the work currently needed to be done. Seeing a goal which will be reached in 2 weeks is way easier than seeing a goal which will be reached in 6 weeks. A nice example is the weather forecast. The weather can be predicted maybe a couple of days in advance more or less precise. If you look at the forecast for ten days later it’s more a recommendation than a forecast.A sprint is a learning loop. It consists of a plan, do, check, act or learn, build and measure. No matter what definition you prefer, try, learn, try again. For example, a two-week long sprint means that a new version of the software will be delivered 26 times a year. This means 26 times feedback from the customer and 26 chances to improve the own process within the development team.It’s important to note that to change bad things you actually have to change. Changes can be hard for a team but they should be seen as a chance to improve the building process which results in a happier customer.VelocityThe productivity can be measured in velocity. The velocity is measured every sprint. After some sprints, the team will be able to make a forecast on how much work they can get done in the next sprint. There is no unit for the velocity. This could be finished features, story points or whatever fits your team best.If the team finished on average between 50 and 60 units over the last five sprint then it is likely that the team will finish between 50 and 60 units in the next sprint too. This helps you to choose the right amount of features for the sprint. If you estimate your features will take you 25 units to finish then the team either estimated too positively or took too little features. If this occurs the planning should be revised. It’s also unlikely that the team can suddenly finish 80 units in one sprint.It’s common to start and finish each sprint on the same day. I like to start on a Monday and finish on a Friday. So the team comes back hopefully motivated and fresh from the weekend and goes into the weekend with a finished sprint.Sprint PlanningThe Sprint Planning is a time-boxed meeting which should last max. 2 hours for every week of the sprint. The whole team comes together to talk about uncertainties and plan the tasks for the coming sprint. It is also possible to invite external people, for example, domain experts who can help by clarifying some problems. During the Sprint Planning, the Sprint Backlog and Sprint goals are created. To plan a sprint, an ordered Product Backlog is needed. Otherwise, the team can’t decide which tasks are important and which are not. Therefore the PO should also attend the Sprint Planning.The team takes the tasks one after another and discusses them. If they think that they can implement the task in the sprint, then they will add it to the Sprint Backlog. After the Sprint Backlog is finished, the team will talk about the plan on how to deliver the chosen tasks. They also set a Sprint goal. At the end of this discussion, the team will be able to explain to the PO what the goal is for this Sprint and how they want to achieve it.Planning PokerThe Planning Poker is a technique to estimate the effort of a feature. First, the Product Owner explains a feature. Then the development teams discuss the feature and if necessary, asks questions. After this discussion, the actual planning takes place. Everyone selects a card (a physical card or in an app) and then all team member present their selection at the same time. Let me give you an example why it’s important to reveal the card at the same time. For example, the lead developer presents his estimation first. His estimation will influence every team member and their results will be close to his. If every estimation is presented at the same time, the range of estimates will be wider.Unit of workAs a unit of work, you can choose whatever fits your team best. I like using story points with the values of Fibonacci (1, 2, 3, 5, 8, 13, 21 and 40). 40 is not a part of Fibonacci but if a feature takes 40 story points, it’s too big and needs breaking into smaller pieces. The reason why I like these values is that I can see that 21 one is a bit less than double of a 13. It is way harder to differentiate between 13 and 15. If everyone estimated the same amount for the feature the round is finished and the result will be put onto the Sprint Backlog Item.If the outcome is different, the team has to discuss why the member chose different values. For example, if the outcome is 3, 8, 8, 13, 21 then the person who selected 3 and 21 should explain why they chose this number. This does not mean that they are wrong at all. This could mean that they haven’t understood everything or maybe the whole team didn’t see a specific detail which made the feature look way harder or easier. After the discussion, the poker round is repeated.Choosing an appropriate amount of workThis estimation process is repeated until all items are estimated. After the estimation is finished, every item has a value of story points. This can be mapped to hours of work. But that is not necessary. If your team has a velocity of 50 story points and your Sprint Backlog has items with 49 story points in total, you know that these items will take you the whole sprint to implement. I have to admit that it might take a bit to get used to. Story points are not comparable between two teams. If team A estimates 50 story points and team B estimates 150 story points, it could result in two weeks of work for both teams.I have also seen teams using T-shirt sizes instead of story points. The sizes are between small and XL. In the end, it will have the same outcome as story points. If this approach works better for your team, use it.Daily Scrum (Daily Stand-up Meeting)The Daily Stand-up Meeting is a daily meeting for and by the development team. The time box is 15 minutes. This meeting serves as a quick check on what’s going on in the team. In this meeting, the team makes a plan for the current day. If the team has some problems which it can’t solve by itself, it’s the Scrum Masters responsibility to help the team to get these problems solved.Every developer has to answer 3 questions. These questions are: What did I do yesterday? What will I do today? Are there any impediments in my way?Only one person talks at a time. If there is anything else that needs to be discussed and doesn’t affect the whole team, it should be discussed after the meeting with only the needed people.Sprint ReviewThis meeting is conducted at the end of the sprint. The goal is to share the result within the organization or with the stakeholders. Interested people of the organization could be the management but also marketing. The time box is set to 1 hour per week of the sprint. A shorter time span is more likely to attract more people. During the Sprint Review, the team shows the implemented features and also asks about feedback of the attendances. The goal is to get feedback which can result in new Product Backlog Items. These new items will lead to an improved quality and customer happiness.Sprint RetrospectiveThe Sprint Retrospective marks the end of the sprint. After the Sprint Retrospective is over, the sprint is over. Attending is the entire Scrum Team (PO, SM and the team). Outsiders are not present in this meeting. The Sprint Retrospective is a chance to reflect over the last sprint and talk about what the team can improve to achieve better results with the next sprint. If the team has issues finding problems or can’t decide which problems need improvement the most, then the Scrum Master should guide the team to a solution.The meeting is also a chance to talk about the Definition of Done. Maybe the definition needs refinement. This can be discussed within the whole team.How my team does the RetrospectiveIn my team we have a white board with 3 columns: the good the bad informationEvery team member prepares post-its and while sticking the post-it to the white board, he says a couple of words about the post it. After all post-its are on the board, we try to group them into three groups. The groups could be improvements, emotions, processes and so on. After we have defined the groups, everyone places two points to the most important topic and one point to the second most important topic. After all points are placed we talk about the category with the most points and try to find ways to improve in the next sprint. It is also possible that the team focuses on good things and decides to try to keep something which went really well at the current high level.If you hear for the first time about these meetings, you might think that the whole team will spend too much time in meetings and won’t get work done. In my experience when applying Scrum, the team spends less time in meetings. Most meetings are status updates or other coordination tasks which will go away when using Scrum. Next: Scrum Part 6 – RulesPrevious: Scrum Part 4 – Scrum Artifacts" }, { "title": "Scrum Part 4 - Scrum Artifacts", "url": "/scrum-part-4-scrum-artifacts/", "categories": "Miscellaneous", "tags": "Agile, Scrum", "date": "2017-10-12 21:43:36 +0200", "snippet": "Scrum has a set of predefined Scrum artifacts. In this post, I will give you an overview of the different roles and talk about my own experience.Product Backlog Items in the Product Backlog can be called features, backlog item, change requests, bugs, requirements, defects… Contains all features of the product The Product Backlog is owned by the Product Owner Every item has a description, an order and an effort estimation Constantly changing (developers implement features or new features will be added) Items on the top are higher prioritized and therefore described in more detailThe Product Owner and the team should work together to prioritize the tasks in the right order and give each other feedback if a feature needs a better description. An example for making a prioritize change by the team could be a feature which is highly technical and also really important to be implemented. If it’s too technical the PO probably doesn’t understand it or can’t see its importance. In this case, the team has to explain the details to the PO to get him on the same page.The Product Backlog should be managed with a tool like TFS. If you use TFS as your source control too, it’s possible to link a feature with a commit to increase the tractability of changes. In the following, you can see the Product Backlog in TFS. Source Sprint BacklogThe Sprint Backlog is a subset of selected Backlog Items for the current sprint and also a plan for implementing them into a product. The remaining work should be summed up at least daily (Burndown chart). The Sprint Backlog belongs only to the development team and shows all necessary work to reach the sprint goal. The items in the Backlog can change several times within a sprint since the team gains more experience about the work needed to be done during the sprint. The Sprint Backlog should be seen as a forecast on what will be done instead of a commitment.Personally, I like to display the Sprint Backlog on a whiteboard. On the following screenshot, you can see my current whiteboard. I whitened business relevant delicate tasks. Kanban Board for the Sprint Backlog My Kanban Board for the Sprint BacklogOn our board we have the following columns: Backlog: current sprint Backlog Items Prioritized: important tasks which should be implemented soon Development In Progress: features which are currently implemented Development Complete: features which are implemented but not forwarded for the review yet Team To Verify In Progress: according to our Definition of Done, an implementation has to be reviewed by a second team member Team To Verify Complete: the feature was reviewed and can be deployed to the test server Customer  To Verify In Progress: the feature is deployed on the test server Customer To Verify Complete: the customer reviewed the feature and gave his ok Done: the feature is deployed onto the live serverWe also have a small area where we can write down impediments or tasks we want to improve in the current sprint.Every team member has a magnet with their picture on it. So, you can easily see who is working on what. The flash symbol means that the work on this feature is blocked. Next to the white board we hung our Definition of Done. It’s good to have a look at it every now and then.Sprint GoalThe Sprint Goal is set during the Sprint Planning. It’s a decision made by the development team and Product Owner after selecting and estimating the tasks for the sprint.Definition of DoneWhen is a developer done with his task? When the code is checked in? Code committed? Product shipped?The answer is: it depends. It depends on the product you are working on and on the team. If you are working on a simple calculator then the definition of done won’t be too strict. If you are working on a software which controls parts of a spaceship your definition of done will be long and strict.I worked on a team where done meant that the programmer said that he is done. Committing the code, writing tests, customer feedback… Nothing needed. Only the word of the developer. I guess it’s no surprise to you that the quality of the shipped product wasn’t too high and there were plenty of bugs.For me, done means done. A feature or bug is done when the code is checked in into the version control, all tests passed (with a high code coverage), the branch is merged into the develop branch, the CI build is green and the customer or product owner gives his ok.With all these criteria, a high code quality and customer satisfaction can be achieved.Every team member must agree to the Definition of Done. This definition can vary from company to company. The Definition of Done is also a standard which ensures a certain level of quality.A piece of software can only be in one of two states: done or not done. There is no almost done.Burndown ChartThe Burndown Chart displays the number of remaining features with the remaining time. Tools like TFS also show you an ideal progress which is linear from the start to the end with no work left. Such a linear progress is only in theory possible but the actual progress should be around the theoretical progress. Often the actual progress is like a wave, sometimes over this ideal trend and sometimes below it. On the following screenshot, you can see such a burndown chart from the TFS. Source IncrementThe increment is the result of the sprint, the software delivered at the end of the sprintThe Product Owner determines what to do with it. Maybe it needs refinement, maybe it can be shipped. The result of the sprint only contains finished work. Features which haven’t met the Definition of Done are not included. Next: Scrum Part 5 – MeetingsPrevious: Scrum Part 3 – Scrum roles" }, { "title": "Scrum Part 3 - Scrum Roles", "url": "/scrum-part-3-scrum-roles/", "categories": "Miscellaneous", "tags": "Agile, Scrum", "date": "2017-10-12 21:26:50 +0200", "snippet": "Scrum knows only three different roles: the Product Owner, the Scrum Master and the development team.Product Owner (PO)The Product Owner owns and manages the Product Backlog. A PO is a single person who is alone accountable for the product.Scrum Master (SM)The Scrum Master acts as a manager within the team and ensures that everyone is on the same page. He acts like a moderator in meetings and makes sure that the meetings only last as long as scheduled. The SM is also responsible for mentoring and teaching team members who need help to understand Scrum better. If there are any impediments for the development team, the SM ensures that these obstacles get removed. A good SM helps to increase the skills of the team which leads to a higher performance. Moreover, he also helps to implement Scrum within the company to guarantee an as effective process as possible. The SM does not make technical decisions.Development TeamThe development consists only of developers. The team is self-organizing, which means that the team decides which and how Backlog Items will be implemented during the sprint. Usually, they take the top Backlog Items since these are prioritized the highest by the PO. The team members possess all the needed abilities to deliver a complete product at the end of the sprint.The size of the team is somewhere between 3 and 9 developers. With less than 3 people Scrum doesn’t make much sense since there are no people working together who need to be managed. More than 10 developers bring too much complexity into the project to make progress. However, the PO and SM are not part of this count.The Responsibility is shared between every team member. Since everyone is responsible for the product, sentences like “This is not my problem” should never come up. If there is a problem, it’s everyone’s problem.For more information about the roles check out the documentation.Next: Scrum  Part 4 – Scrum ArtifactsPrevious: Scrum Part 2 – An overview of Scrum" }, { "title": "Scrum part 2 - An overview of Scrum", "url": "/scrum-part-2-overview-scrum/", "categories": "Miscellaneous", "tags": "Agile, Scrum", "date": "2017-10-12 21:15:13 +0200", "snippet": "As already mentioned, Scrum is a framework for how to manage work. It might be incomplete and neither does it solve the problems for you nor does it tell you how to write your code. Scrum only gives guidance on how to solve problems.Scrum is for you when planning and implementing new features or releasing your product takes longer than it should. Also when you see a decline in quality, customer trust and relationship, the morale of the team or amount of work done.Scrum doesn’t use the term project. It uses the term product because the goal is to produce shippable products with every iteration (Sprint).  It helps the organization to have a transparent progress (Burndown Chart). There also must be a pre-defined criterion for being done (Definition of Done). Scrum helps to adapt your process from sprint to sprint (Retrospective). With Scrum, feature after feature gets implemented. This means that only what is needed to build the needed feature will be implemented. For example, only a small piece of the data layer (instead of building the whole layer at once) will be implemented.I will explain all the keywords in the braces, in the next parts.Implementing Scrum in an organization can be hard and might take a long time. Usually, a company has, depending on its size, several levels of management. Every manager gives the employees below him tasks to do. Tasks like fix this bug or implement this feature. However, the idea of Scrum is that the team is self-managing and outsiders only provide help to the team. This means that suddenly these managers have to serve the team and provide help and support. Ideally, a company has high performing, self-managed teams and the management can focus on the strategic direction of the company.Self-organizing, high-performance teamsSelf-organizing means that the team manages itself without help or interference from outside. The team decides what work will be done in a sprint and decides how to implement it. The role of the external management is to see the bigger picture and long-term goals and to remove impediments to the team’s success. This is the key to create a high performing team.If the team takes the ownership of the product, the team feels more responsible for the outcome and therefore is more committed to it. This leads into trying harder to improve the quality. If the team does not support the idea of Scrum, it won’t succeed. A high performing team sees the success as the reward of their hard work. Every team member should reflect on themselves and try to improve their own performance at every sprint. The result of a high performing team is a finished superior product at the end of every sprint.How to introduce Scrum to your companyStart small: do fewer products, less at once, less complex in the beginningThink big, start small: a big bang introduction to Scrum will lead into failure. Start small with a pilot project. Take what you learned in this project and take it further within the organization.Explore and adopt: learn the system within which you operate and then adopt.One team one goal: if every team member has the same goal, you will achieve better resultsFocus on value: focus on what’s the most valuable feature for the customer at the current time.Empower teams: the team has the permission to make mistakes. Mistakes are the best source of growth and learning.Lead by example: the organizational leader must lead by acknowledging the agile process by accepting that the team is self-managing and giving it the freedom it needs. Don’t do it as shown in the following comic 😉 Source Collaborate: Leaders must work not only with the Scrum teams but also with the other leaders within a company.At the beginning, do Scrum exactly as the Scrum Guide suggests. After you are used to the process, try to experiment and start adapting it, so that it fits your team better. Reflect on the changes. If they were good, keep them. If they didn’t improve anything or even made you less productive, revert them. Next: Scrum Part 3 – Scrum rolesPrevious: Scrum Part 1 – Welcome to the future" }, { "title": "Scrum Part 1 – Welcome to agile development", "url": "/scrum-part-1-welcome-agile-development/", "categories": "Miscellaneous", "tags": "Agile, Scrum", "date": "2017-10-12 21:10:00 +0200", "snippet": "Scrum is a framework which describes an agile approach on how to manage a project. It gives recommendations to teams on how to manage their work to achieve high team performance and support each other in the process.Scrum is easy to learn but hard to master. It is like a hammer, it’s a tool. The people using this tool must have the skills to use it appropriately. In this series, I will tell you everything you need to know about Scrum.If you have never heard about Scrum, you might wonder why use it? Because you will be able to ship working, high-quality software. If you don’t believe me look at the comparison in success rates between waterfall and agile projects.Comparison between waterfall and agile project success rates Source  The graphic shows you that only 9% of agile projects failed, compared to 29% failed projects which used a waterfall approach.When using waterfall, a product gets shipped once at the end of the cycle which can be after years of development. This means that it might take years until the team gets feedback from the customer. Scrum does a little bit of every activity of the SDLC and delivers working software in short circles. This enables an organization to ship the most important features first and give the customer a working product early on. Another advantage of short shipping circles is that the customer can give feedback which the team can implement in the next iteration. Without this feedback, the developer might build something completely different from what the customer expects.The following picture is often used to describe project management. I really like it because it also shows the difference between what the customer explains and what the customer actually wants. With an agile approach, the development team would get early feedback and therefore would not develop so far in the wrong direction. Source The more uncertain you are about how and what you build, the more complicated and chaotic your process becomes. Scrum is appropriate when the project is somewhere between complicated and chaotic. With Scrum, you don’t care too much about what or how you build something at the beginning of the project. You should not use it when the project is simple. It would be an overkill. Next Scrum Part 2 – An overview of Scrum" }, { "title": "Resharper Shortcut List", "url": "/resharper-shortcut-list/", "categories": "Miscellaneous", "tags": "Efficiency, Resharper, Tools, Visual Studio", "date": "2017-10-12 08:53:15 +0200", "snippet": "Resharper can help you to increase your performance to produce code faster and in higher quality. It is a great tool with many functions and shortcuts. Probably too many. Therefore I made my own list of shortcuts which I find useful and which I often use. I will update the list if I find a new useful shortcut.My Resharper shortcut list Shortcut Result CTRL + R, R Rename highlighted element CTRL + E, C Clean file, the setting for the cleaning can be found under Resharper &#8211;&gt; Options &#8211;&gt; Code Editing &#8211;&gt; Code Cleanup CTRL + T Find type in solution CTRL + SHIFT + T Find file or folder in solution CTRL + SHIFT + R Extract interface CTRL + U, R Run unit test, if the cursor is inside a test, run this test, otherwise, run all tests of the class CTRL + U, D Debug unit test, if the cursor is inside a test, run this test, otherwise, run all tests of the class CRTL + U, U Repeat previous test CTRL + U, L Run all tests in solution CTRL + R, I Variable inline CTRL + R, F Introduce Field CTRL + R, V Introduce Variable CTRL + R, P Introduce Parameter CTRL + Shift + Alt + Up Move code up (you can highlight a single line or a whole method and move it up) CTRL + Shift + Alt + Down Move code down(you can highlight a single line or a whole method and move it down) CTRL + Shift + Alt + Left Move code left (useful for switching the parameter order in a method) CTRL + Shift + Alt + Right Move code right(useful for switching the parameter order in a method) Shift + Alt + L Find current file in solution or assembly explorer Alt + Insert Generate a type member.  For the full Resharper shortcut list see Jetbrain’s documentation." }, { "title": "The Agile Manifesto", "url": "/agile-manifesto/", "categories": "Miscellaneous", "tags": "Agile, Scrum", "date": "2017-10-09 16:06:20 +0200", "snippet": "The Agile Manifesto is a guideline how to collaborate in agile projects. The goal is to achieve better results by working together.The Agile Manifesto defines four main rules on how to work together in a project: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan Individuals and interactions: Talking to each other and meeting a colleagues or customers needs is more important than insisting on certain processes or tools.Working software: The main goal is to have a working software. This must be the focus. A great documentation is nice but if the code is self-explaining, then the documentation is not that important anymore (a user documentation might be needed though). See Clean CodeCustomer collaboration: Work together with your customer and try to meet his or her wishes. This leads to a way better result and a more satisfied customer than negotiating every little detail in a contract.Responding to change: Requirements change all the time. Try to adapt to the changes. This leads to a better software and higher customer satisfaction. The Agile Manifesto also defines Twelve Principles of Agile Software which can be found on their website. Sourcehttp://agilemanifesto.org/" }, { "title": "Implementing a Decision Tree using the ID3 and C#", "url": "/implementing-decision-tree-using-c-id3/", "categories": "Miscellaneous", "tags": "C#, ID3, Machine Learning", "date": "2017-10-09 14:48:13 +0200", "snippet": "A decision tree is a classification algorithm used to predict the outcome of an event with given attributes. For example can I play ball when the outlook is sunny, the temperature hot, the humidity high and the wind weak. This post will give an overview on how the algorithm works. For more detailed information please see the later named source. I will focus on the C# implementation.Theory behind the decision treeThe decision tree is a supervised algorithm. This means that the algorithm needs to learn with training data first. After this training phase, the algorithm creates the decision tree and can predict with this tree the outcome of a query. Every leaf is a result and every none leaf is a decision node. If all results of an attribute have the same value, add this result to the decision node.  You can find a great explanation of the ID3 algorithm here.Training dataAs first step you need a set of training data. Day Outlook Temperature Humidity Wind Play ball D1 Sunny Hot High Weak No D2 Sunny Hot High Strong No D3 Overcast Hot High Weak Yes D4 Rain Mild High Weak Yes D5 Rain Cool Normal Weak Yes D6 Rain Cool Normal Strong No D7 Overcast Cool Normal Strong Yes D8 Sunny Mild High Weak No D9 Sunny Cool Normal Weak Yes D10 Rain Mild Normal Weak Yes D11 Sunny Mild Normal Strong Yes D12 Overcast Mild High Strong Yes D13 Overcast Hot Normal Weak Yes D14 Rain Mild High Strong No Applying the algorithmTo apply the ID3 follow the following 4 steps: To figure out which attribute to choose, the algorithm has to calculate the entropy. The entropy indicates how ordered the elements are where an entropy of 1 means totally randomly and 0 means perfectly classified. With this entropy, the algorithm can calculate the information gain of the attribute, where the higher the better. After the information gain is calculated for every attribute, the attribute with the highest information gain will be placed as the root node of the decision tree. With the sample data from above, Outlook will be placed as the root node.  The next step is to repeat this process but without Outlook and only where the value of Outlook is Sunny. This will place Humidity as next decision node. With Sunny and Humidity High all result nodes are No, therefore No will be placed as a leaf node. Sunny and Normal Humidity has only Yes as result. Therefore Yes will be placed as leaf and this side of the tree is finished. After finishing the Sunny side the next attribute is Overcast. Every result with Overcast is Yes. Therefore Yes will be placed as leaf and this route is finished too. The last side Rain will be processed as in 1. but only with Wind and Temperature where Outlook is Rain. Figure 1 shows the result of the finished decision tree.  Figure 1: Decision Tree [Source](https://www.cise.ufl.edu/~ddd/cap6635/Fall-97/Short-papers/2.htm)  The result shows that not all attributes have to be considered. In this example, the attribute Temperature does not appear in the tree.Getting a result for a queryAfter the decision tree is created, getting a result for a query is pretty straightforward. If the user enters the following attributes in his query: Sunny, Mild, Normal, Strong then the algorithm only has to follow the tree and return the leafs node value. In this example, the route leads from Outlook over Sunny to Humidity and over Normal to Yes. The result of this query, therefore, is Yes.Optimizing the algorithmThere are several ways to optimize the algorithm like pruning, subtree raising or subtree replacement. My implementation does neither of that and therefore I won’t go into these topics here.ImplementationWhen I googled for an implementation of a decision tree I could only find one solution in Spanish which didn’t work. My implementation is not perfect but it should run without any problems and helped me to understand how the ID3 algorithm works.The implementation has the following Features: Creating a decision tree Importing training data from a CSV file Exporting training data into a CSV file Manually entered training data Printing the decision tree (as far as possible in the console) Finding a result for a query and printing the used route through the treeTraining phaseThe probably most interesting part of the application is the Tree class. This class contains all the logic for creating the tree, finding a result and printing the tree. The first method called is Learn. This method is responsible for creating the tree. GetRoodNode finds the root node applying the ID3 algorithm. Then the algorithm checks whether the edge of the previously found node leads to a leaf. This is checked in the CheckIfIsLeaf method. If the edge leads to a leaf, the leaf is added. If the edge doesn’t lead to a leaf then CreateSmallerTable is called. This method removes the column of the previously found node from the dataset. With this smaller dataset, Learn is called. This recursion is repeated until every edge points to a leaf.Finding the resultThe CalculateResult method is called to find the outcome of the entered query. This method is pretty straightforward. It takes the root node and then used the entered edge to reach the next node. This process is repeated until a leaf is found. If no edge can be found for the input an error message is displayed. The edges and node names are separated by — and –&gt;. This is to improve the readability of the result when printed to the console.OutputDue to the nature of the console, printing the tree results into displaying every possible route through the tree. To improve the readability nodes are written in upper case and edges in lower case. The root node is printed in Magenta, normal decision nodes in Cyan and leafs in yellow. Edges are printed in yellow. Figure 2: Printing the decision tree If a result for a query is found, the found route will be printed with the same principle as just explained. Figure 3: Printed result of a query LimitationAs already mentioned, my implementation doesn’t do any form of optimization.Another limitation is that the last column is the result column and should only contain two different values. If these values are not Yes or No the coloring of the leafs doesn’t work properly.The last Limitation is the console. Due to its nature, printing a proper tree is (especially dynamically with different inputs) is a pretty tricky task. Since the focus is on the implementation of the algorithm, I didn’t put too much time into the visualization.Further documentation and the source codeYou can find the source code on GitHub. You need at least Visual Studio 2015 to run the solution. Inside the solution folder, you can find a CSV file with test data. For a more comprehensive description of my implementation and related paper to the ID3 see my documentation which you can find here. " } ]
